<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-10-03 Thu 14:12 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Mean Field Analysis of Neural Networks Review</title>
<meta name="author" content="Zain Jabbar, Chuang Xu" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>

          <link rel="stylesheet" href="static/css/site.css" type="text/css"/>
          <header><div class="menu"><ul>
          <li><a href="/">/</a></li>
          <li><a href="/about">/about</a></li>
          <li><a href="/posts">/posts</a></li></ul></div></header>
          <script src="static/js/nastaliq.js"></script>
          <script src="static/js/stacking.js"></script>
          <link href='https://unpkg.com/tippy.js@6.2.3/themes/light.css' rel='stylesheet'>
          <script src="https://unpkg.com/@popperjs/core@2"></script>
          <script src="https://unpkg.com/tippy.js@6"></script>
          <script>
          document.addEventListener('DOMContentLoaded', function() {
            let page = document.querySelector('.page');
            if (page) {
              initializePreviews(page);
            }
          });
          </script>
<script>MathJax = { loader: { load: ['[custom]/xypic.js'], paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'} }, tex: { packages: {'[+]': ['xypic']}, macros: { R: "{\\bf R}" } } };</script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js"></script>
<div class="grid-container"><div class="ds-grid"><div class="page">
<script>MathJax = { loader: { load: ['[custom]/xypic.js'], paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'} }, tex: { packages: {'[+]': ['xypic']}, macros: { R: "{\\bf R}" } } };</script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Mean Field Analysis of Neural Networks Review</h1>
<div id="outline-container-org8b941a1" class="outline-2">
<h2 id="org8b941a1"><span class="section-number-2">1.</span> <span class="todo TODO">TODO</span> List</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org2bac64e" class="outline-3">
<h3 id="org2bac64e"><span class="section-number-3">1.1.</span> Plot Model Performance Training and Test Loss as a function of SGD iterations</h3>
</div>

<div id="outline-container-org6f89ec4" class="outline-3">
<h3 id="org6f89ec4"><span class="section-number-3">1.2.</span> Plot Model Performance with same SGD iteration but as a function of neurons</h3>
</div>

<div id="outline-container-orgefa1a56" class="outline-3">
<h3 id="orgefa1a56"><span class="section-number-3">1.3.</span> Plot Histogram of Model weights</h3>
</div>

<div id="outline-container-org8c5ccf1" class="outline-3">
<h3 id="org8c5ccf1"><span class="section-number-3">1.4.</span> Read about different Scalings</h3>
</div>
</div>
<div id="outline-container-org5cf7c6d" class="outline-2">
<h2 id="org5cf7c6d"><span class="section-number-2">2.</span> Intuition / Motivation of Problem</h2>
<div class="outline-text-2" id="text-2">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.nn.init <span style="color: #531ab6;">as</span> init
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torchvision
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #531ab6;">def</span> <span style="color: #721045;">create_and_train_model</span><span style="color: #000000;">(</span>n_layers, neurons_per_layer, epochs, train_loader<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Create layer sizes array</span>
    <span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>neurons_per_layer<span style="color: #000000;">]</span> * n_layers + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Initialize the model</span>
    <span style="color: #005e8b;">model</span> = CustomNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=0.04<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Function to plot histogram of weights</span>
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">plot_weights</span><span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
        <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
            <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:
                all_weights.extend<span style="color: #000000;">(</span>param.data.cpu<span style="color: #dd22dd;">()</span>.numpy<span style="color: #dd22dd;">()</span>.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>
        plt.hist<span style="color: #000000;">(</span>all_weights, bins=50, alpha=0.7<span style="color: #000000;">)</span>
        plt.title<span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>
        plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
        plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
        plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
        plt.show<span style="color: #000000;">()</span>
        plt.clf<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights before training</span>
    plot_weights<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights Before Training'</span><span style="color: #000000;">)</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Training loop</span>
    <span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>epochs<span style="color: #000000;">)</span>:
        model.train<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
            optimizer.zero_grad<span style="color: #000000;">()</span>
            <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
            <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
            loss.backward<span style="color: #000000;">()</span>
            optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Epoch </span>{epoch+1}<span style="color: #3548cf;">, Loss: </span>{loss.item()}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

        <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights after each epoch</span>
        plot_weights<span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Histogram of Weights After Epoch </span>{epoch+1}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example usage:</span>
<span style="color: #005e8b;">batch_size_train</span> = 128
<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
    torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                               transform=transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                                   transforms.ToTensor<span style="color: #808000;">()</span>,
                                   transforms.Normalize<span style="color: #808000;">(</span><span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                               <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
    batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>nn.Linear<span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #008899;">[</span>i<span style="color: #008899;">]</span>, layer_sizes<span style="color: #008899;">[</span>i+1<span style="color: #008899;">]</span><span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=25, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>
</div>
<div id="outline-container-orgf34f9df" class="outline-3">
<h3 id="orgf34f9df"><span class="section-number-3">2.1.</span> 3 Layer 128 Neurons</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="outline-container-org3b9698d" class="outline-4">
<h4 id="org3b9698d"><span class="section-number-4">2.1.1.</span> <span class="done DONE">DONE</span> Initialization 1</h4>
<div class="outline-text-4" id="text-2-1-1">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torchvision
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>nn.Linear<span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #008899;">[</span>i<span style="color: #008899;">]</span>, layer_sizes<span style="color: #008899;">[</span>i+1<span style="color: #008899;">]</span><span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

<span style="color: #531ab6;">def</span> <span style="color: #721045;">create_and_train_model</span><span style="color: #000000;">(</span>n_layers, neurons_per_layer, epochs, train_loader<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Create layer sizes array</span>
    <span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>neurons_per_layer<span style="color: #000000;">]</span> * n_layers + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Initialize the model</span>
    <span style="color: #005e8b;">model</span> = CustomNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=0.01, momentum=0.5<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Function to plot histogram of weights</span>
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">plot_weights</span><span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
        <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
            <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:
                all_weights.extend<span style="color: #000000;">(</span>param.data.cpu<span style="color: #dd22dd;">()</span>.numpy<span style="color: #dd22dd;">()</span>.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>
        plt.hist<span style="color: #000000;">(</span>all_weights, bins=50, alpha=0.7<span style="color: #000000;">)</span>
        plt.title<span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>
        plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
        plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
        plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
        plt.show<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights before training</span>
    plot_weights<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights Before Training'</span><span style="color: #000000;">)</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Training loop</span>
    <span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>epochs<span style="color: #000000;">)</span>:
        model.train<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
            optimizer.zero_grad<span style="color: #000000;">()</span>
            <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
            <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
            loss.backward<span style="color: #000000;">()</span>
            optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Epoch </span>{epoch+1}<span style="color: #3548cf;">, Loss: </span>{loss.item()}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

        <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights after each epoch</span>
        plot_weights<span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Histogram of Weights After Epoch </span>{epoch+1}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example usage:</span>
<span style="color: #005e8b;">batch_size_train</span> = 128
<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
    torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                               transform=transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                                   transforms.ToTensor<span style="color: #808000;">()</span>,
                                   transforms.Normalize<span style="color: #808000;">(</span><span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                               <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
    batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=25, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="org0ad34f2" class="figure">
<p><img src="./.ob-jupyter/565c69171a4714fc85d14e668cd0ae78fa2950b6.png" alt="565c69171a4714fc85d14e668cd0ae78fa2950b6.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 2.3078696727752686
</pre>


<div id="org9c4af03" class="figure">
<p><img src="./.ob-jupyter/caf203d0d51b5aca48269cea807e2b019deff0d8.png" alt="caf203d0d51b5aca48269cea807e2b019deff0d8.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 2.291422128677368
</pre>


<div id="orga7f166d" class="figure">
<p><img src="./.ob-jupyter/bd83c69110000eb06125c329fca67748b29f52c8.png" alt="bd83c69110000eb06125c329fca67748b29f52c8.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 2.300224542617798
</pre>


<div id="org5e30a9c" class="figure">
<p><img src="./.ob-jupyter/27630ad3b824246960bd37edfbe938d8b7ffd717.png" alt="27630ad3b824246960bd37edfbe938d8b7ffd717.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 2.285484790802002
</pre>


<div id="org0a775bf" class="figure">
<p><img src="./.ob-jupyter/4f3d0bbaf549d3a28cd03820750ea5c8c856633d.png" alt="4f3d0bbaf549d3a28cd03820750ea5c8c856633d.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 2.2888987064361572
</pre>


<div id="org4e45515" class="figure">
<p><img src="./.ob-jupyter/44c4de5b3edbff93c0724692ab781b0ffaa022da.png" alt="44c4de5b3edbff93c0724692ab781b0ffaa022da.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 2.283215284347534
</pre>


<div id="org4470da3" class="figure">
<p><img src="./.ob-jupyter/216fe71d3cf95bfbdedc3aba66f8903709a0b756.png" alt="216fe71d3cf95bfbdedc3aba66f8903709a0b756.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 2.2855021953582764
</pre>


<div id="org4c4bbb8" class="figure">
<p><img src="./.ob-jupyter/c988977d7f1d9695ff692ef6210589d3875b7c04.png" alt="c988977d7f1d9695ff692ef6210589d3875b7c04.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 2.276198625564575
</pre>


<div id="org053fa7b" class="figure">
<p><img src="./.ob-jupyter/f0994372cbc0f108889137c0b5804142ca5830f9.png" alt="f0994372cbc0f108889137c0b5804142ca5830f9.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 2.2443416118621826
</pre>


<div id="org6dec272" class="figure">
<p><img src="./.ob-jupyter/92bb9009e46c9bd55619d94078a764d724f0e11d.png" alt="92bb9009e46c9bd55619d94078a764d724f0e11d.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 2.1926913261413574
</pre>


<div id="org8b23e54" class="figure">
<p><img src="./.ob-jupyter/1d80f9d4f4be555c54b8d555a8325359f6795dbe.png" alt="1d80f9d4f4be555c54b8d555a8325359f6795dbe.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 2.040389060974121
</pre>


<div id="org28a7829" class="figure">
<p><img src="./.ob-jupyter/e3b33d9a94e1548acf14d110a77afa1ce108c748.png" alt="e3b33d9a94e1548acf14d110a77afa1ce108c748.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 1.7373534440994263
</pre>


<div id="orgce91e9c" class="figure">
<p><img src="./.ob-jupyter/e55125989d93219feeb3c131457debece68942c0.png" alt="e55125989d93219feeb3c131457debece68942c0.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 1.5835962295532227
</pre>


<div id="org5ce836f" class="figure">
<p><img src="./.ob-jupyter/c35fc8e0b415fc6abc8d281452cb7ad9587be122.png" alt="c35fc8e0b415fc6abc8d281452cb7ad9587be122.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 1.2558506727218628
</pre>


<div id="orgcb6862c" class="figure">
<p><img src="./.ob-jupyter/e601bfd61a08589a6efa5286fb2f6384ee2edd90.png" alt="e601bfd61a08589a6efa5286fb2f6384ee2edd90.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 1.0134446620941162
</pre>


<div id="org5211708" class="figure">
<p><img src="./.ob-jupyter/f7e4e4be2d5a76c5043636c92ebf3fc03ca9efd0.png" alt="f7e4e4be2d5a76c5043636c92ebf3fc03ca9efd0.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 0.9287149310112
</pre>


<div id="org39f0156" class="figure">
<p><img src="./.ob-jupyter/293227ccb7d2da452e720fc540ddd2ca8ccbf3ab.png" alt="293227ccb7d2da452e720fc540ddd2ca8ccbf3ab.png" />
</p>
</div>
<pre class="example">
Epoch 17, Loss: 0.9402747750282288
</pre>


<div id="orgbe85b83" class="figure">
<p><img src="./.ob-jupyter/61f9e8d152906e9a1d4b59688b8f7e38555e85e6.png" alt="61f9e8d152906e9a1d4b59688b8f7e38555e85e6.png" />
</p>
</div>
<pre class="example">
Epoch 18, Loss: 0.754228413105011
</pre>


<div id="org218b0f8" class="figure">
<p><img src="./.ob-jupyter/64cc8c8272c7751881397151c58d3c677b2bc51b.png" alt="64cc8c8272c7751881397151c58d3c677b2bc51b.png" />
</p>
</div>
<pre class="example">
Epoch 19, Loss: 0.6900477409362793
</pre>


<div id="orgefd0270" class="figure">
<p><img src="./.ob-jupyter/9079fb9437b98ee20f2050e214da7ae8f33a67b4.png" alt="9079fb9437b98ee20f2050e214da7ae8f33a67b4.png" />
</p>
</div>
<pre class="example">
Epoch 20, Loss: 0.6121166348457336
</pre>


<div id="org0332729" class="figure">
<p><img src="./.ob-jupyter/988e116b0bad28bdef1959bb509d281ef3ba4044.png" alt="988e116b0bad28bdef1959bb509d281ef3ba4044.png" />
</p>
</div>
<pre class="example">
Epoch 21, Loss: 0.8453538417816162
</pre>


<div id="org4eec4b5" class="figure">
<p><img src="./.ob-jupyter/f4257453fae2d3336917778af7e86768fe8bcd33.png" alt="f4257453fae2d3336917778af7e86768fe8bcd33.png" />
</p>
</div>
<pre class="example">
Epoch 22, Loss: 0.4835546910762787
</pre>


<div id="org3e9725d" class="figure">
<p><img src="./.ob-jupyter/8d1b18c41e95c105fa1ab9cd03c4cfb40f043a31.png" alt="8d1b18c41e95c105fa1ab9cd03c4cfb40f043a31.png" />
</p>
</div>
<pre class="example">
Epoch 23, Loss: 0.47057580947875977
</pre>


<div id="org0f96966" class="figure">
<p><img src="./.ob-jupyter/5a480fca7bbb205758f766959638dd387149aa20.png" alt="5a480fca7bbb205758f766959638dd387149aa20.png" />
</p>
</div>
<pre class="example">
Epoch 24, Loss: 0.5682224631309509
</pre>


<div id="org6bef264" class="figure">
<p><img src="./.ob-jupyter/a110566affe74bb99d8f84ff1f48f604ec9ab73b.png" alt="a110566affe74bb99d8f84ff1f48f604ec9ab73b.png" />
</p>
</div>
<pre class="example">
Epoch 25, Loss: 0.36390092968940735
</pre>


<div id="org06fe97b" class="figure">
<p><img src="./.ob-jupyter/5dec4cbc57a2b6bb5c5ea2a1ab466486cb9b8b4f.png" alt="5dec4cbc57a2b6bb5c5ea2a1ab466486cb9b8b4f.png" />
</p>
</div>
<pre class="example">
Epoch 26, Loss: 0.4967745542526245
</pre>


<div id="orgf361480" class="figure">
<p><img src="./.ob-jupyter/05ce988665811dea88641ecd69d597f7366d55b7.png" alt="05ce988665811dea88641ecd69d597f7366d55b7.png" />
</p>
</div>
<pre class="example">
Epoch 27, Loss: 0.34841158986091614
</pre>


<div id="org6b545fd" class="figure">
<p><img src="./.ob-jupyter/ac9637566cee1db66bae9e531abee5b7adf926a4.png" alt="ac9637566cee1db66bae9e531abee5b7adf926a4.png" />
</p>
</div>
<pre class="example">
Epoch 28, Loss: 0.3732123076915741
</pre>


<div id="orge5f1cba" class="figure">
<p><img src="./.ob-jupyter/f3ff3719c88768851c6fd1903b9a7e9db9f65b24.png" alt="f3ff3719c88768851c6fd1903b9a7e9db9f65b24.png" />
</p>
</div>
<pre class="example">
Epoch 29, Loss: 0.38130906224250793
</pre>


<div id="org0fbc5d1" class="figure">
<p><img src="./.ob-jupyter/048c2940eda5d2e9ed71cc95b241e09995379991.png" alt="048c2940eda5d2e9ed71cc95b241e09995379991.png" />
</p>
</div>
<pre class="example">
Epoch 30, Loss: 0.354348748922348
</pre>


<div id="org8245208" class="figure">
<p><img src="./.ob-jupyter/d756ec83f900360b7046709d0b9737b2484488a0.png" alt="d756ec83f900360b7046709d0b9737b2484488a0.png" />
</p>
</div>
<pre class="example">
Epoch 31, Loss: 0.3233436644077301
</pre>


<div id="org51f6d85" class="figure">
<p><img src="./.ob-jupyter/a799205954fb592a6de9dfc67b63b4b82cd0d051.png" alt="a799205954fb592a6de9dfc67b63b4b82cd0d051.png" />
</p>
</div>
<pre class="example">
Epoch 32, Loss: 0.3101452887058258
</pre>


<div id="orgeae96cd" class="figure">
<p><img src="./.ob-jupyter/f9faf99a236375bba17706c4abeeef868d0b45a7.png" alt="f9faf99a236375bba17706c4abeeef868d0b45a7.png" />
</p>
</div>
<pre class="example">
Epoch 33, Loss: 0.29405736923217773
</pre>


<div id="org9694248" class="figure">
<p><img src="./.ob-jupyter/906ae8c4ef61d522ace4c9e4043d9aec2ff2cfb2.png" alt="906ae8c4ef61d522ace4c9e4043d9aec2ff2cfb2.png" />
</p>
</div>
<pre class="example">
Epoch 34, Loss: 0.3791450262069702
</pre>


<div id="org741953f" class="figure">
<p><img src="./.ob-jupyter/7d7d1b104fb186e1b15b68b017ced43f2a8f1206.png" alt="7d7d1b104fb186e1b15b68b017ced43f2a8f1206.png" />
</p>
</div>
<pre class="example">
Epoch 35, Loss: 0.33774372935295105
</pre>


<div id="orga2e0038" class="figure">
<p><img src="./.ob-jupyter/243e0f4aaa03943849f8de43c1c47182d558b639.png" alt="243e0f4aaa03943849f8de43c1c47182d558b639.png" />
</p>
</div>
<pre class="example">
Epoch 36, Loss: 0.39602819085121155
</pre>


<div id="org85ef57f" class="figure">
<p><img src="./.ob-jupyter/49fcf168e9d4e684ccfbbb12a9c4a327bc3a401a.png" alt="49fcf168e9d4e684ccfbbb12a9c4a327bc3a401a.png" />
</p>
</div>
<pre class="example">
Epoch 37, Loss: 0.5321682095527649
</pre>


<div id="org97db3c1" class="figure">
<p><img src="./.ob-jupyter/f915a2c272154e65056724706e182b6df1a36e0f.png" alt="f915a2c272154e65056724706e182b6df1a36e0f.png" />
</p>
</div>
<pre class="example">
Epoch 38, Loss: 0.24547946453094482
</pre>


<div id="orgb85e18a" class="figure">
<p><img src="./.ob-jupyter/1d7d09540784d8fc027ba06ddd4c4faf26ba891e.png" alt="1d7d09540784d8fc027ba06ddd4c4faf26ba891e.png" />
</p>
</div>
<pre class="example">
Epoch 39, Loss: 0.39200475811958313
</pre>


<div id="orga17c331" class="figure">
<p><img src="./.ob-jupyter/0004bb6cfa1f84ff488000d0fec52f69093f3fc3.png" alt="0004bb6cfa1f84ff488000d0fec52f69093f3fc3.png" />
</p>
</div>
<pre class="example">
Epoch 40, Loss: 0.47377943992614746
</pre>


<div id="org4a9ae32" class="figure">
<p><img src="./.ob-jupyter/7fd02502b93699b0a2ccc390a7423f1e6675c8c9.png" alt="7fd02502b93699b0a2ccc390a7423f1e6675c8c9.png" />
</p>
</div>
<pre class="example">
Epoch 41, Loss: 0.5222976207733154
</pre>


<div id="org226d1fd" class="figure">
<p><img src="./.ob-jupyter/c08c1a89ea3656b7e6de5b92a1f3faa03b9e9ce9.png" alt="c08c1a89ea3656b7e6de5b92a1f3faa03b9e9ce9.png" />
</p>
</div>
<pre class="example">
Epoch 42, Loss: 0.22054296731948853
</pre>


<div id="org9f2695b" class="figure">
<p><img src="./.ob-jupyter/e5d7746dd42ceee2132351a8a507286c5fb5bb43.png" alt="e5d7746dd42ceee2132351a8a507286c5fb5bb43.png" />
</p>
</div>
<pre class="example">
Epoch 43, Loss: 0.4093993008136749
</pre>


<div id="orgaadd3ce" class="figure">
<p><img src="./.ob-jupyter/abd746c2b2480e50a95e789fe1e8072ef66f78fd.png" alt="abd746c2b2480e50a95e789fe1e8072ef66f78fd.png" />
</p>
</div>
<pre class="example">
Epoch 44, Loss: 0.2442048341035843
</pre>


<div id="org6bde987" class="figure">
<p><img src="./.ob-jupyter/f4a3cd702f058ba6b4c69dc5337bffdfad544de1.png" alt="f4a3cd702f058ba6b4c69dc5337bffdfad544de1.png" />
</p>
</div>
<pre class="example">
Epoch 45, Loss: 0.2746380865573883
</pre>


<div id="org74b3ec1" class="figure">
<p><img src="./.ob-jupyter/648ee281826382a767c5200130bfa55982f378b6.png" alt="648ee281826382a767c5200130bfa55982f378b6.png" />
</p>
</div>
<pre class="example">
Epoch 46, Loss: 0.3785521984100342
</pre>


<div id="orgb7cc95e" class="figure">
<p><img src="./.ob-jupyter/b36d27c24309702134464d628d703f87c0f35a32.png" alt="b36d27c24309702134464d628d703f87c0f35a32.png" />
</p>
</div>
<pre class="example">
Epoch 47, Loss: 0.27123016119003296
</pre>


<div id="org8cf04e6" class="figure">
<p><img src="./.ob-jupyter/31b0312a0b2e1abd1446e6dec065c7197b5e0afc.png" alt="31b0312a0b2e1abd1446e6dec065c7197b5e0afc.png" />
</p>
</div>
<pre class="example">
Epoch 48, Loss: 0.14650079607963562
</pre>


<div id="org535bdd9" class="figure">
<p><img src="./.ob-jupyter/242f1cb4fc2a83aa61fe3fbfe37e8a2e8c3fcc1d.png" alt="242f1cb4fc2a83aa61fe3fbfe37e8a2e8c3fcc1d.png" />
</p>
</div>
<pre class="example">
Epoch 49, Loss: 0.13213397562503815
</pre>


<div id="org43af260" class="figure">
<p><img src="./.ob-jupyter/67f07356f753e13be30a5033d23d5ebf03536b0a.png" alt="67f07356f753e13be30a5033d23d5ebf03536b0a.png" />
</p>
</div>
<pre class="example">
Epoch 50, Loss: 0.22143115103244781
</pre>


<div id="org1408f21" class="figure">
<p><img src="./.ob-jupyter/408ddbd0fab66b7e1ec3c34b0837f4d377eae8fa.png" alt="408ddbd0fab66b7e1ec3c34b0837f4d377eae8fa.png" />
</p>
</div>
<pre class="example">
Epoch 51, Loss: 0.17998461425304413
</pre>


<div id="orgeed6a4b" class="figure">
<p><img src="./.ob-jupyter/437020e7ce4566ffd4acbfce5b9fe84811227f43.png" alt="437020e7ce4566ffd4acbfce5b9fe84811227f43.png" />
</p>
</div>
<pre class="example">
Epoch 52, Loss: 0.15385738015174866
</pre>


<div id="org4ed5a45" class="figure">
<p><img src="./.ob-jupyter/401e1d5da36dde606db46033122908b23eb55a37.png" alt="401e1d5da36dde606db46033122908b23eb55a37.png" />
</p>
</div>
<pre class="example">
Epoch 53, Loss: 0.23202376067638397
</pre>


<div id="org6136782" class="figure">
<p><img src="./.ob-jupyter/d1428f77182b7a8e3c32dff8624962fafa959190.png" alt="d1428f77182b7a8e3c32dff8624962fafa959190.png" />
</p>
</div>
<pre class="example">
Epoch 54, Loss: 0.1866399049758911
</pre>


<div id="org0ef6b67" class="figure">
<p><img src="./.ob-jupyter/6fa45fa2f14d82a33aee5a98d7afb9d2760cda1a.png" alt="6fa45fa2f14d82a33aee5a98d7afb9d2760cda1a.png" />
</p>
</div>
<pre class="example">
Epoch 55, Loss: 0.2170058935880661
</pre>


<div id="org9fd5cbd" class="figure">
<p><img src="./.ob-jupyter/4ab99dc84d384d8688f4d113229b240d8381e0a9.png" alt="4ab99dc84d384d8688f4d113229b240d8381e0a9.png" />
</p>
</div>
<pre class="example">
Epoch 56, Loss: 0.12580864131450653
</pre>


<div id="orgfe3848c" class="figure">
<p><img src="./.ob-jupyter/7a0bcefb8bcc9c19452e61d11304072365295f95.png" alt="7a0bcefb8bcc9c19452e61d11304072365295f95.png" />
</p>
</div>
<pre class="example">
Epoch 57, Loss: 0.22474472224712372
</pre>


<div id="orgfe22648" class="figure">
<p><img src="./.ob-jupyter/3cdde024177784e5f70a8af6dff028d3dc16442e.png" alt="3cdde024177784e5f70a8af6dff028d3dc16442e.png" />
</p>
</div>
<pre class="example">
Epoch 58, Loss: 0.11792764812707901
</pre>


<div id="org8f468ca" class="figure">
<p><img src="./.ob-jupyter/d9ea5f47811421d62fb333c8af21e9eb458684d6.png" alt="d9ea5f47811421d62fb333c8af21e9eb458684d6.png" />
</p>
</div>
<pre class="example">
Epoch 59, Loss: 0.10734958201646805
</pre>


<div id="org3aef8ea" class="figure">
<p><img src="./.ob-jupyter/ab681a55a26c21ff0acfc10dea2a4eed4a6e354c.png" alt="ab681a55a26c21ff0acfc10dea2a4eed4a6e354c.png" />
</p>
</div>
<pre class="example">
Epoch 60, Loss: 0.11728920787572861
</pre>


<div id="orgb024693" class="figure">
<p><img src="./.ob-jupyter/a18b619432fbf7915a99da438c6c49bcd441946d.png" alt="a18b619432fbf7915a99da438c6c49bcd441946d.png" />
</p>
</div>
<pre class="example">
Epoch 61, Loss: 0.1717362403869629
</pre>


<div id="orgc4bb44d" class="figure">
<p><img src="./.ob-jupyter/090bbb73ede2d1f4381f477111d7e3f07f62480c.png" alt="090bbb73ede2d1f4381f477111d7e3f07f62480c.png" />
</p>
</div>
<pre class="example">
Epoch 62, Loss: 0.28059032559394836
</pre>


<div id="org85310b9" class="figure">
<p><img src="./.ob-jupyter/c25649791e564a7cdaf32b0266e81d4c08b5d415.png" alt="c25649791e564a7cdaf32b0266e81d4c08b5d415.png" />
</p>
</div>
<pre class="example">
Epoch 63, Loss: 0.14648835361003876
</pre>


<div id="orga01e190" class="figure">
<p><img src="./.ob-jupyter/48b91b2a3d39464f936ac854a519361042670fa4.png" alt="48b91b2a3d39464f936ac854a519361042670fa4.png" />
</p>
</div>
<pre class="example">
Epoch 64, Loss: 0.4252641201019287
</pre>


<div id="org4fba86e" class="figure">
<p><img src="./.ob-jupyter/4ab7eb9b011d5962fbc37f9db5bb43a4dea0aa1e.png" alt="4ab7eb9b011d5962fbc37f9db5bb43a4dea0aa1e.png" />
</p>
</div>
<pre class="example">
Epoch 65, Loss: 0.11454359441995621
</pre>


<div id="org32ab33a" class="figure">
<p><img src="./.ob-jupyter/aa0841ccca7af5cc4daa7e99c3125193b0272559.png" alt="aa0841ccca7af5cc4daa7e99c3125193b0272559.png" />
</p>
</div>
<pre class="example">
Epoch 66, Loss: 0.30044999718666077
</pre>


<div id="org49a0f68" class="figure">
<p><img src="./.ob-jupyter/3debd1627c35e5b25466f662487beef7bb744dfa.png" alt="3debd1627c35e5b25466f662487beef7bb744dfa.png" />
</p>
</div>
<pre class="example">
Epoch 67, Loss: 0.20920330286026
</pre>


<div id="org319034d" class="figure">
<p><img src="./.ob-jupyter/23f9a28bd051a80614744fae752239722509d76b.png" alt="23f9a28bd051a80614744fae752239722509d76b.png" />
</p>
</div>
<pre class="example">
Epoch 68, Loss: 0.09689810872077942
</pre>


<div id="orgf8e6b96" class="figure">
<p><img src="./.ob-jupyter/9c80fc036bdb66af3eec825651aeba0c8b5ce732.png" alt="9c80fc036bdb66af3eec825651aeba0c8b5ce732.png" />
</p>
</div>
<pre class="example">
Epoch 69, Loss: 0.21189147233963013
</pre>


<div id="org945957f" class="figure">
<p><img src="./.ob-jupyter/3c89ca71f5dc4ff912219549637d5d7126f4b879.png" alt="3c89ca71f5dc4ff912219549637d5d7126f4b879.png" />
</p>
</div>
<pre class="example">
Epoch 70, Loss: 0.12376383692026138
</pre>


<div id="org70889e2" class="figure">
<p><img src="./.ob-jupyter/423eb25fe8e5aa2efbb3dcbf36954807277a1d88.png" alt="423eb25fe8e5aa2efbb3dcbf36954807277a1d88.png" />
</p>
</div>
<pre class="example">
Epoch 71, Loss: 0.10320442169904709
</pre>


<div id="org13a0693" class="figure">
<p><img src="./.ob-jupyter/295ea3da78c91d31405f28653ae22fab0dcada93.png" alt="295ea3da78c91d31405f28653ae22fab0dcada93.png" />
</p>
</div>
<pre class="example">
Epoch 72, Loss: 0.1560615450143814
</pre>


<div id="org537a6f4" class="figure">
<p><img src="./.ob-jupyter/07c95b03c7bdf88ffe7a6dddb411d9f1862ad791.png" alt="07c95b03c7bdf88ffe7a6dddb411d9f1862ad791.png" />
</p>
</div>
<pre class="example">
Epoch 73, Loss: 0.11476748436689377
</pre>


<div id="org11869c5" class="figure">
<p><img src="./.ob-jupyter/47baa1ff59e01c244ea4ba866ed89e6454951285.png" alt="47baa1ff59e01c244ea4ba866ed89e6454951285.png" />
</p>
</div>
<pre class="example">
Epoch 74, Loss: 0.1459258794784546
</pre>


<div id="orgb867366" class="figure">
<p><img src="./.ob-jupyter/676eb6bc77b33650d9c347fd98631896860b8eba.png" alt="676eb6bc77b33650d9c347fd98631896860b8eba.png" />
</p>
</div>
<pre class="example">
Epoch 75, Loss: 0.1488148421049118
</pre>


<div id="orga623e94" class="figure">
<p><img src="./.ob-jupyter/bb9b82abbc06bae1159d073fbb792dacb75710e4.png" alt="bb9b82abbc06bae1159d073fbb792dacb75710e4.png" />
</p>
</div>
<pre class="example">
Epoch 76, Loss: 0.1751343160867691
</pre>


<div id="org9a926f8" class="figure">
<p><img src="./.ob-jupyter/3211366998fa45cab9365678a28d2117d265b7c0.png" alt="3211366998fa45cab9365678a28d2117d265b7c0.png" />
</p>
</div>
<pre class="example">
Epoch 77, Loss: 0.10351305454969406
</pre>


<div id="orgffb61d2" class="figure">
<p><img src="./.ob-jupyter/5dd3a896039604e315513dd81b505e69c903ddd3.png" alt="5dd3a896039604e315513dd81b505e69c903ddd3.png" />
</p>
</div>
<pre class="example">
Epoch 78, Loss: 0.13436515629291534
</pre>


<div id="orgaf3e683" class="figure">
<p><img src="./.ob-jupyter/7b07b480b767ab9ffe53340325fbd152b49f6bbe.png" alt="7b07b480b767ab9ffe53340325fbd152b49f6bbe.png" />
</p>
</div>
<pre class="example">
Epoch 79, Loss: 0.05224045738577843
</pre>


<div id="org721ae62" class="figure">
<p><img src="./.ob-jupyter/4d7ecd9af0071c54e7816ec5ae1618aaaf1ba941.png" alt="4d7ecd9af0071c54e7816ec5ae1618aaaf1ba941.png" />
</p>
</div>
<pre class="example">
Epoch 80, Loss: 0.12379950284957886
</pre>


<div id="org73ab212" class="figure">
<p><img src="./.ob-jupyter/de5f4bc1e1b06d3ee3c461601f96b26176d9bf66.png" alt="de5f4bc1e1b06d3ee3c461601f96b26176d9bf66.png" />
</p>
</div>
<pre class="example">
Epoch 81, Loss: 0.09641330689191818
</pre>


<div id="org303d733" class="figure">
<p><img src="./.ob-jupyter/446842c10fa2b4aaf7f6971283615bf73d35b954.png" alt="446842c10fa2b4aaf7f6971283615bf73d35b954.png" />
</p>
</div>
<pre class="example">
Epoch 82, Loss: 0.061189886182546616
</pre>


<div id="org249af16" class="figure">
<p><img src="./.ob-jupyter/e01472a22a7f239f54782458b8ba846c0084d369.png" alt="e01472a22a7f239f54782458b8ba846c0084d369.png" />
</p>
</div>
<pre class="example">
Epoch 83, Loss: 0.0773191824555397
</pre>


<div id="org20d8fab" class="figure">
<p><img src="./.ob-jupyter/61211732436d3bb3cb6312dec4f3460148395086.png" alt="61211732436d3bb3cb6312dec4f3460148395086.png" />
</p>
</div>
<pre class="example">
Epoch 84, Loss: 0.06494055688381195
</pre>


<div id="org32035e6" class="figure">
<p><img src="./.ob-jupyter/99d252a059e19d3bb4485ba5ea53a5e7a276df3e.png" alt="99d252a059e19d3bb4485ba5ea53a5e7a276df3e.png" />
</p>
</div>
<pre class="example">
Epoch 85, Loss: 0.15761306881904602
</pre>


<div id="orgdb89483" class="figure">
<p><img src="./.ob-jupyter/227504664730a214c2a208292d632889d8097db4.png" alt="227504664730a214c2a208292d632889d8097db4.png" />
</p>
</div>
<pre class="example">
Epoch 86, Loss: 0.07646659761667252
</pre>


<div id="org944a86f" class="figure">
<p><img src="./.ob-jupyter/ece7f4c474fde078aaae8ae3bc4f74ba185bccff.png" alt="ece7f4c474fde078aaae8ae3bc4f74ba185bccff.png" />
</p>
</div>
<pre class="example">
Epoch 87, Loss: 0.08346181362867355
</pre>


<div id="orga979ab5" class="figure">
<p><img src="./.ob-jupyter/8308056ba81963f828341f0383cb793069ee2241.png" alt="8308056ba81963f828341f0383cb793069ee2241.png" />
</p>
</div>
<pre class="example">
Epoch 88, Loss: 0.057207170873880386
</pre>


<div id="orge3aa4e7" class="figure">
<p><img src="./.ob-jupyter/c165eea6696f860fe1601d593360e606a09cc776.png" alt="c165eea6696f860fe1601d593360e606a09cc776.png" />
</p>
</div>
<pre class="example">
Epoch 89, Loss: 0.03770401328802109
</pre>


<div id="orgbdfbe48" class="figure">
<p><img src="./.ob-jupyter/fd134267fc8b66f392fd4f224ce73e16e6288145.png" alt="fd134267fc8b66f392fd4f224ce73e16e6288145.png" />
</p>
</div>
<pre class="example">
Epoch 90, Loss: 0.14383292198181152
</pre>


<div id="org21dbf95" class="figure">
<p><img src="./.ob-jupyter/d8212f0381c072f4dc5894e78e5ea3916441c600.png" alt="d8212f0381c072f4dc5894e78e5ea3916441c600.png" />
</p>
</div>
<pre class="example">
Epoch 91, Loss: 0.09692437201738358
</pre>


<div id="org23dc4b9" class="figure">
<p><img src="./.ob-jupyter/7eedc091bf7ead11dc15e6537b1d74d5430b98b9.png" alt="7eedc091bf7ead11dc15e6537b1d74d5430b98b9.png" />
</p>
</div>
<pre class="example">
Epoch 92, Loss: 0.07983925938606262
</pre>


<div id="orgcc8632a" class="figure">
<p><img src="./.ob-jupyter/c3efb434a8d6952ce8c06adebbbeb689ac63d7bf.png" alt="c3efb434a8d6952ce8c06adebbbeb689ac63d7bf.png" />
</p>
</div>
<pre class="example">
Epoch 93, Loss: 0.06481090933084488
</pre>


<div id="orgd2b4844" class="figure">
<p><img src="./.ob-jupyter/3606694f2dda6b537ed3fe974ac4931449f1d278.png" alt="3606694f2dda6b537ed3fe974ac4931449f1d278.png" />
</p>
</div>
<pre class="example">
Epoch 94, Loss: 0.0778236836194992
</pre>


<div id="org6a6207f" class="figure">
<p><img src="./.ob-jupyter/cbc864602977fc12dfb6ee5db9a889fd29165655.png" alt="cbc864602977fc12dfb6ee5db9a889fd29165655.png" />
</p>
</div>
<pre class="example">
Epoch 95, Loss: 0.06882669776678085
</pre>


<div id="orgd0dd694" class="figure">
<p><img src="./.ob-jupyter/ee530628ff556a12194d694b4d7cbdc9d22d5b0d.png" alt="ee530628ff556a12194d694b4d7cbdc9d22d5b0d.png" />
</p>
</div>
<pre class="example">
Epoch 96, Loss: 0.0475856252014637
</pre>


<div id="org43bdaba" class="figure">
<p><img src="./.ob-jupyter/5084b5696c87a0d4a6a31f1ea4e99986f43abe67.png" alt="5084b5696c87a0d4a6a31f1ea4e99986f43abe67.png" />
</p>
</div>
<pre class="example">
Epoch 97, Loss: 0.09465021640062332
</pre>


<div id="orgc19410e" class="figure">
<p><img src="./.ob-jupyter/330e8122c48fe0c8bca7e6c4326dc36f05405e9e.png" alt="330e8122c48fe0c8bca7e6c4326dc36f05405e9e.png" />
</p>
</div>
<pre class="example">
Epoch 98, Loss: 0.1396171599626541
</pre>


<div id="org612cbbc" class="figure">
<p><img src="./.ob-jupyter/6046e773a05504efc4cb0ca02cf19bc3bea3b72c.png" alt="6046e773a05504efc4cb0ca02cf19bc3bea3b72c.png" />
</p>
</div>
<pre class="example">
Epoch 99, Loss: 0.22794313728809357
</pre>


<div id="orgb2a1919" class="figure">
<p><img src="./.ob-jupyter/7c7a8fb73f1c4ea4d26fab3cf696af61b284d8cd.png" alt="7c7a8fb73f1c4ea4d26fab3cf696af61b284d8cd.png" />
</p>
</div>
<pre class="example">
Epoch 100, Loss: 0.06968068331480026
</pre>


<div id="orgbe1e1c1" class="figure">
<p><img src="./.ob-jupyter/3775982fc1e6dc359bcb12a0e3eb49bcc58363f0.png" alt="3775982fc1e6dc359bcb12a0e3eb49bcc58363f0.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org1a023d5" class="outline-4">
<h4 id="org1a023d5"><span class="section-number-4">2.1.2.</span> <span class="done WAITING">WAITING</span> Initialization Normal with 0 Bias</h4>
<div class="outline-text-4" id="text-2-1-2">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torchvision
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np
<span style="color: #531ab6;">import</span> torch.nn.init <span style="color: #531ab6;">as</span> init

<span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #005e8b;">layer</span> = nn.Linear<span style="color: #000000;">(</span>layer_sizes<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">]</span>, layer_sizes<span style="color: #dd22dd;">[</span>i+1<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>layer<span style="color: #000000;">)</span>
            <span style="color: #595959;"># </span><span style="color: #595959;">Apply Xavier Normal initialization to each layer</span>
            init.xavier_normal_<span style="color: #000000;">(</span>layer.weight<span style="color: #000000;">)</span>
            <span style="color: #595959;"># </span><span style="color: #595959;">Set biases to zero (optional)</span>
            init.zeros_<span style="color: #000000;">(</span>layer.bias<span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

<span style="color: #531ab6;">def</span> <span style="color: #721045;">create_and_train_model</span><span style="color: #000000;">(</span>n_layers, neurons_per_layer, epochs, train_loader<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Create layer sizes array</span>
    <span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>neurons_per_layer<span style="color: #000000;">]</span> * n_layers + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Initialize the model</span>
    <span style="color: #005e8b;">model</span> = CustomNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=0.01, momentum=0.5<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Function to plot histogram of weights</span>
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">plot_weights</span><span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
        <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
            <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:
                all_weights.extend<span style="color: #000000;">(</span>param.data.cpu<span style="color: #dd22dd;">()</span>.numpy<span style="color: #dd22dd;">()</span>.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>
        plt.hist<span style="color: #000000;">(</span>all_weights, bins=50, alpha=0.7<span style="color: #000000;">)</span>
        plt.title<span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>
        plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
        plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
        plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
        plt.show<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights before training</span>
    plot_weights<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights Before Training'</span><span style="color: #000000;">)</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Training loop</span>
    <span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>epochs<span style="color: #000000;">)</span>:
        model.train<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
            optimizer.zero_grad<span style="color: #000000;">()</span>
            <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
            <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
            loss.backward<span style="color: #000000;">()</span>
            optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Epoch </span>{epoch+1}<span style="color: #3548cf;">, Loss: </span>{loss.item()}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

        <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights after each epoch</span>
        plot_weights<span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Histogram of Weights After Epoch </span>{epoch+1}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example usage:</span>
<span style="color: #005e8b;">batch_size_train</span> = 128
<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
    torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                               transform=transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                                   transforms.ToTensor<span style="color: #808000;">()</span>,
                                   transforms.Normalize<span style="color: #808000;">(</span><span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                               <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
    batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=100, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="org4f3c47d" class="figure">
<p><img src="./.ob-jupyter/517c218e9baf5ef9e8e43a8c92b0e700d7f7a858.png" alt="517c218e9baf5ef9e8e43a8c92b0e700d7f7a858.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 2.243140459060669
</pre>


<div id="orgcb4a771" class="figure">
<p><img src="./.ob-jupyter/1bb37760f33c11381a38c89f8db38507e726e44a.png" alt="1bb37760f33c11381a38c89f8db38507e726e44a.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 2.178248167037964
</pre>


<div id="orge8e3784" class="figure">
<p><img src="./.ob-jupyter/d9e75bfc2a0793a6b47c9b6f1a46e5d9e577ade7.png" alt="d9e75bfc2a0793a6b47c9b6f1a46e5d9e577ade7.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 1.886781096458435
</pre>


<div id="orgf007f04" class="figure">
<p><img src="./.ob-jupyter/be71f43285b7715a1ed48ee06598f27d629eb2e7.png" alt="be71f43285b7715a1ed48ee06598f27d629eb2e7.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 1.5410761833190918
</pre>


<div id="org74f9bd7" class="figure">
<p><img src="./.ob-jupyter/0d280097e313fc21780a044ed13c98ece7655e5c.png" alt="0d280097e313fc21780a044ed13c98ece7655e5c.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 1.1767771244049072
</pre>


<div id="org0608951" class="figure">
<p><img src="./.ob-jupyter/4ddb0d4fc7923c4bde4550361a1a31595bdb0213.png" alt="4ddb0d4fc7923c4bde4550361a1a31595bdb0213.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 0.906932532787323
</pre>


<div id="orga348d2f" class="figure">
<p><img src="./.ob-jupyter/fc577c09ba07a8e0bb5b5ddd545fe1692a1dd1c3.png" alt="fc577c09ba07a8e0bb5b5ddd545fe1692a1dd1c3.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 0.7335243225097656
</pre>


<div id="orgd970943" class="figure">
<p><img src="./.ob-jupyter/7dc2157398d91c309c5e0419cefb74cd06a183f3.png" alt="7dc2157398d91c309c5e0419cefb74cd06a183f3.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 0.8196816444396973
</pre>


<div id="org96e493f" class="figure">
<p><img src="./.ob-jupyter/0cdaf68531fc674e11ddd4e2b46b38980e48907f.png" alt="0cdaf68531fc674e11ddd4e2b46b38980e48907f.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 0.5901268124580383
</pre>


<div id="org0d847ec" class="figure">
<p><img src="./.ob-jupyter/baccb20d8b2b474e309db8f5d442f5c8612f78f8.png" alt="baccb20d8b2b474e309db8f5d442f5c8612f78f8.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 0.500286340713501
</pre>


<div id="org16802c5" class="figure">
<p><img src="./.ob-jupyter/c5ad9c793e26c1f8aed892f46418bb6e2653a8ae.png" alt="c5ad9c793e26c1f8aed892f46418bb6e2653a8ae.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 0.5042247176170349
</pre>


<div id="org8f97f0f" class="figure">
<p><img src="./.ob-jupyter/da75b14a4e65192437807be0c92826cc1839a8ea.png" alt="da75b14a4e65192437807be0c92826cc1839a8ea.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 0.3072202801704407
</pre>


<div id="orgb6c1648" class="figure">
<p><img src="./.ob-jupyter/c77ecaa5ed78ef364a76bb398153505d3574bf5e.png" alt="c77ecaa5ed78ef364a76bb398153505d3574bf5e.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 0.6142950654029846
</pre>


<div id="org7eee47e" class="figure">
<p><img src="./.ob-jupyter/0a552a8f10f92443bfa3780b5781c2a2e1a2c60e.png" alt="0a552a8f10f92443bfa3780b5781c2a2e1a2c60e.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 0.3469279110431671
</pre>


<div id="org3e88688" class="figure">
<p><img src="./.ob-jupyter/c40ea2c9814ba8ec6c0a78bcc45d080497fbc52c.png" alt="c40ea2c9814ba8ec6c0a78bcc45d080497fbc52c.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 0.26291847229003906
</pre>


<div id="orgfe41246" class="figure">
<p><img src="./.ob-jupyter/b0dd1f4e04f5d2285c45dee68196eba3e557a066.png" alt="b0dd1f4e04f5d2285c45dee68196eba3e557a066.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 0.27780410647392273
</pre>


<div id="orgcb1ed74" class="figure">
<p><img src="./.ob-jupyter/a8136e64b2eff474ecbd7a36fc12bd12191985b3.png" alt="a8136e64b2eff474ecbd7a36fc12bd12191985b3.png" />
</p>
</div>
<pre class="example">
Epoch 17, Loss: 0.43214061856269836
</pre>


<div id="orga49815d" class="figure">
<p><img src="./.ob-jupyter/c424904c6a5106f6b1d7201599d1d6446a97cf7a.png" alt="c424904c6a5106f6b1d7201599d1d6446a97cf7a.png" />
</p>
</div>
<pre class="example">
Epoch 18, Loss: 0.30986008048057556
</pre>


<div id="org328abc5" class="figure">
<p><img src="./.ob-jupyter/f879c90509bfd4f2c39d77b0180839b5126c7266.png" alt="f879c90509bfd4f2c39d77b0180839b5126c7266.png" />
</p>
</div>
<pre class="example">
Epoch 19, Loss: 0.4180997908115387
</pre>


<div id="orgcd4f1ad" class="figure">
<p><img src="./.ob-jupyter/b642cdd29ca4bd214bd22666d28bbd1f28ec6951.png" alt="b642cdd29ca4bd214bd22666d28bbd1f28ec6951.png" />
</p>
</div>
<pre class="example">
Epoch 20, Loss: 0.3627864420413971
</pre>


<div id="orgb212508" class="figure">
<p><img src="./.ob-jupyter/1d4b7cb551ba9e5b9930ded0781ae6c615ab7919.png" alt="1d4b7cb551ba9e5b9930ded0781ae6c615ab7919.png" />
</p>
</div>
<pre class="example">
Epoch 21, Loss: 0.33999601006507874
</pre>


<div id="orgb39bb94" class="figure">
<p><img src="./.ob-jupyter/06feb40b0270261b72f72c19b94b5f9dd22900af.png" alt="06feb40b0270261b72f72c19b94b5f9dd22900af.png" />
</p>
</div>
<pre class="example">
Epoch 22, Loss: 0.1783614307641983
</pre>


<div id="org91976ca" class="figure">
<p><img src="./.ob-jupyter/308c5c43373bbc27f21cb27dc97f5e9dde37a3bf.png" alt="308c5c43373bbc27f21cb27dc97f5e9dde37a3bf.png" />
</p>
</div>
<pre class="example">
Epoch 23, Loss: 0.38329899311065674
</pre>


<div id="org10250a9" class="figure">
<p><img src="./.ob-jupyter/2b9ea833c5390c9f149b5455f8be631a67398638.png" alt="2b9ea833c5390c9f149b5455f8be631a67398638.png" />
</p>
</div>
<pre class="example">
Epoch 24, Loss: 0.36371365189552307
</pre>


<div id="orgc0e7800" class="figure">
<p><img src="./.ob-jupyter/70c3357e134224116da0e0bdffbb25d4277eefb5.png" alt="70c3357e134224116da0e0bdffbb25d4277eefb5.png" />
</p>
</div>
<pre class="example">
Epoch 25, Loss: 0.16481803357601166
</pre>


<div id="org05d082e" class="figure">
<p><img src="./.ob-jupyter/c4addd75f98dbb0ac4d2a14c4b91c6f17e4e9050.png" alt="c4addd75f98dbb0ac4d2a14c4b91c6f17e4e9050.png" />
</p>
</div>
<pre class="example">
Epoch 26, Loss: 0.3388056755065918
</pre>


<div id="orgaddee5d" class="figure">
<p><img src="./.ob-jupyter/ac18f6de89b9f4aea33c8e835767f19667de15b9.png" alt="ac18f6de89b9f4aea33c8e835767f19667de15b9.png" />
</p>
</div>
<pre class="example">
Epoch 27, Loss: 0.23921765387058258
</pre>


<div id="org235ac59" class="figure">
<p><img src="./.ob-jupyter/ccfd2c780c192f0e0a1a017e9db597613f7b388d.png" alt="ccfd2c780c192f0e0a1a017e9db597613f7b388d.png" />
</p>
</div>
<pre class="example">
Epoch 28, Loss: 0.13831645250320435
</pre>


<div id="org1d11a9f" class="figure">
<p><img src="./.ob-jupyter/818da78ce988efb756ddaec73867a1dad1961e5f.png" alt="818da78ce988efb756ddaec73867a1dad1961e5f.png" />
</p>
</div>
<pre class="example">
Epoch 29, Loss: 0.33213913440704346
</pre>


<div id="orga579458" class="figure">
<p><img src="./.ob-jupyter/16d665db781dd87a42a1054cbdab1faaa6db209c.png" alt="16d665db781dd87a42a1054cbdab1faaa6db209c.png" />
</p>
</div>
<pre class="example">
Epoch 30, Loss: 0.18167275190353394
</pre>


<div id="orgc1e708c" class="figure">
<p><img src="./.ob-jupyter/e864152f91f7c370c28921c24a7254c7a2e7ff15.png" alt="e864152f91f7c370c28921c24a7254c7a2e7ff15.png" />
</p>
</div>
<pre class="example">
Epoch 31, Loss: 0.24784862995147705
</pre>


<div id="org6f24b8b" class="figure">
<p><img src="./.ob-jupyter/e108953e7eb619b321538d364378b34704f0446d.png" alt="e108953e7eb619b321538d364378b34704f0446d.png" />
</p>
</div>
<pre class="example">
Epoch 32, Loss: 0.1568192094564438
</pre>


<div id="org3287716" class="figure">
<p><img src="./.ob-jupyter/379aaacbe99212d7a28d3eb5c33563ef0bb0cf37.png" alt="379aaacbe99212d7a28d3eb5c33563ef0bb0cf37.png" />
</p>
</div>
<pre class="example">
Epoch 33, Loss: 0.33296921849250793
</pre>


<div id="org8d913a0" class="figure">
<p><img src="./.ob-jupyter/81d2f9574bef59a1e425cb463678a1fb36bc260b.png" alt="81d2f9574bef59a1e425cb463678a1fb36bc260b.png" />
</p>
</div>
<pre class="example">
Epoch 34, Loss: 0.14033427834510803
</pre>


<div id="org807991b" class="figure">
<p><img src="./.ob-jupyter/29d3f3cb76f9b0fd381b138cde3fae171ceb0b59.png" alt="29d3f3cb76f9b0fd381b138cde3fae171ceb0b59.png" />
</p>
</div>
<pre class="example">
Epoch 35, Loss: 0.2000270038843155
</pre>


<div id="org3d70d42" class="figure">
<p><img src="./.ob-jupyter/2aa4f8441b29669f6270e84d74304fa6efde658c.png" alt="2aa4f8441b29669f6270e84d74304fa6efde658c.png" />
</p>
</div>
<pre class="example">
Epoch 36, Loss: 0.26879453659057617
</pre>


<div id="org062afa9" class="figure">
<p><img src="./.ob-jupyter/8ff5ad06748e914830663048a518244f405cd220.png" alt="8ff5ad06748e914830663048a518244f405cd220.png" />
</p>
</div>
<pre class="example">
Epoch 37, Loss: 0.3742542266845703
</pre>


<div id="org034f84f" class="figure">
<p><img src="./.ob-jupyter/edff8f1ac9e158cf583b4917536e688a3fd790d2.png" alt="edff8f1ac9e158cf583b4917536e688a3fd790d2.png" />
</p>
</div>
<pre class="example">
Epoch 38, Loss: 0.15526171028614044
</pre>


<div id="org6337ba9" class="figure">
<p><img src="./.ob-jupyter/8775a4a9c038bc66c7988d8aa187f315b1104674.png" alt="8775a4a9c038bc66c7988d8aa187f315b1104674.png" />
</p>
</div>
<pre class="example">
Epoch 39, Loss: 0.13358378410339355
</pre>


<div id="orgd0a88ff" class="figure">
<p><img src="./.ob-jupyter/37cafe9f997879ba884fd37f5877788303dc6aae.png" alt="37cafe9f997879ba884fd37f5877788303dc6aae.png" />
</p>
</div>
<pre class="example">
Epoch 40, Loss: 0.12863579392433167
</pre>


<div id="org28875cf" class="figure">
<p><img src="./.ob-jupyter/064649a97beb6c58877cb7fcd507469143034d84.png" alt="064649a97beb6c58877cb7fcd507469143034d84.png" />
</p>
</div>
<pre class="example">
Epoch 41, Loss: 0.2640365660190582
</pre>


<div id="orgfac9e6c" class="figure">
<p><img src="./.ob-jupyter/0ef910f7ad4bd0092862ecad4354689049718f63.png" alt="0ef910f7ad4bd0092862ecad4354689049718f63.png" />
</p>
</div>
<pre class="example">
Epoch 42, Loss: 0.19071394205093384
</pre>


<div id="org5ef5d0b" class="figure">
<p><img src="./.ob-jupyter/8d74241f189ed36b03c7c760176a69db0fcd8903.png" alt="8d74241f189ed36b03c7c760176a69db0fcd8903.png" />
</p>
</div>
<pre class="example">
Epoch 43, Loss: 0.18808774650096893
</pre>


<div id="org73fcd0b" class="figure">
<p><img src="./.ob-jupyter/c3c7e5bbedea0decf7c35fdfecb41155188ae3d3.png" alt="c3c7e5bbedea0decf7c35fdfecb41155188ae3d3.png" />
</p>
</div>
<pre class="example">
Epoch 44, Loss: 0.17353232204914093
</pre>


<div id="org2285e8d" class="figure">
<p><img src="./.ob-jupyter/f1fa4fe42e5d7761cb370001a722dd542b9a8296.png" alt="f1fa4fe42e5d7761cb370001a722dd542b9a8296.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-orge777ab9" class="outline-4">
<h4 id="orge777ab9"><span class="section-number-4">2.1.3.</span> <span class="todo TODO">TODO</span> Initialization Uniform Weights</h4>
<div class="outline-text-4" id="text-2-1-3">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torchvision
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np
<span style="color: #531ab6;">import</span> torch.nn.init <span style="color: #531ab6;">as</span> init

<span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #005e8b;">layer</span> = nn.Linear<span style="color: #000000;">(</span>layer_sizes<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">]</span>, layer_sizes<span style="color: #dd22dd;">[</span>i+1<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>layer<span style="color: #000000;">)</span>
            <span style="color: #595959;"># </span><span style="color: #595959;">Apply Xavier Normal initialization to each layer</span>
            init.uniform_<span style="color: #000000;">(</span>layer.weight<span style="color: #000000;">)</span>
            <span style="color: #595959;"># </span><span style="color: #595959;">Set biases to zero (optional)</span>
            init.uniform_<span style="color: #000000;">(</span>layer.bias<span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

<span style="color: #531ab6;">def</span> <span style="color: #721045;">create_and_train_model</span><span style="color: #000000;">(</span>n_layers, neurons_per_layer, epochs, train_loader<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Create layer sizes array</span>
    <span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>neurons_per_layer<span style="color: #000000;">]</span> * n_layers + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Initialize the model</span>
    <span style="color: #005e8b;">model</span> = CustomNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=0.01, momentum=0.5<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Function to plot histogram of weights</span>
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">plot_weights</span><span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
        <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
            <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:
                all_weights.extend<span style="color: #000000;">(</span>param.data.cpu<span style="color: #dd22dd;">()</span>.numpy<span style="color: #dd22dd;">()</span>.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>
        plt.hist<span style="color: #000000;">(</span>all_weights, bins=50, alpha=0.7<span style="color: #000000;">)</span>
        plt.title<span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>
        plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
        plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
        plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
        plt.show<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights before training</span>
    plot_weights<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights Before Training'</span><span style="color: #000000;">)</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Training loop</span>
    <span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>epochs<span style="color: #000000;">)</span>:
        model.train<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
            optimizer.zero_grad<span style="color: #000000;">()</span>
            <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
            <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
            loss.backward<span style="color: #000000;">()</span>
            optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Epoch </span>{epoch+1}<span style="color: #3548cf;">, Loss: </span>{loss.item()}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

        <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights after each epoch</span>
        plot_weights<span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Histogram of Weights After Epoch </span>{epoch+1}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example usage:</span>
<span style="color: #005e8b;">batch_size_train</span> = 128
<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
    torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                               transform=transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                                   transforms.ToTensor<span style="color: #808000;">()</span>,
                                   transforms.Normalize<span style="color: #808000;">(</span><span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                               <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
    batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=100, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="org3c41199" class="figure">
<p><img src="./.ob-jupyter/517c218e9baf5ef9e8e43a8c92b0e700d7f7a858.png" alt="517c218e9baf5ef9e8e43a8c92b0e700d7f7a858.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 2.243140459060669
</pre>


<div id="org813a074" class="figure">
<p><img src="./.ob-jupyter/1bb37760f33c11381a38c89f8db38507e726e44a.png" alt="1bb37760f33c11381a38c89f8db38507e726e44a.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 2.178248167037964
</pre>


<div id="orge256094" class="figure">
<p><img src="./.ob-jupyter/d9e75bfc2a0793a6b47c9b6f1a46e5d9e577ade7.png" alt="d9e75bfc2a0793a6b47c9b6f1a46e5d9e577ade7.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 1.886781096458435
</pre>


<div id="org86200c6" class="figure">
<p><img src="./.ob-jupyter/be71f43285b7715a1ed48ee06598f27d629eb2e7.png" alt="be71f43285b7715a1ed48ee06598f27d629eb2e7.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 1.5410761833190918
</pre>


<div id="org2d7c704" class="figure">
<p><img src="./.ob-jupyter/0d280097e313fc21780a044ed13c98ece7655e5c.png" alt="0d280097e313fc21780a044ed13c98ece7655e5c.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 1.1767771244049072
</pre>


<div id="org89e6c21" class="figure">
<p><img src="./.ob-jupyter/4ddb0d4fc7923c4bde4550361a1a31595bdb0213.png" alt="4ddb0d4fc7923c4bde4550361a1a31595bdb0213.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 0.906932532787323
</pre>


<div id="orgbf8413f" class="figure">
<p><img src="./.ob-jupyter/fc577c09ba07a8e0bb5b5ddd545fe1692a1dd1c3.png" alt="fc577c09ba07a8e0bb5b5ddd545fe1692a1dd1c3.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 0.7335243225097656
</pre>


<div id="org2ed6833" class="figure">
<p><img src="./.ob-jupyter/7dc2157398d91c309c5e0419cefb74cd06a183f3.png" alt="7dc2157398d91c309c5e0419cefb74cd06a183f3.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 0.8196816444396973
</pre>


<div id="orge41ec69" class="figure">
<p><img src="./.ob-jupyter/0cdaf68531fc674e11ddd4e2b46b38980e48907f.png" alt="0cdaf68531fc674e11ddd4e2b46b38980e48907f.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 0.5901268124580383
</pre>


<div id="orgdf544b8" class="figure">
<p><img src="./.ob-jupyter/baccb20d8b2b474e309db8f5d442f5c8612f78f8.png" alt="baccb20d8b2b474e309db8f5d442f5c8612f78f8.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 0.500286340713501
</pre>


<div id="orgf8158b5" class="figure">
<p><img src="./.ob-jupyter/c5ad9c793e26c1f8aed892f46418bb6e2653a8ae.png" alt="c5ad9c793e26c1f8aed892f46418bb6e2653a8ae.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 0.5042247176170349
</pre>


<div id="orgf21f38f" class="figure">
<p><img src="./.ob-jupyter/da75b14a4e65192437807be0c92826cc1839a8ea.png" alt="da75b14a4e65192437807be0c92826cc1839a8ea.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 0.3072202801704407
</pre>


<div id="orgb124637" class="figure">
<p><img src="./.ob-jupyter/c77ecaa5ed78ef364a76bb398153505d3574bf5e.png" alt="c77ecaa5ed78ef364a76bb398153505d3574bf5e.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 0.6142950654029846
</pre>


<div id="org7dbeff3" class="figure">
<p><img src="./.ob-jupyter/0a552a8f10f92443bfa3780b5781c2a2e1a2c60e.png" alt="0a552a8f10f92443bfa3780b5781c2a2e1a2c60e.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 0.3469279110431671
</pre>


<div id="org0974fad" class="figure">
<p><img src="./.ob-jupyter/c40ea2c9814ba8ec6c0a78bcc45d080497fbc52c.png" alt="c40ea2c9814ba8ec6c0a78bcc45d080497fbc52c.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 0.26291847229003906
</pre>


<div id="orge096a91" class="figure">
<p><img src="./.ob-jupyter/b0dd1f4e04f5d2285c45dee68196eba3e557a066.png" alt="b0dd1f4e04f5d2285c45dee68196eba3e557a066.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 0.27780410647392273
</pre>


<div id="org2187a90" class="figure">
<p><img src="./.ob-jupyter/a8136e64b2eff474ecbd7a36fc12bd12191985b3.png" alt="a8136e64b2eff474ecbd7a36fc12bd12191985b3.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org6b55412" class="outline-4">
<h4 id="org6b55412"><span class="section-number-4">2.1.4.</span> <span class="todo TODO">TODO</span> Initialization 0 Weights</h4>
<div class="outline-text-4" id="text-2-1-4">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torchvision
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np
<span style="color: #531ab6;">import</span> torch.nn.init <span style="color: #531ab6;">as</span> init

<span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #005e8b;">layer</span> = nn.Linear<span style="color: #000000;">(</span>layer_sizes<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">]</span>, layer_sizes<span style="color: #dd22dd;">[</span>i+1<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>layer<span style="color: #000000;">)</span>
            <span style="color: #595959;"># </span><span style="color: #595959;">Apply Xavier Normal initialization to each layer</span>
            init.uniform_<span style="color: #000000;">(</span>layer.weight<span style="color: #000000;">)</span>
            <span style="color: #595959;"># </span><span style="color: #595959;">Set biases to zero (optional)</span>
            init.uniform_<span style="color: #000000;">(</span>layer.bias<span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

<span style="color: #531ab6;">def</span> <span style="color: #721045;">create_and_train_model</span><span style="color: #000000;">(</span>n_layers, neurons_per_layer, epochs, train_loader<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Create layer sizes array</span>
    <span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>neurons_per_layer<span style="color: #000000;">]</span> * n_layers + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Initialize the model</span>
    <span style="color: #005e8b;">model</span> = CustomNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=0.01, momentum=0.5<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Function to plot histogram of weights</span>
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">plot_weights</span><span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
        <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
            <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:
                all_weights.extend<span style="color: #000000;">(</span>param.data.cpu<span style="color: #dd22dd;">()</span>.numpy<span style="color: #dd22dd;">()</span>.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>
        plt.hist<span style="color: #000000;">(</span>all_weights, bins=50, alpha=0.7<span style="color: #000000;">)</span>
        plt.title<span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>
        plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
        plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
        plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
        plt.show<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights before training</span>
    plot_weights<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights Before Training'</span><span style="color: #000000;">)</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Training loop</span>
    <span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>epochs<span style="color: #000000;">)</span>:
        model.train<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
            optimizer.zero_grad<span style="color: #000000;">()</span>
            <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
            <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
            loss.backward<span style="color: #000000;">()</span>
            optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Epoch </span>{epoch+1}<span style="color: #3548cf;">, Loss: </span>{loss.item()}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

        <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights after each epoch</span>
        plot_weights<span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Histogram of Weights After Epoch </span>{epoch+1}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example usage:</span>
<span style="color: #005e8b;">batch_size_train</span> = 128
<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
    torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                               transform=transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                                   transforms.ToTensor<span style="color: #808000;">()</span>,
                                   transforms.Normalize<span style="color: #808000;">(</span><span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                               <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
    batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=100, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="orga337fc1" class="figure">
<p><img src="./.ob-jupyter/517c218e9baf5ef9e8e43a8c92b0e700d7f7a858.png" alt="517c218e9baf5ef9e8e43a8c92b0e700d7f7a858.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 2.243140459060669
</pre>


<div id="orgfd342d9" class="figure">
<p><img src="./.ob-jupyter/1bb37760f33c11381a38c89f8db38507e726e44a.png" alt="1bb37760f33c11381a38c89f8db38507e726e44a.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 2.178248167037964
</pre>


<div id="orgc329921" class="figure">
<p><img src="./.ob-jupyter/d9e75bfc2a0793a6b47c9b6f1a46e5d9e577ade7.png" alt="d9e75bfc2a0793a6b47c9b6f1a46e5d9e577ade7.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 1.886781096458435
</pre>


<div id="orgac62746" class="figure">
<p><img src="./.ob-jupyter/be71f43285b7715a1ed48ee06598f27d629eb2e7.png" alt="be71f43285b7715a1ed48ee06598f27d629eb2e7.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 1.5410761833190918
</pre>


<div id="org9cc2c12" class="figure">
<p><img src="./.ob-jupyter/0d280097e313fc21780a044ed13c98ece7655e5c.png" alt="0d280097e313fc21780a044ed13c98ece7655e5c.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 1.1767771244049072
</pre>


<div id="org5862a59" class="figure">
<p><img src="./.ob-jupyter/4ddb0d4fc7923c4bde4550361a1a31595bdb0213.png" alt="4ddb0d4fc7923c4bde4550361a1a31595bdb0213.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 0.906932532787323
</pre>


<div id="org77435f6" class="figure">
<p><img src="./.ob-jupyter/fc577c09ba07a8e0bb5b5ddd545fe1692a1dd1c3.png" alt="fc577c09ba07a8e0bb5b5ddd545fe1692a1dd1c3.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 0.7335243225097656
</pre>


<div id="org66d8957" class="figure">
<p><img src="./.ob-jupyter/7dc2157398d91c309c5e0419cefb74cd06a183f3.png" alt="7dc2157398d91c309c5e0419cefb74cd06a183f3.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 0.8196816444396973
</pre>


<div id="orga6ca688" class="figure">
<p><img src="./.ob-jupyter/0cdaf68531fc674e11ddd4e2b46b38980e48907f.png" alt="0cdaf68531fc674e11ddd4e2b46b38980e48907f.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 0.5901268124580383
</pre>


<div id="orge3bd7b5" class="figure">
<p><img src="./.ob-jupyter/baccb20d8b2b474e309db8f5d442f5c8612f78f8.png" alt="baccb20d8b2b474e309db8f5d442f5c8612f78f8.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 0.500286340713501
</pre>


<div id="org86b9903" class="figure">
<p><img src="./.ob-jupyter/c5ad9c793e26c1f8aed892f46418bb6e2653a8ae.png" alt="c5ad9c793e26c1f8aed892f46418bb6e2653a8ae.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 0.5042247176170349
</pre>


<div id="orgc424bf9" class="figure">
<p><img src="./.ob-jupyter/da75b14a4e65192437807be0c92826cc1839a8ea.png" alt="da75b14a4e65192437807be0c92826cc1839a8ea.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 0.3072202801704407
</pre>


<div id="orga380865" class="figure">
<p><img src="./.ob-jupyter/c77ecaa5ed78ef364a76bb398153505d3574bf5e.png" alt="c77ecaa5ed78ef364a76bb398153505d3574bf5e.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 0.6142950654029846
</pre>


<div id="org613b477" class="figure">
<p><img src="./.ob-jupyter/0a552a8f10f92443bfa3780b5781c2a2e1a2c60e.png" alt="0a552a8f10f92443bfa3780b5781c2a2e1a2c60e.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 0.3469279110431671
</pre>


<div id="org9ba3f9a" class="figure">
<p><img src="./.ob-jupyter/c40ea2c9814ba8ec6c0a78bcc45d080497fbc52c.png" alt="c40ea2c9814ba8ec6c0a78bcc45d080497fbc52c.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 0.26291847229003906
</pre>


<div id="org40af3d9" class="figure">
<p><img src="./.ob-jupyter/b0dd1f4e04f5d2285c45dee68196eba3e557a066.png" alt="b0dd1f4e04f5d2285c45dee68196eba3e557a066.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 0.27780410647392273
</pre>


<div id="org6bdbe70" class="figure">
<p><img src="./.ob-jupyter/a8136e64b2eff474ecbd7a36fc12bd12191985b3.png" alt="a8136e64b2eff474ecbd7a36fc12bd12191985b3.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org34c233a" class="outline-3">
<h3 id="org34c233a"><span class="section-number-3">2.2.</span> 3 Layer 64 Neurons</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-orgd79f797" class="outline-4">
<h4 id="orgd79f797"><span class="section-number-4">2.2.1.</span> Normal Distribution (0,1)</h4>
<div class="outline-text-4" id="text-2-2-1">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #005e8b;">layer</span> = nn.Linear<span style="color: #000000;">(</span>layer_sizes<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">]</span>, layer_sizes<span style="color: #dd22dd;">[</span>i+1<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>layer<span style="color: #000000;">)</span>
            init.normal_<span style="color: #000000;">(</span>layer.weight<span style="color: #000000;">)</span>
            init.normal_<span style="color: #000000;">(</span>layer.bias<span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=25, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="org3533950" class="figure">
<p><img src="./.ob-jupyter/8e7b93460fbc23df16a2ad726298dd3ccfc57bf3.png" alt="8e7b93460fbc23df16a2ad726298dd3ccfc57bf3.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 1.7374809980392456
</pre>


<div id="orgc1f6ff4" class="figure">
<p><img src="./.ob-jupyter/9e923a21d47154453ff1106a7b784ff782317356.png" alt="9e923a21d47154453ff1106a7b784ff782317356.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 1.1774097681045532
</pre>


<div id="org851a950" class="figure">
<p><img src="./.ob-jupyter/a75028cde959c9692902bcb09e0028ce36eb47e0.png" alt="a75028cde959c9692902bcb09e0028ce36eb47e0.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 0.9354411959648132
</pre>


<div id="org9e393fb" class="figure">
<p><img src="./.ob-jupyter/39dd3a312f9e22227bacc77640ea538a13dd4c35.png" alt="39dd3a312f9e22227bacc77640ea538a13dd4c35.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 0.7064132690429688
</pre>


<div id="org1666c82" class="figure">
<p><img src="./.ob-jupyter/420864f938cc740e9dda2f5147a2926bc894f1f3.png" alt="420864f938cc740e9dda2f5147a2926bc894f1f3.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 0.7849247455596924
</pre>


<div id="orgb9f6a07" class="figure">
<p><img src="./.ob-jupyter/68532a103c6202477ab2d9b552523e2a5fb07938.png" alt="68532a103c6202477ab2d9b552523e2a5fb07938.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 0.6170485615730286
</pre>


<div id="org9a6af01" class="figure">
<p><img src="./.ob-jupyter/b9bc9ccf1ec281c1796f2de1111788880d59122f.png" alt="b9bc9ccf1ec281c1796f2de1111788880d59122f.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 0.671164333820343
</pre>


<div id="org2771fa3" class="figure">
<p><img src="./.ob-jupyter/08d25a93490794e113476f7a5e20e1febe2b5d9c.png" alt="08d25a93490794e113476f7a5e20e1febe2b5d9c.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 0.7070703506469727
</pre>


<div id="orgef5167f" class="figure">
<p><img src="./.ob-jupyter/64b59f4dc1f5e62ac594ff00fd58f951848735b7.png" alt="64b59f4dc1f5e62ac594ff00fd58f951848735b7.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 0.6183724999427795
</pre>


<div id="orgb671b9f" class="figure">
<p><img src="./.ob-jupyter/0484201c7b1273bc7e1cfbb9758f29f9253075f1.png" alt="0484201c7b1273bc7e1cfbb9758f29f9253075f1.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 0.5834630131721497
</pre>


<div id="orgb345f3b" class="figure">
<p><img src="./.ob-jupyter/696ec0da3ae7bd48cc87b537a4fef1c7a304bd25.png" alt="696ec0da3ae7bd48cc87b537a4fef1c7a304bd25.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 0.6377518773078918
</pre>


<div id="org117b311" class="figure">
<p><img src="./.ob-jupyter/8e35c4010ba11295acff758bee5baaf1a9729502.png" alt="8e35c4010ba11295acff758bee5baaf1a9729502.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 0.4866362512111664
</pre>


<div id="orgd5c749c" class="figure">
<p><img src="./.ob-jupyter/1e8e74a2833646dc2875db605c3d253b8a3f9727.png" alt="1e8e74a2833646dc2875db605c3d253b8a3f9727.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 0.5534214973449707
</pre>


<div id="orga54a435" class="figure">
<p><img src="./.ob-jupyter/56a8290099e733cea81554e45f829a82d7f2f6dd.png" alt="56a8290099e733cea81554e45f829a82d7f2f6dd.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 0.33660563826560974
</pre>


<div id="org85a13b4" class="figure">
<p><img src="./.ob-jupyter/cf929bbb27e800f1bedac0c155390fd1e76f8080.png" alt="cf929bbb27e800f1bedac0c155390fd1e76f8080.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 0.47805359959602356
</pre>


<div id="org7210eb2" class="figure">
<p><img src="./.ob-jupyter/e9f147dee6b4bfbcc0c83c48f1513e41513b221f.png" alt="e9f147dee6b4bfbcc0c83c48f1513e41513b221f.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 0.48472437262535095
</pre>


<div id="org4c3f470" class="figure">
<p><img src="./.ob-jupyter/fe9bb62328c87d2334769152888768d153ae6b94.png" alt="fe9bb62328c87d2334769152888768d153ae6b94.png" />
</p>
</div>
<pre class="example">
Epoch 17, Loss: 0.4368095397949219
</pre>


<div id="org846483b" class="figure">
<p><img src="./.ob-jupyter/d523e51504e096495de23134bd0338c7e71d619b.png" alt="d523e51504e096495de23134bd0338c7e71d619b.png" />
</p>
</div>
<pre class="example">
Epoch 18, Loss: 0.46487340331077576
</pre>


<div id="org3e5156f" class="figure">
<p><img src="./.ob-jupyter/0acfde7335d39c03cfc82ce07ea5548aaf895a66.png" alt="0acfde7335d39c03cfc82ce07ea5548aaf895a66.png" />
</p>
</div>
<pre class="example">
Epoch 19, Loss: 0.5110486149787903
</pre>


<div id="org0cb8f03" class="figure">
<p><img src="./.ob-jupyter/735620d18e01ef3a36c315c4c90d65d5c98a6ab9.png" alt="735620d18e01ef3a36c315c4c90d65d5c98a6ab9.png" />
</p>
</div>
<pre class="example">
Epoch 20, Loss: 0.5361409187316895
</pre>


<div id="org2506c02" class="figure">
<p><img src="./.ob-jupyter/980253215a1d71270a024df92acdd10fe0adf266.png" alt="980253215a1d71270a024df92acdd10fe0adf266.png" />
</p>
</div>
<pre class="example">
Epoch 21, Loss: 0.5355967879295349
</pre>


<div id="org62d8310" class="figure">
<p><img src="./.ob-jupyter/39e59e982ed624d1871a42305809969879d1401f.png" alt="39e59e982ed624d1871a42305809969879d1401f.png" />
</p>
</div>
<pre class="example">
Epoch 22, Loss: 0.36756324768066406
</pre>


<div id="orgeae53a9" class="figure">
<p><img src="./.ob-jupyter/da44593f70f0763bc82c7817bb4ac122fad90468.png" alt="da44593f70f0763bc82c7817bb4ac122fad90468.png" />
</p>
</div>
<pre class="example">
Epoch 23, Loss: 0.49373146891593933
</pre>


<div id="orgf727eb7" class="figure">
<p><img src="./.ob-jupyter/c5f442aa4abb7b90ff0174b705a82b77c4f58333.png" alt="c5f442aa4abb7b90ff0174b705a82b77c4f58333.png" />
</p>
</div>
<pre class="example">
Epoch 24, Loss: 0.36973071098327637
</pre>


<div id="orgf96e541" class="figure">
<p><img src="./.ob-jupyter/9f539513cba0f0e054b85e4f27c5c9bb1f91961d.png" alt="9f539513cba0f0e054b85e4f27c5c9bb1f91961d.png" />
</p>
</div>
<pre class="example">
Epoch 25, Loss: 0.46977075934410095
</pre>


<div id="orgcb36726" class="figure">
<p><img src="./.ob-jupyter/3fd7c9069e64f58941072e31d1639d1bb21dc7b5.png" alt="3fd7c9069e64f58941072e31d1639d1bb21dc7b5.png" />
</p>
</div>
<pre class="example">
&lt;Figure size 432x288 with 0 Axes&gt;
</pre>
</div>
</div>
<div id="outline-container-orge0ecf15" class="outline-4">
<h4 id="orge0ecf15"><span class="section-number-4">2.2.2.</span> Uniform [0,1]</h4>
<div class="outline-text-4" id="text-2-2-2">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #005e8b;">layer</span> = nn.Linear<span style="color: #000000;">(</span>layer_sizes<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">]</span>, layer_sizes<span style="color: #dd22dd;">[</span>i+1<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>layer<span style="color: #000000;">)</span>
            init.uniform_<span style="color: #000000;">(</span>layer.weight<span style="color: #000000;">)</span>
            init.uniform_<span style="color: #000000;">(</span>layer.bias<span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=25, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="org8d4216d" class="figure">
<p><img src="./.ob-jupyter/43818bb8fed0d2d8d01865cbc35c55a5730e9d90.png" alt="43818bb8fed0d2d8d01865cbc35c55a5730e9d90.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 2.2895634174346924
</pre>


<div id="org89b21ca" class="figure">
<p><img src="./.ob-jupyter/327febc2ed85cfdd104f895c5844d609d302ebe2.png" alt="327febc2ed85cfdd104f895c5844d609d302ebe2.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 2.292506217956543
</pre>


<div id="orgac11fd2" class="figure">
<p><img src="./.ob-jupyter/1993f7f80baf65953445602c6c8e1dfffe6d645b.png" alt="1993f7f80baf65953445602c6c8e1dfffe6d645b.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 2.320305585861206
</pre>


<div id="orge617be6" class="figure">
<p><img src="./.ob-jupyter/7e514794ba0e1b577545d2eb1c07accd8ba2a8ee.png" alt="7e514794ba0e1b577545d2eb1c07accd8ba2a8ee.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 2.30061936378479
</pre>


<div id="orgefd6079" class="figure">
<p><img src="./.ob-jupyter/81f038ec07a78fb4a6001971c62fd500d8ddd716.png" alt="81f038ec07a78fb4a6001971c62fd500d8ddd716.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 2.3356828689575195
</pre>


<div id="org1b57b13" class="figure">
<p><img src="./.ob-jupyter/74c73dc133a2abeb111bbbb820f95120235cbd09.png" alt="74c73dc133a2abeb111bbbb820f95120235cbd09.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 2.284691095352173
</pre>


<div id="orgcc1bb36" class="figure">
<p><img src="./.ob-jupyter/03a1a56f7e827ad3eea262cd139fb9e989019fdc.png" alt="03a1a56f7e827ad3eea262cd139fb9e989019fdc.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 2.3002583980560303
</pre>


<div id="org5451841" class="figure">
<p><img src="./.ob-jupyter/50c4db28e448d54a158b1a9c703e7fcb8185d3ff.png" alt="50c4db28e448d54a158b1a9c703e7fcb8185d3ff.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 2.297882080078125
</pre>


<div id="orgd55f80f" class="figure">
<p><img src="./.ob-jupyter/c8ed4ff045676630e0d958c73d7899ded19e0807.png" alt="c8ed4ff045676630e0d958c73d7899ded19e0807.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 2.303570032119751
</pre>


<div id="orged8cd9d" class="figure">
<p><img src="./.ob-jupyter/7c69f8fdb33aeac893186c62695a1f228c7dc102.png" alt="7c69f8fdb33aeac893186c62695a1f228c7dc102.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 2.287445306777954
</pre>


<div id="orgbac6489" class="figure">
<p><img src="./.ob-jupyter/a479c1e197313595180a1e4ceb8ef3576af2983e.png" alt="a479c1e197313595180a1e4ceb8ef3576af2983e.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 2.3249855041503906
</pre>


<div id="orga9a8dfe" class="figure">
<p><img src="./.ob-jupyter/cbefff6daaf0bbb1fe3ebcebaed688819788b949.png" alt="cbefff6daaf0bbb1fe3ebcebaed688819788b949.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 2.3124051094055176
</pre>


<div id="org392461f" class="figure">
<p><img src="./.ob-jupyter/ac7a8c195e2a62f486e555942ce947bb067ef770.png" alt="ac7a8c195e2a62f486e555942ce947bb067ef770.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 2.3211677074432373
</pre>


<div id="org5a4b36c" class="figure">
<p><img src="./.ob-jupyter/712e4fa7c6265026a3c78c511d1c9112b314c3c6.png" alt="712e4fa7c6265026a3c78c511d1c9112b314c3c6.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 2.3177366256713867
</pre>


<div id="org250046c" class="figure">
<p><img src="./.ob-jupyter/3ed4721c221e71e6e36b7cf0ab8627aabe74b4db.png" alt="3ed4721c221e71e6e36b7cf0ab8627aabe74b4db.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 2.30816912651062
</pre>


<div id="orgaf70c49" class="figure">
<p><img src="./.ob-jupyter/aefa917f2a390d64aa1ae3918814b16b503528cb.png" alt="aefa917f2a390d64aa1ae3918814b16b503528cb.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 2.2896058559417725
</pre>


<div id="orgf0bbba4" class="figure">
<p><img src="./.ob-jupyter/bda744b48427b492a19a983d92f78344463369fd.png" alt="bda744b48427b492a19a983d92f78344463369fd.png" />
</p>
</div>
<pre class="example">
Epoch 17, Loss: 2.3232648372650146
</pre>


<div id="org62f5de0" class="figure">
<p><img src="./.ob-jupyter/94abb12c5fe9967321b0eb5be1beb094bbc30e04.png" alt="94abb12c5fe9967321b0eb5be1beb094bbc30e04.png" />
</p>
</div>
<pre class="example">
Epoch 18, Loss: 2.309356451034546
</pre>


<div id="org9e0ae2c" class="figure">
<p><img src="./.ob-jupyter/e5891fef6b0dee88ba3185518c08a572f0189a62.png" alt="e5891fef6b0dee88ba3185518c08a572f0189a62.png" />
</p>
</div>
<pre class="example">
Epoch 19, Loss: 2.33986496925354
</pre>


<div id="org879ca51" class="figure">
<p><img src="./.ob-jupyter/0188dbee929f1fd6f4170d3974d8fcf18f87cf59.png" alt="0188dbee929f1fd6f4170d3974d8fcf18f87cf59.png" />
</p>
</div>
<pre class="example">
Epoch 20, Loss: 2.3159825801849365
</pre>


<div id="org9d67fb0" class="figure">
<p><img src="./.ob-jupyter/47fd39c9a1a24640d52ef9bb9437bbedf91d9d33.png" alt="47fd39c9a1a24640d52ef9bb9437bbedf91d9d33.png" />
</p>
</div>
<pre class="example">
Epoch 21, Loss: 2.3211779594421387
</pre>


<div id="orgcd37fdf" class="figure">
<p><img src="./.ob-jupyter/3165156804bff80fce545a4f84fd862503a6b21e.png" alt="3165156804bff80fce545a4f84fd862503a6b21e.png" />
</p>
</div>
<pre class="example">
Epoch 22, Loss: 2.303123950958252
</pre>


<div id="orgf7551ce" class="figure">
<p><img src="./.ob-jupyter/17e5f8a3da2d8e60d86646b5d0141fd02c090ba1.png" alt="17e5f8a3da2d8e60d86646b5d0141fd02c090ba1.png" />
</p>
</div>
<pre class="example">
Epoch 23, Loss: 2.359586477279663
</pre>


<div id="org32a4d67" class="figure">
<p><img src="./.ob-jupyter/0c45265a349c058382ac50ed91bac3cc938739b4.png" alt="0c45265a349c058382ac50ed91bac3cc938739b4.png" />
</p>
</div>
<pre class="example">
Epoch 24, Loss: 2.296290874481201
</pre>


<div id="org45f6544" class="figure">
<p><img src="./.ob-jupyter/f088719e80f420719fa44d42a061baccddfa830e.png" alt="f088719e80f420719fa44d42a061baccddfa830e.png" />
</p>
</div>
<pre class="example">
Epoch 25, Loss: 2.316251039505005
</pre>


<div id="org36aea39" class="figure">
<p><img src="./.ob-jupyter/3b8f78a6abe9d92fcbf5825d72d349f6822f9f8d.png" alt="3b8f78a6abe9d92fcbf5825d72d349f6822f9f8d.png" />
</p>
</div>
<pre class="example">
&lt;Figure size 432x288 with 0 Axes&gt;
</pre>
</div>
</div>
<div id="outline-container-org0a68ccb" class="outline-4">
<h4 id="org0a68ccb"><span class="section-number-4">2.2.3.</span> Zeros</h4>
<div class="outline-text-4" id="text-2-2-3">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #005e8b;">layer</span> = nn.Linear<span style="color: #000000;">(</span>layer_sizes<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">]</span>, layer_sizes<span style="color: #dd22dd;">[</span>i+1<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>layer<span style="color: #000000;">)</span>
            init.zeros_<span style="color: #000000;">(</span>layer.weight<span style="color: #000000;">)</span>
            init.zeros_<span style="color: #000000;">(</span>layer.bias<span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=25, train_loader=train_loader<span style="color: #000000;">)</span>
</pre>
</div>


<div id="org3a2c3d0" class="figure">
<p><img src="./.ob-jupyter/e87a22b777ac26a2a42827706b8ce3ff76e167a1.png" alt="e87a22b777ac26a2a42827706b8ce3ff76e167a1.png" />
</p>
</div>
<pre class="example">
Epoch 1, Loss: 2.294684410095215
</pre>


<div id="orgab2556d" class="figure">
<p><img src="./.ob-jupyter/dc10190baae90a23b1bab58ebae525059412bc72.png" alt="dc10190baae90a23b1bab58ebae525059412bc72.png" />
</p>
</div>
<pre class="example">
Epoch 2, Loss: 2.311328411102295
</pre>


<div id="org919c279" class="figure">
<p><img src="./.ob-jupyter/0de5223c9933cff886245fa2eee410496da19f4f.png" alt="0de5223c9933cff886245fa2eee410496da19f4f.png" />
</p>
</div>
<pre class="example">
Epoch 3, Loss: 2.2964327335357666
</pre>


<div id="org55a7abc" class="figure">
<p><img src="./.ob-jupyter/bca0fae45f86fa00799375b074f901ada18e8c60.png" alt="bca0fae45f86fa00799375b074f901ada18e8c60.png" />
</p>
</div>
<pre class="example">
Epoch 4, Loss: 2.320838689804077
</pre>


<div id="org55c4e5d" class="figure">
<p><img src="./.ob-jupyter/df7d358b561634986a0a1c5ac8a434ffbb076988.png" alt="df7d358b561634986a0a1c5ac8a434ffbb076988.png" />
</p>
</div>
<pre class="example">
Epoch 5, Loss: 2.2916862964630127
</pre>


<div id="orge65a522" class="figure">
<p><img src="./.ob-jupyter/4b657f915b1790bf501f1030183a522f0dbf8061.png" alt="4b657f915b1790bf501f1030183a522f0dbf8061.png" />
</p>
</div>
<pre class="example">
Epoch 6, Loss: 2.302931547164917
</pre>


<div id="orge417de2" class="figure">
<p><img src="./.ob-jupyter/b11af733269e3973962d2136f6c9840867a66bc1.png" alt="b11af733269e3973962d2136f6c9840867a66bc1.png" />
</p>
</div>
<pre class="example">
Epoch 7, Loss: 2.3088455200195312
</pre>


<div id="orgfcd2d55" class="figure">
<p><img src="./.ob-jupyter/c2c9ce66d20fbf341a7c2843d9877ed49bd4417f.png" alt="c2c9ce66d20fbf341a7c2843d9877ed49bd4417f.png" />
</p>
</div>
<pre class="example">
Epoch 8, Loss: 2.302312135696411
</pre>


<div id="org2ca69f4" class="figure">
<p><img src="./.ob-jupyter/5c2ff112754a629493df387c9fb427b5a1fa756c.png" alt="5c2ff112754a629493df387c9fb427b5a1fa756c.png" />
</p>
</div>
<pre class="example">
Epoch 9, Loss: 2.3111631870269775
</pre>


<div id="org0225ab8" class="figure">
<p><img src="./.ob-jupyter/99d1d3ff0bae9060c3e1ed725a682a63f8136aa2.png" alt="99d1d3ff0bae9060c3e1ed725a682a63f8136aa2.png" />
</p>
</div>
<pre class="example">
Epoch 10, Loss: 2.281331777572632
</pre>


<div id="org12c4cf7" class="figure">
<p><img src="./.ob-jupyter/3bd675987e7966a0a60adb042c9a3f4b11322aa7.png" alt="3bd675987e7966a0a60adb042c9a3f4b11322aa7.png" />
</p>
</div>
<pre class="example">
Epoch 11, Loss: 2.3043408393859863
</pre>


<div id="org5a60b72" class="figure">
<p><img src="./.ob-jupyter/68c08ef0f199742931581306d9d01dd250d70cd5.png" alt="68c08ef0f199742931581306d9d01dd250d70cd5.png" />
</p>
</div>
<pre class="example">
Epoch 12, Loss: 2.3050715923309326
</pre>


<div id="orgf3ffd9b" class="figure">
<p><img src="./.ob-jupyter/f5de2a727563a4934a7b0a20a75c949d95307e54.png" alt="f5de2a727563a4934a7b0a20a75c949d95307e54.png" />
</p>
</div>
<pre class="example">
Epoch 13, Loss: 2.295111894607544
</pre>


<div id="orgffd2b72" class="figure">
<p><img src="./.ob-jupyter/1ab37d350d73aa506ef9c7bdbea6b4d8fda68ced.png" alt="1ab37d350d73aa506ef9c7bdbea6b4d8fda68ced.png" />
</p>
</div>
<pre class="example">
Epoch 14, Loss: 2.3030707836151123
</pre>


<div id="org123d330" class="figure">
<p><img src="./.ob-jupyter/6df63f5b1d6c0df24861cbf1fdeaebe18d015757.png" alt="6df63f5b1d6c0df24861cbf1fdeaebe18d015757.png" />
</p>
</div>
<pre class="example">
Epoch 15, Loss: 2.284855604171753
</pre>


<div id="org38a5c6e" class="figure">
<p><img src="./.ob-jupyter/1b4bfc1677bc716cb177382e07b3d7f9fb71f335.png" alt="1b4bfc1677bc716cb177382e07b3d7f9fb71f335.png" />
</p>
</div>
<pre class="example">
Epoch 16, Loss: 2.3004000186920166
</pre>


<div id="org4e17739" class="figure">
<p><img src="./.ob-jupyter/0519182cacca4acc2fd042f629836f92b4487afe.png" alt="0519182cacca4acc2fd042f629836f92b4487afe.png" />
</p>
</div>
<pre class="example">
Epoch 17, Loss: 2.308497667312622
</pre>


<div id="orga5e34d8" class="figure">
<p><img src="./.ob-jupyter/ebb8dd8e75c36996551de499303ac426b5332a1a.png" alt="ebb8dd8e75c36996551de499303ac426b5332a1a.png" />
</p>
</div>
<pre class="example">
Epoch 18, Loss: 2.306018829345703
</pre>


<div id="org1a33513" class="figure">
<p><img src="./.ob-jupyter/adb0a0e5cd68934637298c0b6a909333386f5957.png" alt="adb0a0e5cd68934637298c0b6a909333386f5957.png" />
</p>
</div>
<pre class="example">
Epoch 19, Loss: 2.3032310009002686
</pre>


<div id="orgdfdfca8" class="figure">
<p><img src="./.ob-jupyter/447d8d059e471be36b8d81231d939670ea7b7248.png" alt="447d8d059e471be36b8d81231d939670ea7b7248.png" />
</p>
</div>
<pre class="example">
Epoch 20, Loss: 2.2975432872772217
</pre>


<div id="orgce7ef25" class="figure">
<p><img src="./.ob-jupyter/4efe0e598c29a8227a91f2e3fd0e0cb3778862ce.png" alt="4efe0e598c29a8227a91f2e3fd0e0cb3778862ce.png" />
</p>
</div>
<pre class="example">
Epoch 21, Loss: 2.301711320877075
</pre>


<div id="org2e59bfd" class="figure">
<p><img src="./.ob-jupyter/5ce64ec9f029a1902a6f4bde9083e43d20126ebb.png" alt="5ce64ec9f029a1902a6f4bde9083e43d20126ebb.png" />
</p>
</div>
<pre class="example">
Epoch 22, Loss: 2.302415609359741
</pre>


<div id="org8ee20ab" class="figure">
<p><img src="./.ob-jupyter/6fd6dc75b968829142d0a998d8f137a2c34d20a1.png" alt="6fd6dc75b968829142d0a998d8f137a2c34d20a1.png" />
</p>
</div>
<pre class="example">
Epoch 23, Loss: 2.2926647663116455
</pre>


<div id="org5511389" class="figure">
<p><img src="./.ob-jupyter/ff4e110da8de287ae5fe66757a6f02abfd55eacf.png" alt="ff4e110da8de287ae5fe66757a6f02abfd55eacf.png" />
</p>
</div>
<pre class="example">
Epoch 24, Loss: 2.3220808506011963
</pre>


<div id="org2751383" class="figure">
<p><img src="./.ob-jupyter/f7201087b8407dc1ceb7d11636f676d0273e303e.png" alt="f7201087b8407dc1ceb7d11636f676d0273e303e.png" />
</p>
</div>
<pre class="example">
Epoch 25, Loss: 2.2965810298919678
</pre>


<div id="orge0fd22a" class="figure">
<p><img src="./.ob-jupyter/7c5e0a99a36a5cbe7f86f5ddd9b51a0b1320bc58.png" alt="7c5e0a99a36a5cbe7f86f5ddd9b51a0b1320bc58.png" />
</p>
</div>
<pre class="example">
&lt;Figure size 432x288 with 0 Axes&gt;
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orge01879c" class="outline-2">
<h2 id="orge01879c"><span class="section-number-2">3.</span> Definitions</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org3abf84b" class="outline-3">
<h3 id="org3abf84b"><span class="section-number-3">3.1.</span> Neural Network</h3>
</div>



<div id="outline-container-org91b44e9" class="outline-3">
<h3 id="org91b44e9"><span class="section-number-3">3.2.</span> Two Layer MF Network</h3>
<div class="outline-text-3" id="text-3-2">
<p>
\[ \hat{\mathbf{y}}_{2-\text{layer}}(x ; W) = \frac{1}{n} \sum_{i = 1}^{n} w_{2,i}\sigma(\left\langle w_{1,i}, x \right\rangle) \]
</p>
</div>
</div>
<div id="outline-container-org42b95c6" class="outline-3">
<h3 id="org42b95c6"><span class="section-number-3">3.3.</span> Infinite Width Representation of Two Layer Neural Network</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Rather than use an average sum,
\[ \hat{\mathbf{y}}_{2-\text{layer}}(x ; W) = \frac{1}{n} \sum_{i = 1}^{n} w_{2,i}\sigma(\left\langle w_{1,i}, x \right\rangle) \]
</p>

<p>
Use an average integral
</p>
</div>
</div>
<div id="outline-container-org80223e5" class="outline-3">
<h3 id="org80223e5"><span class="section-number-3">3.4.</span> Number of Layers in a Neural Network</h3>
<div class="outline-text-3" id="text-3-4">
<p>
This number has been defined in two separate ways, either:
</p>

<ul class="org-ul">
<li><p>
The number of weight matrices
</p>

<p>
<a href="file:///home/zjabbar/books/literature/mean_field_theory/A Rigorous Framework for the Mean Field Limit23.pdf">A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks Phan-Minh Nguyen and Huy Tuan Pham 2023</a>
</p></li>

<li>The number of hidden layers</li>
</ul>
</div>
</div>
<div id="outline-container-orgff25d90" class="outline-3">
<h3 id="orgff25d90"><span class="section-number-3">3.5.</span> Mean Field Limit</h3>
<div class="outline-text-3" id="text-3-5">
<p>
The Mean Field Limit of a Neural Network is the limit of the emperical measure of the neural network parameters.
</p>
</div>
</div>
<div id="outline-container-org5c0f5b9" class="outline-3">
<h3 id="org5c0f5b9"><span class="section-number-3">3.6.</span> Mean Field Deterministic Partial Differential Equations</h3>
</div>
</div>
<div id="outline-container-orgbc25440" class="outline-2">
<h2 id="orgbc25440"><span class="section-number-2">4.</span> Regimes</h2>
<div class="outline-text-2" id="text-4">
<p>
We care about the following parameters,
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Variable</th>
<th scope="col" class="org-left">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\( n \)</td>
<td class="org-left">Sample Size</td>
</tr>

<tr>
<td class="org-left">\( N \)</td>
<td class="org-left">Number of Neurons</td>
</tr>

<tr>
<td class="org-left">\( D \)</td>
<td class="org-left">Dimension of Input Space</td>
</tr>

<tr>
<td class="org-left">\( \overline{k} \)</td>
<td class="org-left">Number of SGD Iterations</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>\( N = O(1), n \geq D, \overline{k} >> n \) (Small Network)
Each data point is visited many times
Number of Neurons is relatively small
Studied using Spin Glass
Paper in Neurips by Aubin et al. 2018</li>

<li>\( N \geq n^{c}, \overline{k} \geq n^{d} \) (Kernel, Overparameterized)
Paper by Jacot 2018 and Du 2018
One may linearize the network around the neural network
Does not perform feature learning
Similar to kernel ridge regression with kernel determined by initialization</li>

<li>\( N \geq D, D \leq \overline{k} \leq n, \overline{k} << ND \) (Mean Field Regime)
Mei 2018,
Rotkoff Vanden
Chizat Bach
Each data point is visited once
MF-Scalled networks perform feature learning due to nonlinear dynamics and weights moving away from initialization</li>
</ul>
</div>
<div id="outline-container-org56de740" class="outline-3">
<h3 id="org56de740"><span class="section-number-3">4.1.</span> Papers</h3>
<div class="outline-text-3" id="text-4-1">
<p>
<b>Emperical Work on MF Multilayer</b>
</p>
<ul class="org-ul">
<li></li>
</ul>


<p>
<b>NTK</b>
Arthur Jacot, Franck Gabriel, and Clement Hongler, Neural tangent kernel: Convergence and generalization in neural networks, Advances in neural information processing systems 31, 2018, pp. 8580–8589.
</p>

<p>
Lenaic Chizat and Francis Bach, A note on lazy training in supervised differentiable programming, arXiv preprint arXiv:1812.07956 (2018).
</p>

<p>
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, Gradient descent provably optimizes over-
parameterized neural networks, International conference on learning representations, 2019.
</p>

<p>
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, Stochastic gradient descent optimizes over-parameterized deep relu networks, arXiv preprint arXiv:1811.08888 (2018).
</p>

<p>
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, A convergence theory for deep learning via over-parameterization, arXiv preprint arXiv:1811.03962 (2018).
</p>

<p>
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey Pennington, Wide neural networks of any depth evolve as linear models under gradient descent, arXiv preprint arXiv:1902.06720 (2019).
</p>

<p>
<b>Scaling with Feature Learning</b>
Eugene A Golikov, Dynamically stable infinite-width limits of neural classifiers, arXiv preprint arXiv:2006.06574 (2020).
</p>
</div>
</div>
</div>
<div id="outline-container-orga82b2d0" class="outline-2">
<h2 id="orga82b2d0"><span class="section-number-2">5.</span> MF Model Performance</h2>
<div class="outline-text-2" id="text-5">
<p>
Ngu19 - Phan-Minh Nguyen, Mean field limit of the learning dynamics of multilayer neural networks, arXiv preprint
arXiv:1902.02880 (2019).
</p>
</div>
</div>
<div id="outline-container-orga90f72c" class="outline-2">
<h2 id="orga90f72c"><span class="section-number-2">6.</span> Mean Field Analysis</h2>
<div class="outline-text-2" id="text-6">
<p>
Mean-field theory studies the behavior of high-dimensional random models by employing a simpler model that approximated the original by averaging over degrees of freedom. In many papers, a common parameter to scale is the learning rate (hence the SGD iterations).
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Scaling Factor</th>
<th scope="col" class="org-left">Literature</th>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-left">Take away</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\( 1/N \)</td>
<td class="org-left">Law of Large Numbers</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">\( 1 / \sqrt{N} \)</td>
<td class="org-left">Central Limit Theorem</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
Law of Large Numbers (Scaling \( 1 / N \))
</p>
</div>
<div id="outline-container-org5a5c8c9" class="outline-3">
<h3 id="org5a5c8c9"><span class="section-number-3">6.1.</span> \( 1 / N \)</h3>
</div>

<div id="outline-container-org146d0c6" class="outline-3">
<h3 id="org146d0c6"><span class="section-number-3">6.2.</span> \( 1 / \sqrt{N} \)</h3>
</div>
</div>
<div id="outline-container-orgc1ee65e" class="outline-2">
<h2 id="orgc1ee65e"><span class="section-number-2">7.</span> Neuronal Embedding</h2>
</div>

<div id="outline-container-org50434c7" class="outline-2">
<h2 id="org50434c7"><span class="section-number-2">8.</span> IID Initialization Cause Strong Degeneracy Effects</h2>
</div>

<div id="outline-container-org97fea27" class="outline-2">
<h2 id="org97fea27"><span class="section-number-2">9.</span> Global Convergence</h2>
</div>

<div id="outline-container-org6a3a741" class="outline-2">
<h2 id="org6a3a741"><span class="section-number-2">10.</span> References</h2>
<div class="outline-text-2" id="text-10">
<p>
(Cedric Gerbelot and Emanuele Troiani and Francesca Mignacco and Florent Krzakala and Lenka Zdeborova, 2023)
</p>
</div>
</div>
</div>
</body>
</html>
