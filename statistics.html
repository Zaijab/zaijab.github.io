<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-06-23 Sun 23:43 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Statistics</title>
<meta name="author" content="Zain Jabbar" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<title></title><link rel="stylesheet" href="static/css/site.css" type="text/css"/>
<header><div class="menu"><ul>
<li><a href="/">/</a></li>
<li><a href="/about">/about</a></li>
<li><a href="/posts">/posts</a></li></ul></div></header><script src="static/js/nastaliq.js"></script>

<script>
  MathJax = {
    loader: {
      load: ['[custom]/xypic.js'],
      paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}
    },
    tex: {
      packages: {'[+]': ['xypic']},
      macros: {
        RR: "{\bf R}",
        bold: ["{\bf #1}",1]
      }
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.6/tex-chtml.js"></script>
</head>
<body>
<div id="content" class="content">
<div id="outline-container-org5518df1" class="outline-2">
<h2 id="org5518df1"><span class="section-number-2">1.</span> Probability</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org89dd03f" class="outline-3">
<h3 id="org89dd03f"><span class="section-number-3">1.1.</span> Intuition</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-org901a54f" class="outline-4">
<h4 id="org901a54f"><span class="section-number-4">1.1.1.</span> Logic of the Real World</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
Mathematics takes place in a deterministic framework. Theorems are not true "sometimes", they have been true since the inception of their axioms and definitions. They were true by the time you outline what it means to be true. To logically deduce something, one must begin with a base collection of axioms and use rules of inference which produce statements which are as true as the axioms. This approach is useful for mathematics, however this framework of deduction is almost never used in the real world. We cannot answer the following questions using logical deductions:
</p>

<ul class="org-ul">
<li>Will it rain tomorrow? Will it not rain tomorrow?</li>
<li>What company should I work for?</li>
<li>What shall I eat today?</li>
</ul>

<p>
The main reason why these (and many more) questions cannot be simply answered is due to the unreliability of information given in the real world.
</p>

<p>
We
</p>
</div>
</div>
<div id="outline-container-orgfece933" class="outline-4">
<h4 id="orgfece933"><span class="section-number-4">1.1.2.</span> Formalism</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
Now that we have a collection of rules by which we want our system of logic to follow, we may begin to formalize the process of using them mathematically. This formalism will give us multiple benefits:
</p>

<ul class="org-ul">
<li>Understanding the underpinnings of a "well-posed" problem</li>
<li>Notions of composability (i.e. how to correctly glue random processes together)</li>
<li>Reduction of text (Prove one theorem, apply to many problems)</li>
<li>Notions of Equivalence</li>
</ul>

<p>
Let us begin by looking at some problems in which this formalism would be useful.
</p>
</div>
<ol class="org-ol">
<li><a id="org0b59014"></a>Bertrand's Circle Paradox<br /></li>


<li><a id="orgfeaa962"></a>Random Variables which are Neither Discrete nor Continuous<br />
<div class="outline-text-5" id="text-1-1-2-2">
<p>
Let us consider a simple random process
</p>
</div>
</li>
<li><a id="org5d5b30a"></a>Probabilities over Tricky Sets<br /></li>


<li><a id="orgdb26041"></a>Distribution Functions<br /></li>


<li><a id="orgd163ad7"></a>Strong Law of Large Numbers<br />
<div class="outline-text-5" id="text-1-1-2-5">
<p>
One of the most intuitive ways to think about probability is that of frequentism. That is, if one were to perform a process a number of times, then the long run results will "tend" towards something. In truth, convergence is a tricky subject and we will revisit it. However, we will try to intuitively feel what these statements mean.
</p>

<p>
There are many ways in which a collection of random variables can "approach" another random variable. Suppose we flip a fair coin heads or tails, if we flip once and land heads should we conclude that the probability of landing heads was \( 100\% \)? No, we only care about the long run proportions. That is, how many times on average does the coin appear heads?
</p>

<p>
\[ \overline{X} = \frac{1}{n} \sum_{i = 1}^{n} X_{i} \to 50\% \]
</p>

<p>
In this frequentist scenario, we want our sample mean to not only have the same distributions as the true mean, but to converge in a somewhat smooth way.
</p>

<p>
There is a nuance at play with a family of random variables. A collection can approach a result as a group, but during each realization we cannot specify how each random variable is close to the convergent result.
</p>

<p>
The gold standard of convergence, convergence almost surely, allows you to have a bound on how far away an individual member is in the sequence. This means that when you go out and compute a number, that this number itself is actually close to the limit. Convergence in probability means that you cannot handle the number you computed, instead, you much compute the probability associated with encountering this number.
</p>

<p>
In addition to the other answers, here is a very simple yet formal example of the difference.
</p>

<p>
Consider a sequence of Bernoulli random variables  X1,X2&#x2026;Xn
  given by the pdf  p(X=1)=1n
 .
</p>

<p>
This sequence converges in probability to zero because the sequence of probabilities  P(X1),P(X2)&#x2026;P(Xn)
  has limit  0
 .
</p>

<p>
However, it does not converge almost surely to  0
 . Why? Consider all possible infinite sequences of  X
 , e.g.  1,0,1,1&#x2026;
  or  1,0,1,0,1&#x2026;
 , et cetera. In each of those sequences, the density of ones gets smaller and smaller, but never equals zero. That means that for any given sequence (or technically for a fraction of the possible sequences that has measure  1
 ), there is never a last  1
 —you can always find another  1
  further down the sequence.
</p>

<p>
These sorts of sequences can’t converge*, and if the possible sequences of  X
  do not converge, then  X
  does not converge almost surely.
</p>

<p>
*Why don’t these sequences converge? Take  ε=0.5
 . We want to show that there is some  N
  such that if  n&gt;N,Sn&lt;ε
 . But no matter how large an  N
  we take, there will always be some  n
  such that  n&gt;N
  and  Sn=1&gt;0.5
 . Therefore the sequence does not converge to  0
 .
</p>
</div>
</li>
<li><a id="org56800d7"></a>Convergence<br /></li>



<li><a id="org55ce682"></a>Random Vectors<br /></li>


<li><a id="org4974b57"></a>Brownian Motion<br />
<div class="outline-text-5" id="text-1-1-2-8">
<p>
While observing pollen under a microscope, Dr. Brown noticed that they tend to jiggle in a seemingly random way.
</p>

<p>
In order to justify how one 
</p>
</div>
</li>
<li><a id="org04d618e"></a>Stochastic Differential Equations<br /></li>


<li><a id="org159261e"></a>Equivalent Systems<br /></li>
</ol>
</div>
<div id="outline-container-orgc682c70" class="outline-4">
<h4 id="orgc682c70"><span class="section-number-4">1.1.3.</span> Parameterized Models</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
There are many examples of probability spaces. We may even collect some related spaces as an infinite family. 
</p>
</div>
<ol class="org-ol">
<li><a id="org47b49c0"></a>Normal Distribution<br /></li>

<li><a id="org4af3e8a"></a>Exponential Distribution<br /></li>

<li><a id="org917e811"></a><br /></li>
</ol>
</div>
<div id="outline-container-orgef5e7c6" class="outline-4">
<h4 id="orgef5e7c6"><span class="section-number-4">1.1.4.</span> Random Variables</h4>
<div class="outline-text-4" id="text-1-1-4">
<p>
Given 
</p>
</div>
</div>
<div id="outline-container-org20ebb02" class="outline-4">
<h4 id="org20ebb02"><span class="section-number-4">1.1.5.</span> Probability Inequalities</h4>
<div class="outline-text-4" id="text-1-1-5">
</div>
<ol class="org-ol">
<li><a id="org477995b"></a>Jensen's Inequality<br /></li>

<li><a id="orge1c0104"></a>Chebychev's Inequality<br /></li>

<li><a id="org37ec240"></a>Cantelli's Inequality<br /></li>
</ol>
</div>
</div>
<div id="outline-container-org59ae6a9" class="outline-3">
<h3 id="org59ae6a9"><span class="section-number-3">1.2.</span> Parametric Models</h3>
</div>



<div id="outline-container-orge734906" class="outline-3">
<h3 id="orge734906"><span class="section-number-3">1.3.</span> Summary</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-org2486c45" class="outline-4">
<h4 id="org2486c45"><span class="section-number-4">1.3.1.</span> Sample Space</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
The sample space is a set, normally denoted \( \Omega \). There are no restrictions associated with the set other than that it is a set. We will not dive too deep into the notions of set theory. However we may display some of the second order logical underpinnings of the theory.
</p>
</div>
<ol class="org-ol">
<li><a id="org7189f48"></a>Theorems of Set Theory<br />
<ol class="org-ol">
<li><a id="org2ab6cf3"></a>There Exists an Empty Set<br />
<div class="outline-text-6" id="text-1-3-1-1-1">
<div class="THEOREM" id="org12433f9">
<p>
\[ \exists! \emptyset \colon \forall x \colon x \not \in \emptyset \]
</p>

</div>

<div class="PROOF" id="org4791597">
\begin{lflalign}
\exists N \tag{Axiom of Infinity} \\
\exists x \colon \{ x \in N \mid x \neq x \} \tag{Axiom Schema of Comprehension}
\end{lflalign}

</div>
</div>
</li>
<li><a id="org711d43c"></a>The Cartesian Product Exists<br /></li>

<li><a id="orgda7a3f4"></a>Axioms of Set Theory<br />
<div class="outline-text-6" id="text-1-3-1-1-3">
<ul class="org-ul">
<li>Axiom of Extensionality</li>

<li>Axiom of Pairing</li>

<li>Axiom of Union</li>

<li>Axiom of Infinity</li>

<li>Axiom of Power Set</li>

<li>Axiom Schema of Comprehension</li>

<li>Axiom of Replacement</li>

<li>Axiom of Foundation</li>

<li>Axiom of Choice</li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</div>
<div id="outline-container-orgb113ebb" class="outline-4">
<h4 id="orgb113ebb"><span class="section-number-4">1.3.2.</span> Event Space</h4>
</div>

<div id="outline-container-orgc0af5e3" class="outline-4">
<h4 id="orgc0af5e3"><span class="section-number-4">1.3.3.</span> Probability Measure</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
A <b>probability measure</b> \( \mathbb{P} \colon \mathcal{F} \to \mathbb{R}_{\geq 0} \) 
</p>

<p>
is a function from an event space to the positive real number such that
</p>

<p>
\( \mathbb{P}(\Omega) = 1 \)
</p>

<p>
\[ \mathbb{P}() \]
</p>
</div>
</div>
<div id="outline-container-orgf3a0c75" class="outline-4">
<h4 id="orgf3a0c75"><span class="section-number-4">1.3.4.</span> Random Variable</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
A random variable is a measurable function to \( \mathbb{R} \).
</p>

<p>
We may define a new measure on \( \mathbb{R} \) using the pushforward of the random variable.  
</p>

\begin{align*}
\begin{xy}
\xymatrix{
\Omega \ar[r]^{X} \ar[dr]_{\mathbb{P}} & \mathbb{R} \ar[d]^{X_{\ast}(\mathbb{P})} \\
& \mathbb{R}_{\geq 0}
}
\end{xy}
\end{align*}

<p>
So
</p>

\begin{align*}
X_{\ast}(\mathbb{P}) \colon
&\mathcal{B}(\mathbb{R}) \to \mathbb{R}_{\geq 0} \\
&B \mapsto \mathbb{P}(X^{-1}\left[ B \right])
\end{align*}
</div>
<ol class="org-ol">
<li><a id="orga3b2551"></a>Normal Random Variables<br /></li>

<li><a id="orgac7861d"></a>Bernouli Random Variables<br /></li>
</ol>
</div>
<div id="outline-container-org94d1da8" class="outline-4">
<h4 id="org94d1da8"><span class="section-number-4">1.3.5.</span> Expectation of Random Variables</h4>
</div>

<div id="outline-container-org0ff0bf1" class="outline-4">
<h4 id="org0ff0bf1"><span class="section-number-4">1.3.6.</span> Variance of Random Variables</h4>
</div>
</div>
</div>
<div id="outline-container-org5f677ac" class="outline-2">
<h2 id="org5f677ac"><span class="section-number-2">2.</span> Parametric Models</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-orgc4681be" class="outline-3">
<h3 id="orgc4681be"><span class="section-number-3">2.1.</span> Summary</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="outline-container-orgc4fb5fd" class="outline-4">
<h4 id="orgc4fb5fd"><span class="section-number-4">2.1.1.</span> Normal Random Variable</h4>
</div>

<div id="outline-container-org4ae4e3b" class="outline-4">
<h4 id="org4ae4e3b"><span class="section-number-4">2.1.2.</span> Exponential Random Variable</h4>
</div>

<div id="outline-container-org4968241" class="outline-4">
<h4 id="org4968241"><span class="section-number-4">2.1.3.</span> Geometric Random Variable</h4>
</div>
</div>
</div>
<div id="outline-container-org046d335" class="outline-2">
<h2 id="org046d335"><span class="section-number-2">3.</span> Classical Statistics</h2>
<div class="outline-text-2" id="text-3">
<p>
The world changes, the world <b>happens</b>. We as humans wish to understand what happens inside of the world. The most successful methods of understanding has been to seek an underlying principle by which the world operates:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Question</th>
<th scope="col" class="org-left">Principle to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">How fast will a ball roll?</td>
<td class="org-left">Conservation of Energy</td>
</tr>

<tr>
<td class="org-left">What is the result of a collision?</td>
<td class="org-left">Conservation of Momentum</td>
</tr>

<tr>
<td class="org-left">How much should I make as&#x2026;?</td>
<td class="org-left">Supply and Demand</td>
</tr>
</tbody>
</table>

<p>
Finding such principles are often hard and require a lot of insight into the problem at hand. It is generally easier to look at a bunch of data and draw conclusions based 
</p>

<p>
The real world need not wait for its own perfect description. Leaves fall and accidents happen without humans developing the "correct" model for them. 
</p>

<p>
Statistics is concerned with collecting, analyzing, interpreting, and presenting empirical data.
</p>


<p>
Probability allows for a generalized logical system pertaining to real world events. For the most part, the field starts off with the problem solver already knowing all of the ins and outs of the hypotheses of the problem. Probabilists need not concern themselves with the probability of flipping a coin. They may call the chance of heads, \( p \), and move on with what counting exercise they so wish.
</p>
</div>
<div id="outline-container-orgae82be3" class="outline-3">
<h3 id="orgae82be3"><span class="section-number-3">3.1.</span> Dataset Size on Accuracy</h3>
<div class="outline-text-3" id="text-3-1">
<p>
If \( X_{i} \) are iid \( Uniform(\text{Classes}) \)  
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">import matplotlib.pyplot as plt

points = [74.14916727009413, 83.6350470673425, 89.57277335264301, 90.87617668356263, 91.88993482983345, 92.75887038377986]

plt.plot(points)
</pre>
</div>
</div>
</div>
<div id="outline-container-org16224ea" class="outline-3">
<h3 id="org16224ea"><span class="section-number-3">3.2.</span> Cramer-Rao Bound</h3>
</div>



<div id="outline-container-org31a97a4" class="outline-3">
<h3 id="org31a97a4"><span class="section-number-3">3.3.</span> Estimation</h3>
<div class="outline-text-3" id="text-3-3">
<p>
In statistics, we are often concerned with the estimation of certain quantities in the absence of a precise formulation of said quantity.
</p>

<p>
For example, we like living longer and healthier, right? We have an <b>intuition</b> that people who exercise live both longer and healthier. How may we go about quantifying the difference in lifespan between people who do exercise?
</p>
</div>
</div>
<div id="outline-container-org32359ae" class="outline-3">
<h3 id="org32359ae"><span class="section-number-3">3.4.</span> Estimation of Parameters - General Problem Setup</h3>
<div class="outline-text-3" id="text-3-4">
<p>
In statistics, we assume that a population can be modeled by some general class of functions. Our task is then to specify which function in the class has the "closest" behavior in some sense to our data.
</p>

<p>
We are concerned with families of the form
</p>

<p>
\[ \mathbb{P}(\bm{x}, \bm{\theta}) \]
</p>
</div>
</div>
<div id="outline-container-org4fe8c47" class="outline-3">
<h3 id="org4fe8c47"><span class="section-number-3">3.5.</span> Given Data Estimate the Mean</h3>
</div>

<div id="outline-container-org9785a1f" class="outline-3">
<h3 id="org9785a1f"><span class="section-number-3">3.6.</span> Given Data Estimate the Variance</h3>
</div>

<div id="outline-container-org02b8f20" class="outline-3">
<h3 id="org02b8f20"><span class="section-number-3">3.7.</span> Given conditional distributions estimate the total population</h3>
</div>


<div id="outline-container-org17c062f" class="outline-3">
<h3 id="org17c062f"><span class="section-number-3">3.8.</span> Given Data Estimate the Total Gradient</h3>
</div>

<div id="outline-container-orgf7ab14f" class="outline-3">
<h3 id="orgf7ab14f"><span class="section-number-3">3.9.</span> Given Data Estimate the Natural Gradient</h3>
</div>


<div id="outline-container-orga6b8d73" class="outline-3">
<h3 id="orga6b8d73"><span class="section-number-3">3.10.</span> Summary</h3>
<div class="outline-text-3" id="text-3-10">
</div>
<div id="outline-container-orgd341e7d" class="outline-4">
<h4 id="orgd341e7d"><span class="section-number-4">3.10.1.</span> Cramer-Rao Bound</h4>
</div>
</div>
</div>
<div id="outline-container-org36bc57d" class="outline-2">
<h2 id="org36bc57d"><span class="section-number-2">4.</span> Information Theory</h2>
</div>

<div id="outline-container-org96008a8" class="outline-2">
<h2 id="org96008a8"><span class="section-number-2">5.</span> Statistical Learning Theory</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org5c55535" class="outline-3">
<h3 id="org5c55535"><span class="section-number-3">5.1.</span> The PAC Learning Framework</h3>
<div class="outline-text-3" id="text-5-1">
<p>
PAC - Probably Approximately Correct
</p>

<p>
The PAC framework specifies a class of learnable concepts by specifying the number of sample points to receive an approximate solution. 
</p>
</div>
</div>
<div id="outline-container-org3bedb86" class="outline-3">
<h3 id="org3bedb86"><span class="section-number-3">5.2.</span> Active Learning</h3>
</div>



<div id="outline-container-org89fc149" class="outline-3">
<h3 id="org89fc149"><span class="section-number-3">5.3.</span> Model Evaluation</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Recall that a model is defined very broadly to mean a function.
</p>

<p>
Given any dataset, \( M \colon x \mapsto 7 \) is technically a model. What meaning does \( 7 \) have here? Absolutely nothing. We would like to somehow rate how well our model does at actually representing the true function of the dataset. 
</p>

<p>
Typically this function comes from a parameterized set such that it is able to approximate something of interest.
</p>
</div>
</div>
<div id="outline-container-orge0db362" class="outline-3">
<h3 id="orge0db362"><span class="section-number-3">5.4.</span> Classification</h3>
<div class="outline-text-3" id="text-5-4">
<p>
In the task of classification, we have input features \( X \in \mathbb{R}^{n \times m} \) and output \( y \in \{ g \}_{g = 1}^{c} \).
</p>

<p>
We would like our models to be "correct" a lot of the time. However, there are subtleties involved with using only one metric for evaluation.
</p>
</div>
<div id="outline-container-orgdf98161" class="outline-4">
<h4 id="orgdf98161"><span class="section-number-4">5.4.1.</span> Accuracy</h4>
</div>



<div id="outline-container-org7560d2c" class="outline-4">
<h4 id="org7560d2c"><span class="section-number-4">5.4.2.</span> Precision</h4>
</div>


<div id="outline-container-org56ea56b" class="outline-4">
<h4 id="org56ea56b"><span class="section-number-4">5.4.3.</span> Recall</h4>
</div>

<div id="outline-container-orgce140c9" class="outline-4">
<h4 id="orgce140c9"><span class="section-number-4">5.4.4.</span> Specificity</h4>
</div>

<div id="outline-container-org5c92ed2" class="outline-4">
<h4 id="org5c92ed2"><span class="section-number-4">5.4.5.</span> Sensitivity</h4>
</div>

<div id="outline-container-org1459fb9" class="outline-4">
<h4 id="org1459fb9"><span class="section-number-4">5.4.6.</span> ROC Curve</h4>
</div>
</div>
<div id="outline-container-org20ff121" class="outline-3">
<h3 id="org20ff121"><span class="section-number-3">5.5.</span> Model Fairness and Bias</h3>
</div>



<div id="outline-container-org87bdb83" class="outline-3">
<h3 id="org87bdb83"><span class="section-number-3">5.6.</span> Summary</h3>
<div class="outline-text-3" id="text-5-6">
</div>
<div id="outline-container-orgd325440" class="outline-4">
<h4 id="orgd325440"><span class="section-number-4">5.6.1.</span> Confusion Matrix</h4>
</div>

<div id="outline-container-org8e523b4" class="outline-4">
<h4 id="org8e523b4"><span class="section-number-4">5.6.2.</span> True Positive Rate</h4>
</div>

<div id="outline-container-orgc227bce" class="outline-4">
<h4 id="orgc227bce"><span class="section-number-4">5.6.3.</span> False Positive Rate</h4>
</div>

<div id="outline-container-orgf31a074" class="outline-4">
<h4 id="orgf31a074"><span class="section-number-4">5.6.4.</span> Sensitivity</h4>
</div>

<div id="outline-container-org4920d37" class="outline-4">
<h4 id="org4920d37"><span class="section-number-4">5.6.5.</span> Specificity</h4>
</div>

<div id="outline-container-org6f44e6a" class="outline-4">
<h4 id="org6f44e6a"><span class="section-number-4">5.6.6.</span> Recall</h4>
</div>

<div id="outline-container-org5a43ccf" class="outline-4">
<h4 id="org5a43ccf"><span class="section-number-4">5.6.7.</span> Precision</h4>
</div>

<div id="outline-container-org871ad87" class="outline-4">
<h4 id="org871ad87"><span class="section-number-4">5.6.8.</span> Accuracy</h4>
</div>

<div id="outline-container-org617b813" class="outline-4">
<h4 id="org617b813"><span class="section-number-4">5.6.9.</span> F-Score</h4>
</div>

<div id="outline-container-org89ad631" class="outline-4">
<h4 id="org89ad631"><span class="section-number-4">5.6.10.</span> AUROC</h4>
</div>

<div id="outline-container-org30e44d6" class="outline-4">
<h4 id="org30e44d6"><span class="section-number-4">5.6.11.</span> ROC</h4>
</div>
</div>
</div>
<div id="outline-container-org665a546" class="outline-2">
<h2 id="org665a546"><span class="section-number-2">6.</span> Information Geometry</h2>
<div class="outline-text-2" id="text-6">
<p>
A cross section of Riemannian geometry, information theory, and statistics.
A quite intimidating but rewarding field. There are tons of definitions.
</p>
</div>
<div id="outline-container-org99763bd" class="outline-3">
<h3 id="org99763bd"><span class="section-number-3">6.1.</span> Motivation</h3>
<div class="outline-text-3" id="text-6-1">
</div>
<div id="outline-container-org381104b" class="outline-4">
<h4 id="org381104b"><span class="section-number-4">6.1.1.</span> A Simple Generalization</h4>
<div class="outline-text-4" id="text-6-1-1">
<p>
For real world applications, we would like to find the "best" \( \theta \) (with best changing based on context). This lends itself to an optimization problem over the set of \( \theta \). Naturally we would like to use calculus. A very general setting for calculus ends up being a differentiable manifold.
</p>

<p>
The introduction of geometry gives several useful notions:
</p>

<ul class="org-ul">
<li>Visceral Language (Line, Ball, Projection) allows us to intuit results.</li>

<li>Invariance - We may understand results as being invariant to the underlying parameterization, and focus on the structure of the space.</li>

<li>Equivariance - We may understand how changes in the inputs to the models cause changes in the outputs. In some applications, like edge, shape, and texture detection, this relationship is important.</li>
</ul>
</div>
</div>
<div id="outline-container-org5264b94" class="outline-4">
<h4 id="org5264b94"><span class="section-number-4">6.1.2.</span> Key Terms</h4>
<div class="outline-text-4" id="text-6-1-2">
</div>
<ol class="org-ol">
<li><a id="org86bd2b1"></a>Statistical Model<br />
<div class="outline-text-5" id="text-6-1-2-1">
<p>
A statistical model is a pair \( \left( \Omega , \mathbb{P} \right) \) where \( \Omega \) is the "sample space" (all possible observations) and \( \mathbb{P} \) is the set of probability distributions on \( \Omega \).
</p>

<p>
In the field of parametric statistics, \( \mathbb{P} \) is a parameterized set \( \mathbb{P} = \{ \mathbb{P}_{\theta} \}_{\theta \in \Theta} \).
</p>

<p>
Examples:
</p>

<ul class="org-ul">
<li>The set of Normal distributions:
\( \mathcal{N}(\mu, \sigma^{2}) \) (you give me two numbers, I give you a PDF)</li>

<li><p>
The set of Lines:
\[ \hat{y}(x) = \theta_{0} + \theta_{1}x + \varepsilon, \varepsilon \sim \mathcal{N}(0, 1) \]
</p>

<p>
We note that the data is a probability around the line:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">x</th>
<th scope="col" class="org-right">y</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">5</td>
</tr>
</tbody>
</table>

<p>
For the parameters \( (\theta_{0}, \theta_{1}) = (2, -3) \) the probability of observing this data is:
</p>

<p>
\[ y - \hat{y} = y - ((2) + (-3)x) = \varepsilon \]
</p>

<p>
Hence we have \( 3 \) epsilons (\( 1 \) per each row)
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">x</th>
<th scope="col" class="org-right">y</th>
<th scope="col" class="org-right">y<sub>hat</sub></th>
<th scope="col" class="org-right">&epsilon;</th>
<th scope="col" class="org-left">P(&epsilon;)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">-1</td>
<td class="org-right">3</td>
<td class="org-left">&phi;(3)</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">-4</td>
<td class="org-right">7</td>
<td class="org-left">&phi;(7)</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">5</td>
<td class="org-right">-10</td>
<td class="org-right">15</td>
<td class="org-left">&phi;(15)</td>
</tr>
</tbody>
</table></li>
</ul>
</div>
</li>
<li><a id="orga0086ff"></a>Likelihood<br />
<div class="outline-text-5" id="text-6-1-2-2">
<p>
Probabilities (mostly) appear in two ways:
</p>

<ol class="org-ol">
<li>I have a distribution and I want to see the probability of a datum.</li>

<li>I have a set of data and I want to see the probability it comes from a certain distribution.</li>
</ol>

<p>
Example:
</p>

<ol class="org-ol">
<li>I have a standard normal distribution (\( \mathcal{N}(0,1) \)) and I want to know the probability of \( x = 1 \).
Answer: \( \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2}} \)</li>

<li>I have the data point \( x = 1 \), and I want to know the probability it came from the standard normal distribution.
Answer: \( \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2}} \)</li>
</ol>

<p>
We note that as we vary the data or the distribution, we vary the probability.
</p>

<ol class="org-ol">
<li>Given \( \theta \), what is \( \mathbb{P}(x; \theta) \)?</li>

<li>Given \( x \), what is \( \mathbb{P}(x; \theta) \)?</li>
</ol>

<p>
We are computing the same numbers in case \( 1 \) and \( 2 \), however, the input is different, hence we tend to use different vocabulary for the different names. We call \( x \mapsto \mathbb{P}(x; \theta) \) (&theta; known) the probability density function. We call \( \theta \mapsto P(x; \theta) \) (data known) the likelihood function.
</p>

<p>
Symbolically:
Again, the likelihood is a function of the parameters of a family of distributions. If a choice of parameters maximizes the probability, we note that \( \theta \) is called a maximum likelihood estimate. 
</p>
</div>
</li>
<li><a id="org7ed7862"></a>Score<br />
<div class="outline-text-5" id="text-6-1-2-3">
<p>
The score (or informant) of  is the partial derivative of the log of the likelihood.
</p>

<p>
\[s(\theta) = \nabla_{\theta} \log(f(X; \theta)) \]
</p>

<p>
This is used on the way to getting a maximum likelihood estimate. To maximize the likelihood, we take the gradient of the likelihood and compute the zeros.
</p>

<p>
Some "type checking", working inside out:
</p>

<p>
\( f(X;\theta) \) is the likelihood \( \theta \) given \( X \).
</p>

<p>
This is a real number, hence we may take the natural logarithm (this is done for numerical stability normally).
</p>

<p>
Now we take the gradient of the output of the natural logarithm.
</p>

<p>
We may note that the expected score with a known true parameter is zero. \( \mathbb{E} \left[\nabla_{\theta} \log(f(X;\theta)) \vert \theta \right] = 0 \) 
</p>
</div>
</li>
<li><a id="orgfabac83"></a>Fisher Information<br />
<div class="outline-text-5" id="text-6-1-2-4">
<p>
A way of measuring the amount of "information" that an observable "\( X \)" carries about an unknown parameter \( \theta \).
Formally it is the variance of the score or the expected value of the observed information.
</p>

<p>
We may describe this in components:
</p>

<p>
\[ I_{ij} = \mathbb{E}\left[ \left( \partial_{i} \log f(X \vert \theta) \right) \left( \partial_{j} \log f(X \vert \theta) \right)  \right] = \mathbb{E}\left[ s(\theta) s(\theta)^{T} \right] \]
</p>

<p>
Note that this matrix is of size \( \theta^{2} \). A lot of large machine learning models are now well into the multi-millions of parameters for estimating. Which means that even storing the information matrix is very cumbersome.
</p>

<p>
It can be shown that this matrix is positive semi-definite. 
</p>
</div>
</li>
<li><a id="org899f8df"></a>Differentiable Manifold<br />
<div class="outline-text-5" id="text-6-1-2-5">
<p>
A differentiable manifold is a Hausdorff Second Countable topological space which is locally Euclidean. The main idea is that at every point we can express formulae in a local coordinate system, but the results are normally held without coordinates.
</p>

<p>
In physics, you can only measure temperature in some units (Fahrenheit Celsius or Kelvin) however there are truths which hold no matter the units you choose (Conservation of Energy) and there are relationships between the units (\( C = K + 273.15 \)). For the same experiment, we may have different numbers depending on the units, but the conclusions at the end of the experiment should be the same. 
</p>
</div>
</li>
<li><a id="orgd0b3977"></a>Metric<br />
<div class="outline-text-5" id="text-6-1-2-6">
<p>
On a Riemannian manifold, we have the introduction of a Riemannian metric \( g \).
</p>

<p>
For each tangent space, \( g \) allows one to measure vector lengths, angles, and orthogonality.
</p>
</div>
</li>
<li><a id="org5c498b3"></a>Connection<br />
<div class="outline-text-5" id="text-6-1-2-7">
<p>
An affine connection is a differential operator which allows:
</p>

<ul class="org-ul">
<li>The covariant derivative (calculate differentials of a vector field \( Y \) with respect to another vector field \( X \).</li>

<li>Parallel Transport. A way to transport vectors between tangent planes along any smooth curve \( c \)
\( \Pi_{c}^{\nabla} \)</li>

<li>Geodesics. Locally the shortest path between two points on a manifold</li>

<li>Curvature and torsion of the manifold.</li>
</ul>


<p>
There is an important connection called the Levi-Civita connection which is:
</p>

<ul class="org-ul">
<li>Metric Compatible: \( \left\langle u , v \right\rangle_{c(0)} = \left\langle \Pi_{c(0) \to c(t)}^{\nabla} u , \Pi_{c(0) \to c(t)}^{\nabla} v \right\rangle \)</li>
<li>Torsion Free: For any vector fields \( X,Y \) we have \( [X,Y] = \nabla_{X}Y - \nabla_{Y}X \)</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgdace1f6" class="outline-4">
<h4 id="orgdace1f6"><span class="section-number-4">6.1.3.</span> New Data Structures</h4>
<div class="outline-text-4" id="text-6-1-3">
</div>
<ol class="org-ol">
<li><a id="orgb1352b8"></a>Divergence<br />
<div class="outline-text-5" id="text-6-1-3-1">
<p>
A divergence is a real valued smooth function
</p>

<p>
\[ D \colon M \times M \to \mathbb{R}, (p,q) \mapsto D(p \| q) \]
</p>

<p>
with:
</p>

<ul class="org-ul">
<li>\( D(p \| q) \geq 0 \)</li>
<li>\( D(p \| q) = 0 \iff p = q \)</li>
<li>\( - \partial_{x, i} \partial_{x, j} D(p, q) \) is positive definite</li>
</ul>

<p>
Big Example:
</p>

<p>
KL-Divergence: For two probability distributions, how much information does distribution \( q \) give when \( p \) is the true distribution?
</p>

<p>
\[ D_{KL}(p \| q) = \int_{\Omega} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx \]
</p>

<p>
We are assuming some regularity properties: For example, distribution \( p \) is absolutely continuous wrt \( q \) (that is, \( q(x) = 0 \implies p(x) = 0 \))   
</p>

<p>
This can be thought of as an expected logarithmic difference between \( p \) and \( q \).
</p>

<p>
By minimizing the divergence,
</p>

<p>
\[ p^{\ast} = \argmin_{p \in P} D_{KL}(p \| q) \]
</p>

<p>
We have an "information projection" (the closest distribution to \( q \) in the space of distributions \( P \)). This is analogous to projections in a Hilbert space.
</p>

<p>
It is known that minimizing the KL-Divergence is equivalent to Maximum likelihood estimation. Another thing of note is that the second derivative ends up being (up to a scaling factor of 1/2) the fisher metric.
</p>
</div>
</li>
<li><a id="orgc9dc08a"></a>Conjugate Connection Manifolds<br />
<div class="outline-text-5" id="text-6-1-3-2">
<p>
A conjugate connection manifold (CCM) is a quadruple \( (M, g, \nabla, \nabla^{\ast}) \) 
</p>

<p>
Where a connection \( \nabla^{\ast} \) is said to be conjugate to \( \nabla \) with respect to the metric \( g \) when:
</p>

<p>
\[ X \left\langle Y , Z \right\rangle = \left\langle \nabla_{X}Y,Z \right\rangle + \left\langle Y, \nabla_{X}^{\ast} Z \right\rangle\]
</p>

<p>
For any smooth vector fields \( X, Y, Z \).
</p>

<p>
Properties:
</p>

<ol class="org-ol">
<li>Dual Parallel Transport Preserves the Metric</li>
</ol>

<p>
\[ \left\langle \Pi_{c(0) \to c(t)}^{\nabla} u, \Pi_{c(0) \to c(t)}^{\nabla^{\ast}} v \right\rangle_{c(t)} = \left\langle u , v \right\rangle_{c(0)} \]
</p>

<ol class="org-ol">
<li>Conjugations Exist for any Connection:</li>
</ol>

<p>
Given \( \nabla \) on \( (M,g) \) there exists a unique \( \nabla^{\ast} \) dual structure.
</p>

<ol class="org-ol">
<li><p>
The mean connection is the Levi-Civita connection
</p>

<p>
\[ \frac{\nabla + \nabla^{\ast}}{2} = \nabla_{LC} \]
</p></li>
</ol>
</div>
</li>
<li><a id="org193368b"></a>Statistical Manifolds<br />
<div class="outline-text-5" id="text-6-1-3-3">
<p>
Let \( C \) be a totally symmetric three tensor, 
</p>

<p>
\[ C_{ijk} = \Gamma_{ij}^{k} - \Gamma_{ij}^{\ast k} \]
</p>

<p>
We define \( \alpha \) connections as follows: 
</p>

<p>
\[ \Gamma_{ij,k}^{\alpha} = \frac{1 + \alpha}{2} \Gamma_{ij,k} + \frac{1 - \alpha}{2} \Gamma_{ij,k}^{\ast} \]
</p>

<p>
It can be shown that \( C_{ijk} \) is invariant under sufficient statistics. 
</p>
</div>
</li>
<li><a id="orge2ea4ae"></a>Interesting Papers<br />
<ol class="org-ol">
<li><a id="org9ca4b2d"></a>Cheap Control<br />
<div class="outline-text-6" id="text-6-1-3-4-1">
<p>
<a href="https://arxiv.org/pdf/1407.6836.pdf">https://arxiv.org/pdf/1407.6836.pdf</a>
</p>

<p>
Using IG, authors find a good upper bound on the number of parameters necessary for modelling a distribution. Reducing the number of paramters from an exponential (2<sup>16</sup>) to just 65.
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
<div id="outline-container-orge2e2d7b" class="outline-3">
<h3 id="orge2e2d7b"><span class="section-number-3">6.2.</span> Manifolds</h3>
<div class="outline-text-3" id="text-6-2">
<p>
#+BEGIN<sub>DEFINITION</sub>[Manifold]
An \( n \)-dimensional manifold \( M \) is a second countable, Hausdorff, locally Euclidean topological space. This means:
</p>

<ul class="org-ul">
<li>\( M \) is a collection of points</li>

<li>Around every two points there exists a pair of disjoint neighborhoods around each point
\[ \forall p, q \in M \colon \exists U, V \text{ Open Subset} \colon p \in U \land q \in V \land U \cap V = \emptyset \]</li>

<li>There is a countable basis for the topology</li>

<li>Around every point, there is an open neighborhood of that point which is homeomorphic to a subset of \( \mathbb{R}^{n} \)</li>
</ul>
<p>
#+END<sub>DEFINITION</sub>
</p>

<p>
Some important and useful results:
</p>

<p>
Manifold Creation:
</p>

<ul class="org-ul">
<li>Subsets</li>
<li>Products</li>
<li>Quotients</li>
<li>Cup Product</li>
<li>Pre-images</li>
<li>Graphs</li>
<li>Parameterizations</li>
</ul>
</div>
</div>
<div id="outline-container-orga523bf9" class="outline-3">
<h3 id="orga523bf9"><span class="section-number-3">6.3.</span> Examples of Manifolds</h3>
<div class="outline-text-3" id="text-6-3">
</div>
<div id="outline-container-org6aab169" class="outline-4">
<h4 id="org6aab169"><span class="section-number-4">6.3.1.</span> Euclidean Space</h4>
<div class="outline-text-4" id="text-6-3-1">
<p>
\( \mathbb{R}^{n} \) is an \( n \)-manifold.
</p>

<p>
We note that \( \mathbb{R}^{n} \) is Hausdorff and Second countable.
Around every point, \( p \in \mathbb{R}^{n} \) we note that \( p \in U = \mathbb{R}^{n} \) which is homeomorphic to an open subset by the identity.  
</p>
</div>
</div>
<div id="outline-container-org2fd7844" class="outline-4">
<h4 id="org2fd7844"><span class="section-number-4">6.3.2.</span> Open Sets</h4>
<div class="outline-text-4" id="text-6-3-2">
<p>
Similarly any open set is homeomorphic to itself via the identity
</p>
</div>
</div>
<div id="outline-container-orgf4c1bb7" class="outline-4">
<h4 id="orgf4c1bb7"><span class="section-number-4">6.3.3.</span> Spheres</h4>
<div class="outline-text-4" id="text-6-3-3">
<p>
An \( n \)-sphere is a subset of \( \mathbb{R}^{n + 1} \) whose points satisfy, \( \sqrt{\sum_{i = 1}^{n + 1} x_{i}^{2}} = 1 \)   
</p>
</div>
</div>
</div>
<div id="outline-container-orgc6ac12c" class="outline-3">
<h3 id="orgc6ac12c"><span class="section-number-3">6.4.</span> Manifolds of Probability Distributions</h3>
</div>




<div id="outline-container-org24754c9" class="outline-3">
<h3 id="org24754c9"><span class="section-number-3">6.5.</span> Parameterized Distributions</h3>
<div class="outline-text-3" id="text-6-5">
<p>
Oftentimes, we are interested in understanding how the real world works. We may "ask" nicely about what happens in the real world (that is, to gather data). We may only ask a finite number of times, however, we may attempt to "interpolate" in between the known data points. That is, we may attempt to answer what would happen 
</p>
</div>
</div>
<div id="outline-container-org151337e" class="outline-3">
<h3 id="org151337e"><span class="section-number-3">6.6.</span> Problems Information Geometry Solves</h3>
<div class="outline-text-3" id="text-6-6">
</div>
<div id="outline-container-org6386198" class="outline-4">
<h4 id="org6386198"><span class="section-number-4">6.6.1.</span> New Quantities of Interest</h4>
</div>

<div id="outline-container-org59a3303" class="outline-4">
<h4 id="org59a3303"><span class="section-number-4">6.6.2.</span> New Algorithms</h4>
</div>
</div>
</div>
<div id="outline-container-orgb06f0bf" class="outline-2">
<h2 id="orgb06f0bf"><span class="section-number-2">7.</span> Hessian of Log Likelihood</h2>
<div class="outline-text-2" id="text-7">
<p>
Suppose we observe some datum \( x \), and we have a parameterized distribution \( H(\theta) \) with parameter vector \( \theta = [\theta_{i}]_{i} \).
</p>

<p>
We may calculate the probability density of \( x \) by \( p(x \vert \theta) \).
</p>

<p>
We may calculate the Hessian of the log of this probability.
</p>

\begin{align}
H_{\log(p(x \vert \theta))}
&= J[\nabla \log(p(x \vert \theta))] \\
&= J \left[ \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)} \right]
\end{align}


<div class="REMARK" id="orge3b1e06">
<p>
Note that I have used \( \nabla \) to mean the gradient with respect to our parameters \( \theta \). In statistics, we are often trying to model the real world using a probability distribution from a family of distributions. In other words, the universe gives us our data and we attempt to model it. Thus, it is very uncommon to take the gradient with respect to \( x \). Hence, we may omit the variables we are differentiating, and implicitly assume we are taking the gradient with respect to \( \theta \).    
</p>

</div>

<p>
Suppose we have some data \( X_{i} \) which come from a parameterized distribution \( H(\theta) \). Note that the probability density of the \( X_{i} \) must be nonzero because we have seen the \( X_{i} \).  
</p>

<p>
We may be interested in taking the probability of encountering the data as a function of the parameters.
</p>

<p>
\[ p(X \vert \theta) = \prod_{i = 1}^{n} p(X_{i} \vert \theta) \]
</p>

<p>
One way to maximize this is to straight up take a derivative and set it equal to zero.
This becomes very unwieldy very fast due to the product rule. We may instead optimize the logarithm of the probability. Because the probabilities are always greater than \( 0 \) this results in the same optimization argmax \( \theta \).
</p>

<p>
\[ \log(p(X \vert \theta)) = \log \left( \prod_{i = 1}^{n} p(X_{i} \vert \theta) \right) = \sum_{i = 1}^{n} \log \left( p(X_{i} \vert \theta) \right) \]
</p>

<p>
The gradient wrt to \( \theta \) is:
</p>

\begin{align}
\nabla_{\theta} \log \left( p(X_{i} \vert \theta) \right)
&= \begin{bmatrix} \frac{1}{p(X_{i} \vert \theta)} \left( \frac{ \partial }{ \partial \theta_{j}} p(X_{i} \vert \theta) \right) \end{bmatrix}_{j}
\end{align}

\begin{align}
H_{f} = \begin{bmatrix} \frac{ \partial^{2} f}{ \partial x_{i} \partial x_{j}} \end{bmatrix}_{ij}
\end{align}

\begin{align}
\int H_{f} dx = \begin{bmatrix} \int \frac{ \partial^{2} f}{ \partial x_{i} \partial x_{j}} dx \end{bmatrix}_{ij}
\end{align}
</div>
</div>
<div id="outline-container-orga60b78f" class="outline-2">
<h2 id="orga60b78f"><span class="section-number-2">8.</span> Measurement Mnemonic</h2>
<div class="outline-text-2" id="text-8">
<p>
All measurements for comparing classifiers (accuracy, precision, recall, etc) are a ratio of
</p>

<p>
CORRECT / AGGREGATE
</p>

<p>
Accurate = ALL CORRECT / ALL POPULATION
Precision = POSITIVE CORRECT / LABELED POSITIVE
Recall = 
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Measurement</th>
<th scope="col" class="org-left">Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-left">ALL CORRECT / ALL POPULATION</td>
</tr>

<tr>
<td class="org-left">Precision</td>
<td class="org-left">POSITIVE CORRECT / POSITIVE LABELED</td>
</tr>

<tr>
<td class="org-left">Recall</td>
<td class="org-left">POSITIVE CORRECT / POSITIVE SUBJECT</td>
</tr>

<tr>
<td class="org-left">Negative Predictive Value</td>
<td class="org-left">NEGATIVE CORRECT / NEGATIVE LABELED</td>
</tr>

<tr>
<td class="org-left">Specificity</td>
<td class="org-left">NEGATIVE CORRECT / NEGATIVE SUBJECT</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org742deab" class="outline-2">
<h2 id="org742deab"><span class="section-number-2">9.</span> Neural Networks</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org17a7eab" class="outline-3">
<h3 id="org17a7eab"><span class="section-number-3">9.1.</span> Summary</h3>
<div class="outline-text-3" id="text-9-1">
</div>
<div id="outline-container-org7125aa0" class="outline-4">
<h4 id="org7125aa0"><span class="section-number-4">9.1.1.</span> Convolutional Neural Network</h4>
</div>

<div id="outline-container-orgc71e710" class="outline-4">
<h4 id="orgc71e710"><span class="section-number-4">9.1.2.</span> Graph Neural Network</h4>
</div>

<div id="outline-container-org0638d96" class="outline-4">
<h4 id="org0638d96"><span class="section-number-4">9.1.3.</span> Bayesian Neural Network</h4>
</div>
</div>
</div>
</div>
</body>
</html>
