<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-10-03 Thu 14:27 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Generative Adversarial Nets</title>
<meta name="author" content="Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>

          <link rel="stylesheet" href="static/css/site.css" type="text/css"/>
          <header><div class="menu"><ul>
          <li><a href="/">/</a></li>
          <li><a href="/about">/about</a></li>
          <li><a href="/posts">/posts</a></li></ul></div></header>
          <script src="static/js/nastaliq.js"></script>
          <script src="static/js/stacking.js"></script>
          <link href='https://unpkg.com/tippy.js@6.2.3/themes/light.css' rel='stylesheet'>
          <script src="https://unpkg.com/@popperjs/core@2"></script>
          <script src="https://unpkg.com/tippy.js@6"></script>
          <script>
          document.addEventListener('DOMContentLoaded', function() {
            let page = document.querySelector('.page');
            if (page) {
              initializePreviews(page);
            }
          });
          </script>
<script>MathJax = { loader: { load: ['[custom]/xypic.js'], paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'} }, tex: { packages: {'[+]': ['xypic']}, macros: { R: "{\\bf R}" } } };</script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js"></script>
<div class="grid-container"><div class="ds-grid"><div class="page">
<script>MathJax = { loader: { load: ['[custom]/xypic.js'], paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'} }, tex: { packages: {'[+]': ['xypic']}, macros: { R: "{\\bf R}" } } };</script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Generative Adversarial Nets</h1>
<div id="outline-container-orged81a1b" class="outline-2">
<h2 id="orged81a1b"><span class="section-number-2">1.</span> Probability Background</h2>
<div class="outline-text-2" id="text-1">
<p>
What is expected:
</p>

<ul class="org-ul">
<li><a href="probability-space.html#ID-c9c71707-70a1-4cc2-9895-e46c61398031">Probability Space</a>
\[ (\Omega, \mathcal{F}, \mathbb{P}) \]</li>

<li><a href="random-variable.html#ID-5c46a922-008c-4ddc-9e83-94e174e1f686">Random Variable</a>
\[ X \colon \Omega \to \mathbb{R} \]</li>

<li>\[ \int_{\Omega} X d \mathbb{P} = \begin{dcases} \sum_{x \in \mathcal{X}} x p_{X}(x) & \text{Discrete} \\ \int_{\mathcal{X}} x p_{X}(x) dx & \text{Continuous} \end{dcases} \]</li>
</ul>

<p>
We will assume all probability measures have densities:
</p>

<p>
\[ \mathbb{P}(X \in A) = \int_{A} p_{X}(x) dx \]
</p>

<p>
We also assume that logarithms are base 2.
</p>
</div>
</div>
<div id="outline-container-org3170241" class="outline-2">
<h2 id="org3170241"><span class="section-number-2">2.</span> Surprise</h2>
<div class="outline-text-2" id="text-2">
<p>
The notion of surprise is intuitive but rigorous.
</p>

<p>
Claude Shannon (Father of Information Theory)
</p>

<p>
We define a new function \( S \colon \mathcal{F} \to \mathbb{R} \) (called "surprise") based on the probabilities of events.
</p>
</div>
<div id="outline-container-orgdd77250" class="outline-3">
<h3 id="orgdd77250"><span class="section-number-3">2.1.</span> Functional Form of Surprise</h3>
<div class="outline-text-3" id="text-2-1">
<ol class="org-ol">
<li>Certain Events are Unsurprising
\[ \forall A \in \mathcal{F} : \mathbb{P}(A) = 1 \implies S(A) = 0 \]</li>

<li>The less likely an event is, the more surprising
\[ P(A) < P(B) \implies S(A) > S(B) \]</li>

<li>Surprise is Additive for Independent Events
\[ P(A,B) = P(A) P(B) \implies S(A,B) = S(A) + S(B) \]</li>
</ol>
</div>
</div>
<div id="outline-container-org95bd04c" class="outline-3">
<h3 id="org95bd04c"><span class="section-number-3">2.2.</span> Analytic Form of Surprise</h3>
<div class="outline-text-3" id="text-2-2">
<p>
\[ S(A) = - \log(\mathbb{P}(A)) \]
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt

<span style="color: #005e8b;">x</span> = np.linspace<span style="color: #000000;">(</span>0,1,100<span style="color: #000000;">)[</span>1:<span style="color: #000000;">]</span>
<span style="color: #005e8b;">fig</span>, <span style="color: #005e8b;">ax</span> = plt.subplots<span style="color: #000000;">()</span>
ax.plot<span style="color: #000000;">(</span>x, -np.log<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
ax.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span>, which=<span style="color: #3548cf;">'both'</span><span style="color: #000000;">)</span>
ax.axhline<span style="color: #000000;">(</span>y=0, color=<span style="color: #3548cf;">'k'</span><span style="color: #000000;">)</span>
ax.axvline<span style="color: #000000;">(</span>x=0, color=<span style="color: #3548cf;">'k'</span><span style="color: #000000;">)</span>
ax.set_xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Probability'</span><span style="color: #000000;">)</span>
<span style="color: #005e8b;">_</span> = ax.set_ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Surprise'</span><span style="color: #000000;">)</span>
</pre>
</div>


<div id="org3f584fb" class="figure">
<p><img src="./.ob-jupyter/27087ce19c785e037ebc02239116d62762e76d2f.png" alt="27087ce19c785e037ebc02239116d62762e76d2f.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org5958fef" class="outline-2">
<h2 id="org5958fef"><span class="section-number-2">3.</span> Entropy</h2>
<div class="outline-text-2" id="text-3">
<p>
Entropy is the <b>expected surprise</b>
</p>

<p>
\[ H(p) = \mathbb{E}[- \log(p)] = - \mathbb{E}[\log(p)] = \begin{dcases} - \sum_{x \in \mathcal{X}} p(x) \log(p(x)) & \text{Discrete} \\ - \int_{\mathcal{X}} p(x) \log(p(x)) dx & \text{Continuous} \end{dcases}  \]
</p>
</div>
</div>
<div id="outline-container-org2222f44" class="outline-2">
<h2 id="org2222f44"><span class="section-number-2">4.</span> Cross Entropy</h2>
<div class="outline-text-2" id="text-4">
<p>
Now we start getting into a comparison of models.
</p>
</div>
<div id="outline-container-orgc9ee44d" class="outline-3">
<h3 id="orgc9ee44d"><span class="section-number-3">4.1.</span> Coin Flip Experiment</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Suppose we are given a coin which we thought was fair, but was actually weighted.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #005e8b;">rng</span> = np.random.default_rng<span style="color: #000000;">()</span>
rng.choice<span style="color: #000000;">(</span>2, 10, p=<span style="color: #dd22dd;">[</span>0.01, 0.99<span style="color: #dd22dd;">]</span><span style="color: #000000;">)</span>
</pre>
</div>

<pre class="example">
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
</pre>


<p>
Under the belief that the coin is fair, this would be very surprising
</p>

<p>
\[ S(\text{10 Heads}) = - \log(\mathbb{P}(\text{10 Heads})) = - \log\left(0.5^{10}\right) = 10 \]
</p>

<p>
If we had the "correct" belief, then the surprise would be really low
</p>

<p>
\[ S(\text{10 Heads}) = - \log(\mathbb{P}(\text{10 Heads})) = - \log\left(0.99^{10}\right)\simeq 0.145 \]
</p>
</div>
</div>
<div id="outline-container-orga17f7b0" class="outline-3">
<h3 id="orga17f7b0"><span class="section-number-3">4.2.</span> Cross Entropy Definition</h3>
<div class="outline-text-3" id="text-4-2">
<p>
We define cross entropy to be the average surprise one experiences for distribution \( p \) under belief \( q \).
</p>
</div>
</div>
<div id="outline-container-orgbccee39" class="outline-3">
<h3 id="orgbccee39"><span class="section-number-3">4.3.</span> Cross Entropy Formula</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Probabilities come from \( p_{X} \)
Surprises come from \( q_{X} \)
\[ H(p,q) = -\mathbb{E}_{p} [\log(q)] = \begin{dcases} - \sum_{x \in \mathcal{X}} p(x) \log(q(x)) & \text{Discrete} \\ - \int_{\mathcal{X}} p(x) \log(q(x)) dx & \text{Continuous} \end{dcases} \]
</p>
</div>
</div>
<div id="outline-container-org6c6c721" class="outline-3">
<h3 id="org6c6c721"><span class="section-number-3">4.4.</span> Cross Entropy Properties</h3>
<div class="outline-text-3" id="text-4-4">
<ol class="org-ol">
<li>Cross Entropy is Larger than Entropy (Gibbs' Inequality)
\[ H(p,q) \geq H(p) \]</li>

<li>Cross Entropy can be large for two reasons

<ol class="org-ol">
<li>Inherent uncertainty within the experiment</li>

<li>Believing in a model \( q \) which is really different from the real distribution \( p \).</li>
</ol></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org0f67b6c" class="outline-2">
<h2 id="org0f67b6c"><span class="section-number-2">5.</span> KL-Divergence</h2>
<div class="outline-text-2" id="text-5">
<p>
Can we quantify and isolate the amount of surprise which comes from the choice of model?
</p>
</div>
<div id="outline-container-org5699e22" class="outline-3">
<h3 id="org5699e22"><span class="section-number-3">5.1.</span> KL Divergence Formula</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Kullback-Leibler (KL) Divergence is
</p>

\begin{align*}
D_{\text{KL}}(p \| q) &= \text{Cross-Entropy}(p,q) - \text{Entropy}(p) \\
&= (-\mathbb{E}_{p}[\log(q)]) - (-\mathbb{E}_{p}[\log(p)])\\
&= \mathbb{E}_{p}\left[ \log \left( \frac{p}{q} \right) \right]
\end{align*}
</div>
</div>
</div>
<div id="outline-container-org27607e6" class="outline-2">
<h2 id="org27607e6"><span class="section-number-2">6.</span> Adversarial Learning Problem</h2>
<div class="outline-text-2" id="text-6">
<p>
<b>Given</b>: A set of data \( x \).
</p>

<p>
Let
</p>
<ul class="org-ul">
<li>\( D \colon \text{shape}(x) \to \{0,1\} \)</li>
<li>\( G \colon \text{shape}(\text{hyperparameter}) \to \text{shape}(x) \)</li>
</ul>
</div>
<div id="outline-container-org18ceb0b" class="outline-3">
<h3 id="org18ceb0b"><span class="section-number-3">6.1.</span> Diagram</h3>
<div class="outline-text-3" id="text-6-1">
\begin{align*}
\begin{xy}
\xymatrix{
\text{Real World} \ar[d] \\
\text{Data } \mathbb{R}^{n} \ar[r]^{D} & \{0,1\} \\
\text{Noise } \mathbb{R}^{\lambda} \ar[u]^{G}
}
\end{xy}
\end{align*}
</div>
</div>
<div id="outline-container-org3abe995" class="outline-3">
<h3 id="org3abe995"><span class="section-number-3">6.2.</span> Goal</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>\( D \) wants to be a good classifier
It wants to minimize the KL divergence between itself and real world data</li>

<li>\( G \) wants to generate realistic data
It wants to maximize the KL divergence between \( D \) and the real world data
\( D \) cannot tell \( G \) apart from real data.</li>
</ul>


<p>
\[ \max_{G} \min_{D} \mathbb{E}[D_{\text{KL}}(y \| D)] = \min_{G} \max_{D} - \mathbb{E}[D_{\text{KL}}(y \| D)] \]
</p>

<p>
We do note however, that during optimization, the entropy of the experiment is a constant hence we may instead optimize the cross entropy instead.
</p>
</div>
</div>
<div id="outline-container-org45b13e1" class="outline-3">
<h3 id="org45b13e1"><span class="section-number-3">6.3.</span> Recall Cross Entropy</h3>
<div class="outline-text-3" id="text-6-3">
<p>
We can get a nicer form of cross entropy in this binary classification case.
</p>

\begin{align*}
\text{Cross-Entropy}
&= H(y, \hat{y}) \\
&= - \mathbb{E}_{y} \left[ \log(\hat{y}) \right] \\
&= - \sum_{k \in \{0,1\}} \mathbb{P}(y = k) \log(\mathbb{P}(\hat{y} = k)) \\
&= - \mathbb{P}(y = 0) \log(\mathbb{P}(\hat{y} = 0)) - \mathbb{P}(y = 1) \log(\mathbb{P}(\hat{y} = 1)) \\
&= - (1 - \mathbb{P}(y = 1)) \log(1 - \mathbb{P}(\hat{y} = 1)) - \mathbb{P}(y = 1) \log(\mathbb{P}(\hat{y} = 1))
\end{align*}

<p>
We note that:
</p>
<ul class="org-ul">
<li>when \( y = 1 \) then \( \mathbb{P}(y = 1) = 1 \)</li>
<li>when \( y = 0 \) then \( \mathbb{P}(y = 1) = 0 \)</li>
</ul>

<p>
In other words, \( \mathbb{P}(y = 1) = y \).
</p>

<p>
\[ \text{Cross-Entropy}(y, \hat{y}) = - [ (1 - y) \log(1 - \hat{y}) + y \log(\hat{y}) ] \]
</p>
</div>
</div>
<div id="outline-container-org330f1ac" class="outline-3">
<h3 id="org330f1ac"><span class="section-number-3">6.4.</span> Substitution</h3>
<div class="outline-text-3" id="text-6-4">
<p>
The structure of the input data changes depending on the output.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Image Type</th>
<th scope="col" class="org-left">Label</th>
<th scope="col" class="org-left">Formula for the Image</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Real</td>
<td class="org-left">\( y = 1 \)</td>
<td class="org-left">\( x \)</td>
</tr>

<tr>
<td class="org-left">Generated</td>
<td class="org-left">\( y = 0 \)</td>
<td class="org-left">\( G(z) \)</td>
</tr>
</tbody>
</table>

<p>
\[ \text{Cross-Entropy}(y, D)  = y \log(D(x)) + (1 - y) \log(1 - D(G(z))) \]
</p>
</div>
</div>
</div>
<div id="outline-container-orga11a8ae" class="outline-2">
<h2 id="orga11a8ae"><span class="section-number-2">7.</span> Optimal Discriminator</h2>
<div class="outline-text-2" id="text-7">
<p>
Let us first deal with
</p>

<p>
\[ \max_{D} -\mathbb{E} [D_{\text{KL}}(y \| D)] = \max_{D} \mathbb{E}[\text{Cross-Entropy}(y, D)] = V(D,G)  = \max_{D} \mathbb{E}[ y \log(D(x)) + (1 - y) \log(1 - D(G(z)))] \]
</p>

<p>
Let us rewrite \( V(D,G) \) in terms of their respective integrals,
</p>

\begin{align*}
V(D,G)
%&= \int_{x} \log(D(x)) p_{\text{data}}(x) dx + \int_{z} \log(1 - D(G(z))) p_{z}(z) dz &&\text{Def. $V(D,G)$} \\
&= \int_{x} \log(D(x)) p_{\text{data}}(x) dx + \int_{x} \log(1 - D(x)) p_{g}(x) dx &&\text{Change of Variables (Pushforward Measure)} \\
&= \int_{x} \log(D(x)) p_{\text{data}}(x) + \log(1 - D(x)) p_{g}(x) dx &&\text{Combine Integrals}
\end{align*}

<p>
We note that the integral is maximized whenever \( D(x) \) maximizes the integrand for each \( x \).
This means we want to describe what \( D(x) \) should be which maximizes the following for a fixed \( x \),
</p>

<p>
\[ \log(D(x)) p_{\text{data}}(x) + \log(1 - D(x)) p_{g}(x) \]
</p>

<p>
We may take a step back and notice the "form" of the integrand.
We want to find the \( y \) which maximizes,
</p>

<p>
\[ a \log(y) + b \log(1 - y) \]
</p>

<p>
Simple calculus&#x2026;
</p>

\begin{align*}
&&y &= \argmax_{y} \left[ a \log(y) + b \log(1 - y) \right] &\text{} \\
&\implies &0 &= \frac{ d }{ d y} \left[ a \log(y) + b \log(1 - y) \right] &\text{} \\
&\iff &0 &= \frac{a}{y} - \frac{b}{1 - y} \\
&\iff &0 &= \frac{a(1 - y) - by}{y (1 - y)} \\
&\iff &0 &= a(1-y) - by \\
&\iff &y &= \frac{a}{a + b}
\end{align*}

<p>
By substituting the our variables we see that,
</p>

<p>
\[ D(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_{g}(x)} \]
</p>
</div>
</div>
<div id="outline-container-orga52dc23" class="outline-2">
<h2 id="orga52dc23"><span class="section-number-2">8.</span> Optimal Generator</h2>
<div class="outline-text-2" id="text-8">
<p>
By substituting the optimal discriminator into the minimax game we get the following formula
</p>

\begin{align*}
C(G)
&= \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \log \left( \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) \right] + \mathbb{E}_{x \sim p_{g}(x)} \left[ \log \left( \frac{p_{g}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) \right] & \text{} \\
&= \int_{x} p_{\text{data}}(x) \log \left( \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) dx +  \int_{x} p_{g}(x) \log \left( \frac{p_{g}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) dx &\text{} \\
&= \int_{x} p_{\text{data}}(x) \log \left(\frac{2}{2} \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) dx +  \int_{x} p_{g}(x) \log \left(\frac{2}{2} \frac{p_{g}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) dx &\text{} \\
&= \int_{x} p_{\text{data}}(x) \log \left( \frac{2 p_{\text{data}}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) - p_{\text{data}}(x) \log(2) dx +  \int_{x} p_{g}(x) \log \left(\frac{2 p_{g}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) - p_{g}(x) \log(2) dx &\text{} \\
&= \int_{x} p_{\text{data}}(x) \log \left(  \frac{ 2 p_{\text{data}}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) dx +  \int_{x} p_{g}(x) \log \left(\frac{2 p_{g}(x)}{p_{\text{data}}(x) + p_{g}(x)} \right) dx - \log(4) &\text{} \\
&= - \log(4) + D_{KL} \left( p_{\text{data}} \middle\vert\middle\vert \frac{p_{\text{data}}(x) + p_{g}(x)}{2} \right) + D_{KL} \left( p_{g} \middle\vert\middle\vert \frac{p_{\text{data}}(x) + p_{g}(x)}{2} \right)
\end{align*}

<p>
\[ \frac{2 p_{data}}{p_{data} + p_{g}} = \frac{p_{data}}{\frac{p_{data} + p_{g}}{2}} \]
</p>


<p>
By Gibbs' Inequality \( (D_{KL}(p,q) \geq 0) \) we note that the minimum of this occurs whenever
</p>

<p>
\[ p_{g} =\frac{p_{\text{data} + p_{g}}}{2} ,\; p_{\text{data}} =\frac{p_{\text{data} + p_{g}}}{2} \]
</p>

<p>
Which is precisely when \( p_{g} = p_{\text{data}} \).
</p>
</div>
</div>
</div>
</body>
</html>
