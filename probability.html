<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-10-25 Tue 17:56 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Zain Jabbar" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" href="/css/main-dark.css" type="text/css"/>
<header><div class="menu"><ul>
<li><a href="/">/</a></li>
<li><a href="/about">/about</a></li>
</ul></div></header>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<div id="outline-container-orgc1fb52b" class="outline-2">
<h2 id="orgc1fb52b"><span class="section-number-2">1.</span> Definitions</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org1639217" class="outline-3">
<h3 id="org1639217"><span class="section-number-3">1.1.</span> Probability Space</h3>
</div>
<div id="outline-container-org9e28f9a" class="outline-3">
<h3 id="org9e28f9a"><span class="section-number-3">1.2.</span> Pushforward Measure</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Given \[ X \colon \Omega \to \mathbb{R} \] and \[ g \colon \mathbb{R} \to \mathbb{R} \] both measureable, we have that \[ g \circ X \] is also measureable.
</p>

<p>
The pushforward measure is then:
\( \begin{tikzcd}
\Omega & A & \mathbb{R}_{\geq 0} \\
& & \mathbb{R}_{\geq 0}
\arrow["X", from=1-1, to=1-2]
\arrow["g", from=1-2, to=1-3]
\arrow["P", from=1-1, to=2-3]
\arrow["X_*P", from=1-2, to=2-3]
\arrow["(g \circ X)_* P", from=1-3, to=2-3]
\end{tikzcd}\)
</p>
</div>
</div>
</div>

<div id="outline-container-orga7c03a5" class="outline-2">
<h2 id="orga7c03a5"><span class="section-number-2">2.</span> MATH 649K Nasrin</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org1a5ee91" class="outline-3">
<h3 id="org1a5ee91"><span class="section-number-3">2.1.</span> Homework 1</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="outline-container-orga61f882" class="outline-4">
<h4 id="orga61f882"><span class="section-number-4">2.1.1.</span> Problem 1</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
\(\lambda\)
</p>
<ul class="org-ul">
<li>\((\Omega, \mathcal{F}, P)\) be a probability space</li>
<li>\(|A| \leq \mathbb{N}\)</li>
<li>\(X \colon \Omega \to A\) be a random variable</li>
<li>\(g \colon A \to [0, \infty]\)</li>
</ul>



<p>
\(\therefore\)
</p>

<p>
Let \(Y=g(X)\), then,
\[\begin{align*}
\mathbb{E}[Y]
&= \sum_{y \in Y} y P(Y = y) \\
&= \sum_{y \in Y} y \sum_{x \in g^{-1}[y]} P(X = x) \\
&= \sum_{y \in Y} \sum_{x \in g^{-1}[y]} g(x) P(X = x) \\
\end{align*}\]  
Note: \(\{y\}\) is a partition of \(\text{im}(g)\) hence \(g^{-1}(y)\) forms a partition of \(X\).
Hence: \[ \sum_{y \in Y} \sum_{x \in g^{-1}[y]} \simeq \sum_{x \in X} \]
So we have the desired result, \[ \sum_{x \in X} g(x) P(X = x) \] 
</p>
</div>
</div>
<div id="outline-container-orgfc58782" class="outline-4">
<h4 id="orgfc58782"><span class="section-number-4">2.1.2.</span> Problem 2</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
\(\lambda\)
</p>
<ul class="org-ul">
<li>\(\{E_n\}_{n=1}^{\infty}\) be a sequence of independent events</li>
</ul>

<p>
Note that \(\left\{\bigcap_{n = 1}^{i} E_n \right\}_{i = 1}^{\infty}\)  is a decreasing sequence of events. 
</p>

<p>
\[ P(\bigcap_{n = 1}^{\infty} E_n)
= P(\lim_{i \to \infty} \bigcap_{n = 1}^{i} E_n) \\
= \lim_{i \to \infty} P( \bigcap_{n = 1}^{i} E_n) \\
= \lim_{i \to \infty} \prod_{n = 1}^{i} E_n = \prod_{n = 1}^{\infty} P(E_n)\] 
</p>
</div>
</div>

<div id="outline-container-orgb542655" class="outline-4">
<h4 id="orgb542655"><span class="section-number-4">2.1.3.</span> Problem 3</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
\(\lambda\)
</p>
<ul class="org-ul">
<li>\(T\) be the text "A Chronicle of Ancient Sunlight", partition \(\mathbb{N}\) by the residue classes \(\mod \ell (T)\) (the length of \(T\))</li>
<li>\(E_k \in \mathcal{F}\) be the event that $T $begins at position \(k \cdot \ell (T)\)</li>
</ul>

<p>
\[ \forall k \in \mathbb{N} . P(E_k) = \frac{1}{K^{\ell(T)}} > 0 \]
</p>

<p>
Thus, \[ \sum_{k \in \mathbb{N}} P(E_k) = \infty \] and \[ \{E_k\}_{k \in \mathbb{N}} \] are an independent set of events.
</p>

<p>
Hence by Borel-Cantelli we have that \[ P(\{E_k \ \text{i.o.} \}) = 1 \] The chimp will almost surely write the novel.
(Infinitely many times in fact; what a hard worker!)
</p>
</div>
</div>

<div id="outline-container-org0af2463" class="outline-4">
<h4 id="org0af2463"><span class="section-number-4">2.1.4.</span> Problem 4</h4>
<div class="outline-text-4" id="text-2-1-4">
\begin{align*}

p \left(\left|\frac{1}{n} \sum_{m = 1}^{n} (X_m - \mathbb{E}(X_m))\right| \geq \delta \right) \\
&\leq \frac{1}{\delta^2} \mathbb{E}\left[ \left| \frac{1}{n} \sum_{m = 1}^{n} (X_m - \mathbb{E}(X_m)) \right|^2 \right] \\
&\leq \frac{1}{\delta^2 n^2} \mathbb{E}\left[ \left| \sum_{m = 1}^{n} (X_m - \mathbb{E}(X_m)) \right|^2 \right] \\
&\leq \frac{1}{\delta^2 n^2} \mathbb{E}\left[ \sum_{m = 1}^{n} (X_m - \mathbb{E}(X_m))^2 \right] \\
&= \frac{1}{\delta^2 n^2} \sum_{m = 1}^{n} \mathbb{E}\left[  (X_m - \mathbb{E}(X_m))^2 \right] \\
&= \frac{1}{\delta^2 n^2} \sum_{m = 1}^{n} \right( \mathbb{E}\left[  (X_m - \mathbb{E}(X_m))^2 \right] + \mathbb{E}\left[  (X_m - \mathbb{E}(X_m)) \right]^2 \right) \\
&= \frac{1}{\delta^2 n^2} \sum_{m=1}^{n} \text{Var}(X_m)
\end{align*}
</div>
</div>
<div id="outline-container-org1cc93e7" class="outline-4">
<h4 id="org1cc93e7"><span class="section-number-4">2.1.5.</span> Problem 5</h4>
<div class="outline-text-4" id="text-2-1-5">
<ul class="org-ul">
<li>a</li>
</ul>
<p>
We get that \(Y_n\) is the result of pulling all coins out of the barrel.
This means that \(P(Y_n = j) = 0\) when \(j\) is not the number of silver coins and \(1\) when \(j\) is the number of silver coins.
</p>

<p>
\[ P(Y_n = j) = \sum_{i \in [0 .. n]} P(Y_n = j \vert \ i \text{ Silver Coins}) P(i \text{ Silver Coins}) = \frac{1}{n+1} \] 
</p>

<ul class="org-ul">
<li>b</li>
</ul>
<p>
Let us condition on two properties, \[ Y_n = i \] and that the "Next coin is Silver = S" or "Next coin is Gold = G".
</p>

<p>
\[\begin{align*}
P(Y_{n-1} = j)
&= \sum_{i \in [0 .. n]} P(Y_{n-1} = j \vert S \land \ Y_n = i) P(S \land Y_n = i) \\
&+ P(Y_{n-1} = j \vert G \land \ Y_n = i) P(G \land Y_n = i) \\
&= \sum_{i \in [0 .. n]} P(Y_{n-1} = j \vert S \land \ Y_n = i) P(S \vert Y_n = i) P(Y_n = i) \\
&+ P(Y_{n-1} = j \vert G \land \ Y_n = i) P(G \vert Y_n = i) P(Y_n = i) \\
\end{align*}\]
Now we may evaluate some of the expressions.
\[ P(Y_{n-1} = j \vert S \land \ Y_n = i) = \begin{cases} 1 & j = i-1 \\ 0 & \text{else} \end{cases} \] 
\[ P(Y_{n-1} = j \vert G \land \ Y_n = i) = \begin{cases} 1 & j = i \\ 0 & \text{else} \end{cases} \] 
\[ P(S \vert \ Y_n = i) =  \frac{i}{n} \] 
\[ P(G \vert \ Y_n = i) =  \frac{n - i}{n} \]
\[ P(Y_n = i) = \frac{1}{n+1} \]
</p>

<p>
Due to the first two equations, we get a lot of zero terms. There are only two cases when there are non zero terms. That being when \[ i = j \] and \[ i = j + 1 \] 
So the sum becomes \[ 1 \cdot \frac{j + 1}{n} \cdot \frac{1}{n + 1} + 1 \cdot \frac{n - j}{n} \cdot \frac{1}{n+1} = \frac{1}{n} \]
</p>

<ul class="org-ul">
<li>c</li>
</ul>
<p>
I conjecture:
\[ P(Y_{k} = i) = \frac{1}{k+1} \] 
</p>


<ul class="org-ul">
<li>d</li>
</ul>

<p>
We can calculate this similarly to part (b) (and by similarly, I mean I literally copy pasted the result and did a search and replace)
</p>


<p>
Let us condition ok two properties, \[ Y_k = i \] and that the "Next coin is Silver = S" or "Next coin is Gold = G".
</p>

<p>
\[ \begin{align*}
P(Y_{k-1} = j)
&= \sum_{i \in [0 .. k]} P(Y_{k-1} = j \vert S \land \ Y_k = i) P(S \land Y_k = i)\\ &+ P(Y_{k-1} = j \vert G \land \ Y_k = i) P(G \land Y_k = i) \\
&= \sum_{i \in [0 .. k]} P(Y_{k-1} = j \vert S \land \ Y_k = i) P(S \vert Y_k = i) P(Y_k = i)\\ &+ P(Y_{k-1} = j \vert G \land \ Y_k = i) P(G \vert Y_k = i) P(Y_k = i) \\
\end{align*}\]
Now we may evaluate some of the expressions.
\[ P(Y_{k-1} = j \vert S \land \ Y_k = i) = \begin{cases} 1 & j = i-1 \\ 0 & \text{else} \end{cases} \] 
\[ P(Y_{k-1} = j \vert G \land \ Y_k = i) = \begin{cases} 1 & j = i \\ 0 & \text{else} \end{cases} \] 
\[ P(S \vert \ Y_k = i) =  \frac{i}{k} \] 
\[ P(G \vert \ Y_k = i) =  \frac{k - i}{k} \]
\[ P(Y_k = i) = \frac{1}{k+1} \]
</p>

<p>
Due to the first two equations, we get a lot of zero terms. There are only two cases when there are non zero terms. That being when \[ i = j \] and \[ i = j + 1 \] 
So the sum becomes \[ 1 \cdot \frac{j + 1}{k} \cdot \frac{1}{k + 1} + 1 \cdot \frac{k - j}{k} \cdot \frac{1}{k+1} = \frac{1}{k} \]
</p>
</div>
</div>

<div id="outline-container-orga76585d" class="outline-4">
<h4 id="orga76585d"><span class="section-number-4">2.1.6.</span> Problem 6</h4>
<div class="outline-text-4" id="text-2-1-6">
<ul class="org-ul">
<li>a</li>
</ul>
<p>
Let \(P_{n,m} := P(A \text{ is always ahead} \vert \ [A \text{ has } n \text{ votes}] \land [B \text{ has } m \text{ votes}])\)
</p>

<p>
Then \(1 - P_{m,n}\) is the probability of the negation; thus \(1 - P_{m,n} = P(A \text{ is not always ahead} \vert \ [A \text{ has } n \text{ votes}] \land [B \text{ has } m \text{ votes}])\)
If there is a point where \(A\) is not always ahead, then there exists a time where \(A\) is tied with \(B\). Hence, \(1 - P_{m,n} = P(A \text{ is at some point tied with } B \vert \ [A \text{ has } n \text{ votes}] \land [B \text{ has } m \text{ votes}])\)
</p>


<ul class="org-ul">
<li>b</li>
</ul>
<p>
We know that \(A\) eventually wins due to \(n > m\). Hence any sequence that begins with \(B\) will eventually tie. Also any sequence that begins with \(A\) can be put in put in bijection with a sequence starting with \(B\). This is because any sequence that ties with \(A\) starting can be flipped (A to B and B to A) into a sequence beginning with \(B\) and eventually tying. This is has an inverse, hence bijective.
</p>

<p>
Note: \(A\) and \(B\) cannot tie in the first vote because either \(A\) or \(B\) gets one vote and the other has zero. So someone is ahead on the first vote.
If \(A\) and \(B\) eventually tie, then there is one person who was ahead of the other, then there was one vote that tied them. Call the first such case the \(n\) th vote. Both are tied on the \(n\) th vote.
If one person was ahead of another, and they tie, that means the person that was behind was the one that got the \(n\) th vote.
</p>

<p>
Hence, the one who was ahead gets the first vote and the person who was behind gets the \(n\) th vote.
</p>

<p>
By reversing the order of the votes, we get a dual case where the person who was ahead in the original case is now the person who was behind in the dual case. This is a bijection of cases hence it does not matter who gets the first vote.
</p>

<ul class="org-ul">
<li>c</li>
</ul>
<p>
We prove this by counting.
</p>

<p>
\[ P(T) = \frac{\text{No. of Sequences with Ties}}{\text{No. of Sequences}} \] 
</p>

<p>
There are \[  \] 
</p>

<p>
Let \(t\) be the number of sequences that end have ties. Each sequence is determined by the places where \(n\) votes for \(A\) are given.  
</p>

<p>
The probability of tying is \[ P(T) = P(T \land A) + P(T \land B) = 2 \cdot P(T \land B) \]
</p>

<p>
\[ P(T) = 2 \cdot P(T | B) P(B) \] because there is a bijection between tying sequences beginning with \(A\) or \(B\). Because \(A\) always wins we know \(P(T | B) = 1\) hence \(P(T) = 2 \cdot \frac{m}{m + n}\).
</p>

<p>
Thus
\[ \begin{align*}
P_{n,m}
&= 1 - P(\text{Sequence eventually ties}) \\
&= 1 - 2 \cdot P(\text{Sequence begins with }B) \\
&= 1 - 2 \frac{m}{m+n} \\
&= \frac{n - m}{m+n} \\
\end{align*}\] 
</p>
</div>
</div>
</div>


<div id="outline-container-org46e04ab" class="outline-3">
<h3 id="org46e04ab"><span class="section-number-3">2.2.</span> Homework 2</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-org3c88201" class="outline-4">
<h4 id="org3c88201"><span class="section-number-4">2.2.1.</span> Problem 1</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
\( \lambda \)
</p>
<ul class="org-ul">
<li>\( \{N(t), t \geq 0 \} \) be a PP with rate \( \lambda > 0 \)</li>
<li>\( G \) be a non-negative random variable with mean \( \mu \) and variance \( \sigma^2 \)</li>
<li>\( G \) be independent of \( N(t) \)</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org7caade1"></a>A<br />
<div class="outline-text-5" id="text-2-2-1-1">
<p>
\( \Cov(G,N(G)) = \E[G \cdot N(G)] - \E[N(G)] \E[G] \)
</p>

<p>
We shall condition on \( G \).
</p>

<p>
\[ \E[G \cdot N(G)] = \E[\E[G \cdot N(G) \vert G]] = \E[G \cdot \lambda G]\]
\[ \E[N(G)] = \E[\E[ N(G) \vert G]] = \E[\lambda G] \]
Both of these calculations use \( \E[N(t)] = \lambda t \), when given \( G \) we can substitute in \( G \) for t.
Thus,
\( \Cov(G, N(G)) = \lambda (\E[G^2] - \E[G]^2) = \lambda \Var[G] = \lambda \sigma^2 \) 
</p>
</div>
</li>
<li><a id="orgf2af9da"></a>B<br />
<div class="outline-text-5" id="text-2-2-1-2">
\begin{align*}
\Var[N(G)] &= \E[\E[N(G)^2 \vert G]] - \E[\E[N(G) \vert G]]^2  \\
&= \E[\lambda^2 G^2 + \lambda G] - \E[\lambda G]^2  \\
&= \E[\lambda^2 G^2] - \E[\lambda G]^2 + \E [\lambda G]   \\
&= \lambda^2 \Var[G] + \lambda \E[G]
\end{align*}
</div>
</li>
</ol>
</div>

<div id="outline-container-org2ac8129" class="outline-4">
<h4 id="org2ac8129"><span class="section-number-4">2.2.2.</span> Problem 2</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
\( \lambda \)
</p>
<ul class="org-ul">
<li>\( \{N(t), t \geq 0 \} \) be a PP (Using Definition 2.1.2) with rate \( \lambda \)</li>
<li>\( I \) be an interval of length \( t \)</li>
</ul>

<p>
Calculate the Laplace Transform of \( N(t) \)
</p>
\begin{align*}
F(t + h)
&= \E[e^{-s N(t + h)}] \\
&= \E[e^{-s (N(t) + (N(t+h) - N(t)))}] \\
&= \E[e^{-s N(t)} e^{-s (N(t+h) - N(t)}] \\
&= \E[e^{-s N(t)}] \E[e^{-s (N(t+h) - N(t)}] \\
&= F(t) \E[e^{-s (N(h))}] \\
\end{align*}
<p>
First equality is calculating the laplace transform and defining a function \( F \) to be the value of the laplace transform at \( x \).
Second equality is by using the properties of counting process.
Third equality is using properties of exponentials.
Fourth equality is using independence of increments.
Last equality is noticing that we have another term of the laplace transform. This gives us a functional relationship which will be used to actually solve for the values of \( F \).
</p>

<p>
To find the value of \( \E[e^{-s (N(h))}] \) we may condition on the value of \(N(h)\). This formatting looks gross sorry about that :(
</p>

\begin{align*}
\E[e^{-s (N(h))}]
&= \E[\E[e^{-s (N(h))} \vert N(h)]]\\
&= \E[e^{-s (N(h))} \vert N(h) = 0] P(N(h) = 0) + \\
&\E[e^{-s (N(h))} \vert N(h) = 1] P(N(h) = 1) + \\
&\E[e^{-s (N(h))} \vert N(h) \geq 2] P(N(h) \geq 2) \\
&= 1 \cdot (1 - (\lambda h + o(h) + o(h))) + e^{-s} (\lambda h + o(h)) + \sum_{i = 2}^{\infty} e^{-s \cdot i} P(N(h) = i) \\
&= 1 - \lambda h + e^{-s} \lambda h + o(h)
\end{align*}
<p>
We use three cases, \( n=0,1,\geq 2 \). These cases are motivated by the given definition of our process.
We then apply the properties of the process for the case of \( 0 \) and \( 1 \) and use summations for greater than \( 2 \).
Using the properties of the poisson process again and combining like terms we get the last equality.
</p>

<p>
Hence:
</p>

<p>
\(F(t + h) = F(t) (1 - \lambda h + e^{-s} \lambda h + o(h))\)
</p>

<p>
\(\implies\) \(\frac{F(t + h) - F(t)}{h} = F(t)(-\lambda + e^{-s} \lambda) + \frac{o(h)}{h}\)
</p>

<p>
By letting \(h \to 0\) we obtain, \(F'(t) = F(t)(-\lambda + e^{-s} \lambda)\)
</p>

<p>
Rearanging: \(\frac{F'(t)}{F(t)} = \lambda (-1 + e^{-s})\)
</p>

<p>
Integrating by \(t\) yields: \(\log(F(t)) = \lambda t (-1 + e^{-s})\)
</p>

<p>
So: \(F(t) = e^{\lambda t (-1 + e^{-s})}\) which is the Laplace transform of a Poisson random variable.
</p>

<p>
By uniqueness of the Laplace transform we get that \(N(t)\) is a Poisson random variable.
</p>
</div>
</div>
<div id="outline-container-org89e10f4" class="outline-4">
<h4 id="org89e10f4"><span class="section-number-4">2.2.3.</span> Problem 3</h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
\( \lambda \)
</p>
<ul class="org-ul">
<li>\( \psi_i(t) \) be the indicator function of bug \( i \) causing an error by \( t \)</li>
</ul>

<p>
First, \( \E[M_2(t)] \), I'll use it in the next proof.
We use a different indicator function, \( I_i^{(2)} \) indicates bug \( i \) causes \( 2 \) errors. This is Poisson Distributed.
</p>

\begin{align*}
\E[M_2(t)]
&= \sum_i \E[I_i^{(2)}(t)] \\
&= \sum_{i} e^{-\lambda_i t} \frac{(\lambda_i t)^2}{2}
\end{align*}

\begin{align*}
\E\left[\left(\Lambda(t) - \frac{M_1(t)}{t}\right)^2\right]
&= \E\left[\left(\Lambda(t) - \frac{M_1(t)}{t}\right)^2\right] + 0 \\
&= \E\left[\left(\Lambda(t) - \frac{M_1(t)}{t}\right)^2\right] + \E\left[\left(\Lambda(t) - \frac{M_1(t)}{t}\right)\right]^2 \\
&= \Var[\Lambda(t) - \frac{M_1(t)}{t}] \\
&= \Var[\Lambda(t)] + \frac{\Var(M_1(t))}{t^2} - \frac{2}{t} \Cov(\Lambda(t), M_1(t)) \\
&= [\sum_i \lambda_i^2 (e^{-\lambda_i t})(1-e^{-\lambda_i t})] + \\
&\frac{[\sum_i (\lambda_i t e^{-\lambda_i t}) (1 - \lambda_i t e^{-\lambda_i t})]}{t^2} - \frac{2}{t} [\sum_i \Cov(\psi_i(t), I_i(t))] \\
&= [\sum_i \lambda_i^2 (e^{-\lambda_i t})(1-e^{-\lambda_i t})] + \\
&\frac{[\sum_i (\lambda_i t e^{-\lambda_i t}) (1 - \lambda_i t e^{-\lambda_i t})]}{t^2} - \frac{2}{t} [\sum_i \lambda_i e^{-\lambda_i t} \lambda_i t e^{-\lambda_i t}] \\
&= \sum_i \lambda_i^2 e^{- \lambda_i t} + \frac{\sum_i \lambda_i e^{-\lambda_i t}}{t} \\
&= \frac{\E[M_1(t) + 2 M_2(t)]}{t^2} \\
\end{align*}

<p>
First equality is by adding 0.
Second equality is by using \( \E[\Lambda(t) - \frac{M_1(t)}{t}] = 0 \) (we proved this in class). Hence \( \E\left[\left(\Lambda(t) - \frac{M_1(t)}{t}\right)\right]^2 = 0 \).
Third equality is from \( \Var(X) = \E[X^2] - \E[X]^2 \).
Fourth equality is by using definition of \( \Var \) for possibly non independent random variables.
Fifth equality is from breaking down \( \Lambda \) and \( M_1 \) into their summation forms and applying the variance to all of the terms.
Sixth equality is calculating the covariance between the summands.
Seventh equality is combining terms with the same index \( i \).
Last equality is to notice the two sums are the definitions of \( M_1 \) and \( 2M_2 \) (Which is only motivated because that was what we wanted to show). 
</p>
</div>
</div>
<div id="outline-container-orgb5bcf9b" class="outline-4">
<h4 id="orgb5bcf9b"><span class="section-number-4">2.2.4.</span> Problem 4</h4>
<div class="outline-text-4" id="text-2-2-4">
<p>
\( \lambda \)
</p>
<ul class="org-ul">
<li>\( N(t) \) be a non-homogeneous Poisson Process with rate \( \lambda(t) \)</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgbe73b48"></a>A<br />
<div class="outline-text-5" id="text-2-2-4-1">
<p>
\( \lambda(t) = \begin{cases} 6 & 0 \leq t < 2 \\ 15 & 2 \leq t < 4 \\ 15 - (\frac{5}{2})(t-4) & 4 \leq t \leq 6 \end{cases} \)
From this we can define \( m(t) \)
</p>

<p>
\( m(t) = \begin{cases} 6t & 0 \leq t < 2 \\ 15(t-2) + 12 & 2 \leq t < 4 \\ 15(t-4) - (\frac{5}{2})(\frac{1}{2}(t-4)^2-4(t-4)) + 42 & 4 \leq t \leq 6 \end{cases} \)
</p>
</div>
</li>

<li><a id="orgb710b20"></a>B<br />
<div class="outline-text-5" id="text-2-2-4-2">
<p>
Calculate \( m(5) - m(3) = 62.75 \) 
\( P(N(5) - N(3) = 0) = \exp(-62.75) \frac{62.75^0}{0!} \simeq 5.5978500469\cdot 10^{-28}\) 
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgb842b25" class="outline-4">
<h4 id="orgb842b25"><span class="section-number-4">2.2.5.</span> Problem 5</h4>
<div class="outline-text-4" id="text-2-2-5">
<p>
\( \lambda \)
</p>
<ul class="org-ul">
<li>\( M(t) \) be the total amount of money paid in \( (0,t] \)</li>
<li>The number of payments follow a Poisson Process with \( \lambda = 5 \)</li>
<li>The Payments are exponentially distributed with expected value \( 20,000 \)</li>
<li>\( \E[M(4)] \)</li>
</ul>

<p>
We have the following relationship between the terms:
</p>

<p>
\[ M(t) = \sum_{i=0}^{N(t)} P_i \]
Using the formula from the random number of random variables we can calculate the expected value.
</p>

<p>
Hence \( \E[M(t)] = \E[N(t)] \E[P_i] \)
</p>

<p>
\( \E[M(4)] = 5 \cdot 4 \cdot 20,000 = 400,000 \) 
</p>
</div>
</div>


<div id="outline-container-orgabfda69" class="outline-4">
<h4 id="orgabfda69"><span class="section-number-4">2.2.6.</span> Problem 6</h4>
<div class="outline-text-4" id="text-2-2-6">
<p>
\( \lambda \)
</p>
<ul class="org-ul">
<li>\(N(t)\) be a Poisson Process with rate \(1\)</li>
<li>\(\Lambda\) be a non negative non constant random variable.</li>
<li>\(N_\Lambda(t) := N(\Lambda t)\), \(N_\Lambda\) is then Poisson with rate \(\lambda\) when conditioning on \(\Lambda = \lambda\)</li>
</ul>

<p>
Suppose for sake of contradiction that \(N_\Lambda(t)\) had independent increments,
</p>

\begin{align*}
P(N_\Lambda(1) - N_\Lambda(0) = 0,N_\Lambda(2) - N_\Lambda(1) = 0) 
&= P(N_\Lambda(1) - N_\Lambda(0) = 0) P(N_\Lambda(2) - N_\Lambda(1) = 0) \\
&= P(N_\Lambda(2) = 0)
\end{align*}

<p>
Calculating \(P(N_\Lambda(1) = 0)\) yields:
</p>

<p>
\(P(N_\Lambda(1)) = \E[P(N(\lambda) = 0 \vert \Lambda = \lambda)] = \E[e^{-\Lambda}]\)
</p>

<p>
By stationary increments we also have
</p>

<p>
\(P(N_\Lambda(2) - N_\Lambda(1)) = \E[P(N(2\lambda) - N(\lambda) = 0 \vert \Lambda = \lambda)] = \E[e^{-\Lambda}]\)
</p>

<p>
Calculating \(P(N_\Lambda(2))\) yields:
</p>

<p>
\(P(N_\Lambda(2)) = \E[P(N(2\lambda) = 0 \vert \Lambda = \lambda)] = \E[e^{-2\Lambda}]\)
</p>

<p>
Hence:
</p>

\begin{align*}
P(N_\Lambda(1) - N_\Lambda(0) = 0) P(N_\Lambda(2) - N_\Lambda(1) = 0)
&= \E[e^{-\Lambda}]^2 \\
&< \E[e^{-2\Lambda}] \\
&= P(N_\Lambda(2) = 0)\\
\end{align*}

<p>
The inequality appears using Jensen's inequality where \( x^2 \) is a convex function.
The inequality is strict due to \(\Lambda\) being non constant in the hypothesis.
</p>
</div>
</div>
</div>

<div id="outline-container-orgecd0418" class="outline-3">
<h3 id="orgecd0418"><span class="section-number-3">2.3.</span> Homework 3</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-orgd5a81bc" class="outline-4">
<h4 id="orgd5a81bc"><span class="section-number-4">2.3.1.</span> Problem 1</h4>
<div class="outline-text-4" id="text-2-3-1">
</div>
<ol class="org-ol">
<li><a id="org6e9d740"></a>A<br /></li>

<li><a id="orgc5c6220"></a>B<br /></li>
</ol>
</div>

<div id="outline-container-orgff25ca1" class="outline-4">
<h4 id="orgff25ca1"><span class="section-number-4">2.3.2.</span> Problem 2</h4>
</div>
<div id="outline-container-orgde5491c" class="outline-4">
<h4 id="orgde5491c"><span class="section-number-4">2.3.3.</span> Problem 3</h4>
<div class="outline-text-4" id="text-2-3-3">
</div>
<ol class="org-ol">
<li><a id="orgd66f776"></a>A<br /></li>
<li><a id="org879a109"></a>B<br /></li>
<li><a id="orgc1745d1"></a>C<br /></li>
</ol>
</div>
<div id="outline-container-orga12581b" class="outline-4">
<h4 id="orga12581b"><span class="section-number-4">2.3.4.</span> Problem 4</h4>
<div class="outline-text-4" id="text-2-3-4">
</div>
<ol class="org-ol">
<li><a id="orga18dc20"></a>A<br /></li>
<li><a id="org964b8e0"></a>B<br /></li>
</ol>
</div>
<div id="outline-container-org855b828" class="outline-4">
<h4 id="org855b828"><span class="section-number-4">2.3.5.</span> Problem 5</h4>
</div>
</div>
</div>

<div id="outline-container-orge8883aa" class="outline-2">
<h2 id="orge8883aa"><span class="section-number-2">3.</span> MATH 649D Younsi</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org9cb5862" class="outline-3">
<h3 id="org9cb5862"><span class="section-number-3">3.1.</span> Homework 1</h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="outline-container-org2eb7e5f" class="outline-4">
<h4 id="org2eb7e5f"><span class="section-number-4">3.1.1.</span> Problem 1</h4>
<div class="outline-text-4" id="text-3-1-1">
</div>
<ol class="org-ol">
<li><a id="orgc82f651"></a>A<br />
<div class="outline-text-5" id="text-3-1-1-1">
<p>
Denote \( E_i \) be the event where \( HTTHTH \) happens on the \( i \)-th toss. 
Note: \( \forall i \mathbb{P}(E_i) = \mathbb{P}(HTTHTH) = \frac{1}{2^6} \) because the tosses are independent.
\( \sum_i P(E_i) = \infty \) hence by Borel Cantelli \( \mathbb{P}(HTTHTH \text{ i.o}) = 1  \)  
</p>
</div>
</li>
<li><a id="orgddcf7b9"></a>B<br />
<div class="outline-text-5" id="text-3-1-1-2">
<p>
Denote \( E_i \) be the event where \( H\ldots H \) \( i \)-times for tosses \( i \) to \( 2i \). 
Note: \( \forall i \mathbb{P}(E_i) = \frac{1}{2^i} \).
\( \sum_i P(E_i) = 1 \) hence by Borel Cantelli \( \mathbb{P}(HTTHTH \text{ i.o}) = 0  \)  
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgb11e294" class="outline-4">
<h4 id="orgb11e294"><span class="section-number-4">3.1.2.</span> Problem 2</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
First we do some preliminary simplification, then I will do the proof.
</p>

<p>
Let \( A \) be an arbitrary set and \( \mathbbm{1}_A \) be it's indicator function.
\( \mathcal{F}(\mathbbm{1}_A) = \{\mathbbm{1}_A^{-1}(B) \vert B \text{ Borel} \}\) The characteristic function is only \( 0 \) or \( 1 \) hence we have the cases for \( B \).
</p>

<ol class="org-ol">
<li>\( 0,1 \not \in B \) \( \implies \mathbbm{1}_A^{-1}(B) = \emptyset \)</li>
<li>\( 1 \in B \) and \( 0 \not \in B \) \( \implies \mathbbm{1}_A^{-1}(B) = A \)</li>
<li>\( 0 \in B \) and \( 1 \not \in B \) \( \implies \mathbbm{1}_A^{-1}(B) = A^c \)</li>
<li>\( 0,1 \in B \) \( \implies \mathbbm{1}_A^{-1}(B) = \Omega \)</li>
</ol>

<p>
\( \mathcal{F}(\mathbbm{1}_A) = \{ \emptyset, A, A^c, \Omega \}\)
</p>

<p>
Having proved this for a general indicator function this will be true for given \( A \) and \( B \).
</p>

<p>
Thus the probabilities we need to calculate can be organized into a table.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">\( \emptyset \)</th>
<th scope="col" class="org-left">\( A \)</th>
<th scope="col" class="org-left">\( A^c \)</th>
<th scope="col" class="org-left">\( \Omega \)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\( \emptyset \)</td>
<td class="org-right">0</td>
<td class="org-left">0</td>
<td class="org-left">0</td>
<td class="org-left">0</td>
</tr>

<tr>
<td class="org-left">\( B \)</td>
<td class="org-right">0</td>
<td class="org-left">\( P(A \cap B) \)</td>
<td class="org-left">\( P(A^c \cap B) \)</td>
<td class="org-left">\( P(B) \)</td>
</tr>

<tr>
<td class="org-left">\( B^c \)</td>
<td class="org-right">0</td>
<td class="org-left">\( P(A \cap B^c) \)</td>
<td class="org-left">\( P(A^c \cap B^c) \)</td>
<td class="org-left">\( P(B^c) \)</td>
</tr>

<tr>
<td class="org-left">\( \Omega \)</td>
<td class="org-right">0</td>
<td class="org-left">\( P(A) \)</td>
<td class="org-left">\( P(A^c) \)</td>
<td class="org-left">\( 1\)</td>
</tr>
</tbody>
</table>

<p>
To verify independence in our proof, we only need to see that \( P(A \cap B) = P(A)P(B) \). The rows containing \( \emptyset \) and \( \Omega \) have nothing to verify.
Also short lemma if \( A \) and \( B \) are independent then \( A \), \( A^c \), \( B \), and \( B^c \) are all independent.
</p>

\begin{align*}
P(A^c \cap B^c)
&= P((A \cup B)^c) \\
&= 1 - P(A \cup B) \\
&= 1 - [P(A) + P(B) - P(A \cap B)] \\
&= 1 - P(A) - P(B) + P(A)P(B) \\
&= (1 - P(A))(1 - P(B)) 
\end{align*}

\begin{align*}
P(A \cap B^c)
&= P(A) - P(A \cap B) \\
&= P(A) - P(A)P(B) \\
&= P(A)(1 - P(B))
\end{align*}

\begin{align*}
P(A^c \cap B)
&= P(B) - P(A \cap B) \\
&= P(B) - P(A)P(B) \\
&= P(B)(1 - P(A))
\end{align*}

<p>
Now, for the actual proof.
</p>

<p>
Suppose \( A  \) and \( B \) are independent.
</p>

<p>
Then the correspondent \( \sigma \)-algebras are independent using the lemma.
</p>

<p>
Suppose the \( \sigma \)-algebras are independent. Then all the sets are independent, namely \( A \) and \( B \) are independent.
</p>
</div>
</div>

<div id="outline-container-orga13db96" class="outline-4">
<h4 id="orga13db96"><span class="section-number-4">3.1.3.</span> Problem 3</h4>
<div class="outline-text-4" id="text-3-1-3">
<p>
Let \( X \sim U[-1,1] \) having density \( f(x) = \frac{1}{2} \) 
</p>

<p>
Then \( X \) and \( X^2 \) are not independent. However their expectations are multiplicative.
We will not need to compute \( \E[X^2] \) interestingly enough.
</p>

<p>
Because \( X \) and \( X^3 \) are odd functions on a symmetric interval their expectations are \( 0 \).
</p>

<p>
Hence \( \E[X^3] = 0 = \E[X^2] \E[X] = \E[X^2] \cdot 0 \) 
</p>
</div>
</div>

<div id="outline-container-org55ac536" class="outline-4">
<h4 id="org55ac536"><span class="section-number-4">3.1.4.</span> Problem 4</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
We have stated in class that a distribution determines a random variable.
</p>

<p>
Let \( F(x) = \begin{cases} 0 & x < 0 \\ c(x) & 0 \leq x \leq 1 \\ 1 & 1 < x\end{cases} \) 
</p>

<p>
Where \( c(x) \) is the Cantor Function (the famous counter example that goes from \( 0 \) to \( 1 \))
</p>

<p>
\( F(x) \) is \( 0 \) as \( x \to - \infty \)
\( F(x) \) is \( 1 \) as \( x \to \infty \)
\( F(x) \) is right continuous
\( F(x) \) is increasing
\( \forall x . 0 \leq F(x) \leq 1 \) 
</p>

<p>
Hence \( F \) is a distribution for a random variable \( X \).
</p>

<p>
But \( X \) has no density function.
</p>

<p>
If it did, then firstly, \( f = 0 \) on \( \mathbb{R} - [0,1] \).
</p>

<p>
Then suppose \( f \neq 0 \) on some interval \( J \subseteq [0,1] \) then \( \int_J f d \mu \neq 0 \). But every interval contains a constant part of the cantor function meaning the function should have been \( 0 \) on that constant portion. Hence a contradiction.
</p>
</div>
</div>

<div id="outline-container-org27555dc" class="outline-4">
<h4 id="org27555dc"><span class="section-number-4">3.1.5.</span> Problem 5</h4>
<div class="outline-text-4" id="text-3-1-5">
<p>
I remember talking about this problem!
Consider the experiment of two tosses of a coin.
The columns are the first coin, the rows are the second coin.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">H</td>
<td class="org-left">T</td>
</tr>

<tr>
<td class="org-left">H</td>
<td class="org-left">HH</td>
<td class="org-left">TH</td>
</tr>

<tr>
<td class="org-left">T</td>
<td class="org-left">HT</td>
<td class="org-left">TT</td>
</tr>
</tbody>
</table>

<p>
Let \( A_1 \) be the event where the first toss is heads.
Let \( A_2 \) be the event where the second toss is heads.
Let \( A_3 \) be the event where only one toss is heads.
</p>

<p>
Then,
</p>

<p>
\( P(A_1 \cap A_2)  = \frac{1}{4} = P(A_1) P(A_2) \) (This is the top left element of the table)
\( P(A_1 \cap A_3)  = \frac{1}{4} = P(A_1) P(A_3) \) (This is the bottom left element of the table)
\( P(A_2 \cap A_3)  = \frac{1}{4} = P(A_2) P(A_3) \) (This it the top right element of the table)
</p>

<p>
However,
</p>

<p>
\( P(A_1 \cap A_2 \cap A_3) = 0 \neq \frac{1}{8} \) We cannot have both coins be heads and only one coin be heads.
</p>
</div>
</div>

<div id="outline-container-org313a73e" class="outline-4">
<h4 id="org313a73e"><span class="section-number-4">3.1.6.</span> Problem 6</h4>
<div class="outline-text-4" id="text-3-1-6">
</div>
<ol class="org-ol">
<li><a id="org04128e2"></a>A<br />
<div class="outline-text-5" id="text-3-1-6-1">
<p>
Let \( U \colon [0,1] \to \mathbb{R} \) via \( U(\omega) = \omega \).
</p>

<p>
The distribution of \( U \) is \( F_U(x) = \mathbb{P}(U^{-1}([-\infty,x])) = \mathbb{P}(U^{-1}([0,x])) = \mathbb{P}([0,x]) = x\).
The pushforward measure is the same as the original measure hence absolutely continuous and has density \( f \) given by the Radon-Nikodym theorem.
</p>

<p>
Hence \( \int_0^x f(x) dx = x \implies f(x) = 1 \).
</p>
</div>
</li>

<li><a id="org66c855e"></a>B<br />
<div class="outline-text-5" id="text-3-1-6-2">
<p>
Let \( U(\omega) = \sum_{j = 1}^{\infty} \frac{B_j(\omega)}{2^j} \)
</p>

<p>
Note that \( B_j(\omega) \) is a Bernoulli random variable.
</p>

<p>
Let \( \{I_n\}_{n \in \mathbb{N}} \) be an infinite collection of infinite disjoint subsets of \( \mathbb{N} \). The following \( I \)'s work: \( I_n = \{p_n^k \vert k \in \mathbb{Z}_{>0}\} \), but the structure is both not given and irrelevant.
</p>

<p>
\( U_n = \sum_{j=1}^{\infty} \frac{B_{f_n(j)(\omega)}}{2^j} \), the generating function of each term is
</p>

<p>
\( M_{\frac{B_{f_n(j)}}{2^j}}(t) = \E[e^{t \frac{B_{f_n(j)}}{2^j}}] = e^0 P(\frac{B_{f_n(j)}}{2^j}} = 0) + e^{\frac{t}{2^j}}P(\frac{B_{f_n(j)}}{2^j}} = 2^{-j}) = \frac{e^{\frac{t}{2^k}} + 1}{2} \)
</p>

<p>
The MGF for \( U_n \) is
</p>
\begin{align}
M_{U_n}
&= \lim_{k \to \infty} \prod_{j=1}^{k} \frac{e^{\frac{t}{2^k}} + 1}{2} \\
&= \lim_{k \to \infty} \frac{1}{2^k} \prod_{j=1}^{k} e^{\frac{t}{2^k}} + 1 \\
&= \lim_{k \to \infty} \frac{1}{2^k} (\frac{e^{\frac{t}{2^k}} - 1}{e^{\frac{t}{2^k}} - 1}) \prod_{j=1}^{k} e^{\frac{t}{2^k}} + 1 \\
&= \lim_{k \to \infty} \frac{1}{2^k} (\frac{e^{\frac{t}{2^k}} - 1}{e^{\frac{t}{2^k}} - 1}) (e^{\frac{t}{2^k}} + 1) \prod_{j=1}^{k - 1} e^{\frac{t}{2^k}} + 1 \\
&= \lim_{k \to \infty} \frac{1}{2^k} (\frac{e^{\frac{t}{2^{k-1}}} - 1}{e^{\frac{t}{2^k}} - 1}) \prod_{j=1}^{k - 1} e^{\frac{t}{2^k}} + 1 \\
&= \lim_{k \to \infty} \frac{e^{t} -1}{2^k (e^{\frac{t}{2^{k}}} - 1)} \\
&= \frac{e^t - 1}{t} \\
\end{align}

<p>
This one is crazy.
First equality is due to sum of RV is product of MGF.
Second equality is me taking out the constant \( 2 \) in the denominator.
Third equality is multiplying by \( 1 \) (our favorite analysis trick).
Fourth equality is taking the \( n \)-th term of the product out and reducing the index.
Fifth equality is multiplying the numerators, notice the difference of squares.
Sixth equality is actual black magic, we can multiply \( e^{\frac{t}{2^{k-1}}} -1 \) outside of the product with the \( k-1 \)'st term inside the product, yielding another difference of squares. This reduces the problem (by telescoping) until the last term does not cancel with anything.
Last equality is taking the limit. Use the Taylor series of \( e \). This gives
</p>
\begin{align}
2^k \cdot (e^{\frac{t}{2^k}} - 1)
&= 2^k [\frac{t}{2^k} + \frac{1}{2!}\frac{t^2}{2^{2k}} + \cdots] \\
&= [\frac{t}{1} + \frac{1}{2!}\frac{t^2}{2^{k}} + \cdots] \\
&\to t \text{ as } k \to \infty
\end{align}

<p>
Now we may conclude that this is the MGF of a uniform normal variable. Hence \( U_n \) has a uniform distribution.
</p>

<p>
All the \( U_n \) are independent due to the \( B_{f_n(j)} \) only looks at the binary digits which are not found in any other \( f_i \) with \( i \neq n \) by construction.
</p>
</div>
</li>

<li><a id="org4cb5e1b"></a>C<br />
<div class="outline-text-5" id="text-3-1-6-3">
<p>
Done by calculating the distribution.
\( \mathbb{P}(\Phi^{-1}(U_n) \leq x) = \mathbb{P}(U_n \leq \Phi(x)) = \Phi(x) \) 
</p>

<p>
The first equality is applying \( \Phi \) to both sides not changing the set. The second equality is by noting that \( U_n \) is a uniform random variable, the distribution states \( \mathbb{P}(U_n \leq t) = t \).
</p>

<p>
Hence the \( X_n \) are random variables (composition of measurable functions) have distribution \( \Phi(x) \) (thus normal) and are independent (\( U_n \) independent implies \( g(U_n) \) independent for \( g \) Borel measurable). 
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org4f71c03" class="outline-4">
<h4 id="org4f71c03"><span class="section-number-4">3.1.7.</span> Problem 7</h4>
<div class="outline-text-4" id="text-3-1-7">
</div>
<ol class="org-ol">
<li><a id="orgcba07d0"></a>A<br />
<div class="outline-text-5" id="text-3-1-7-1">
<p>
I will prove this statement using part B.
Let \( \mathcal{F}(X) \) and \( \mathcal{F}(Y, \mathcal{G}) \) be independent \( \sigma \)-algebras.
</p>

\begin{align}
\E[XY \vert \mathcal{G}]
&=  \E[\E[XY \vert \mathcal{F}(Y,\mathcal{G})] \vert \mathcal{G}]\\
&=  \E[Y \E[X \vert \mathcal{F}(Y,\mathcal{G})] \vert \mathcal{G}]\\
&=  \E[Y \E[X] \vert \mathcal{G}]\\
&=  \E[X]\E[Y \vert \mathcal{G}]\\
\end{align}
<p>
The first equality is the tower property.
The second equality is because \( Y \) is \( \mathcal{F}(Y) \)-measurable.
The third equality is because \( X \) is independent from \( \mathcal{F}(Y, \mathcal{G}) \) by hypothesis.
The last equality is linearity of expected value.
</p>
</div>
</li>

<li><a id="org173e298"></a>B<br />
<div class="outline-text-5" id="text-3-1-7-2">
<p>
Suppose \( X \) is \( \mathcal{G} \)-measurable.
We will show that \( \E[XY \vert \mathcal{G}] = X\E[Y \vert \mathcal{G}] \). We do this proof in stages. We will also show that it is enough to know this fact for non-negative random variables. The justification is, given \( X \) and \( Y \) we may decompose using the standard notation \( X = X^+ - X^- \) and \( Y = Y^+ - Y^- \).
Then,
</p>
\begin{align}
\E[(X^+ - X^-)(Y^+ - Y^-) \vert \mathcal{G}]
&= \E[X^+ Y^+ \vert \mathcal{G}] - \\
   &\E[X^+ Y^- \vert \mathcal{G}] - \\
   &\E[X^- Y^+ \vert \mathcal{G}] + \\
   &\E[X^- Y^- \vert \mathcal{G}] \\
&= X^+\E[Y^+ \vert \mathcal{G}] - \\
   &X^+\E[ Y^- \vert \mathcal{G}] - \\
   &X^-\E[ Y^+ \vert \mathcal{G}] + \\
   &X^-\E[ Y^- \vert \mathcal{G}] \\
&= (X^+-X^-)\E[Y^+ \vert \mathcal{G}] - \\
   &(X^+ - X^-)\E[ Y^- \vert \mathcal{G}] \\
&= (X^+-X^-)\E[Y^+ - Y^- \vert \mathcal{G}] \\
&= X\E[Y \vert \mathcal{G}]
\end{align}

<p>
(I apologize for the formatting, I didn't know how to make this look better.)
By using linearity and assuming the theorem for the positive case we have proven it for the more general case. For the remainder of this proof assume \( X,Y \geq 0 \).
</p>

<p>
We have that \( X \E[Y \vert \mathcal{G}] \) is \( \mathcal{G} \)-measurable. It remains to be shown that given an \( A \in \mathcal{G} \)  \( \int_A X \E[Y \vert \mathcal{G}] dP = \int_A XY dP \) 
</p>

<p>
First assume that \( B \in \mathcal{G} \) and \( X = \mathbbm{1}_B \).
Then \(  \)
</p>
\begin{align*}
\int_A \mathbbm{1}_B \E[Y \vert \mathcal{G}] dP
&= \int_{A \cap B} \E[Y \vert \mathcal{G}] dP \) \\
&= \int_{A \cap B} Y dP \\
&= \int_{A} \mathbbm{1}_B Y dP \\
&= \int_{A} X Y dP \\
\end{align*}

<p>
Then we may extend this using linearity to simple functions. Suppose \( X = \sum_{j=1}^{n} c_j \mathbbm{1}_{B_j} \) 
</p>
\begin{align*}
\int_A [\sum_{j=1}^{n} c_j \mathbbm{1}_{B_j}] \E[Y \vert \mathcal{G}] dP
&= \sum_{j=1}^{n} \int_{A \cap B_j} c_j \E[Y \vert \mathcal{G}] dP \) \\
&= \sum_{j=1}^{n} \int_{A \cap Bj} c_j Y dP \\
&= \int_{A} [\sum_{j=1}^n c_j \mathbbm{1}_{B_j}] Y dP \\
&= \int_{A} X Y dP \\
\end{align*}
<p>
Then we suppose that there is an increasing sequence of simple functions \( X_n \to X \).
</p>
\begin{align*}
\lim_{n \to \infty} \int_A X_n \E[Y \vert \mathcal{G}] dP
&= \lim_{n \to \infty} \int_{A} X_n Y dP \\
&= \int_{A} X Y dP \\
\end{align*}
<p>
The first equation is true because of the last step and we know \( X_n \) are simple. The second one is due to the monotone convergence theorem.
</p>
</div>
</li>


<li><a id="orgc0add6f"></a>C<br />
<div class="outline-text-5" id="text-3-1-7-3">
<p>
Let \( f \colon \mathbb{R} \to \mathbb{R} \) be Borel Measureable.
We will show \( \E[f(X) Y \vert X] = f(X) \E[Y \vert X] \). 
</p>

<p>
We know that \( f(X) \) is \( \mathcal{F}(X) \)-measurable. Hence by part B we may pull it out of the expectation.
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org355ccf0" class="outline-4">
<h4 id="org355ccf0"><span class="section-number-4">3.1.8.</span> Problem 8</h4>
<div class="outline-text-4" id="text-3-1-8">
<p>
First we calculate the expected value of \( W_n \)
</p>
\begin{align}
\E[W_n]
&= \E[\sum_{j=1}^{n} B_j X_j] \\
&= \sum_{j=1}^{n} \E[B_j X_j] \\
&= \sum_{j=1}^{n} [\E[\E[B_j X_j \vert B_j]]] \\
&= \sum_{j=1}^{n} \E[B_j \E[X_j]] \\
&= \sum_{j=1}^{n} \E[B_j \E[X_j \vert B_j]] \\
&= \sum_{j=1}^{n} \E[B_j \cdot 0] \\
&= 0
\end{align}

<p>
The first equality is the definition of \( W_n \).
The second equality uses linearity of expected value.
The third equality uses \( \E[X] = \E[\E[X \vert G]] \).
The fourth uses the fact \( B_j \) is measurable with respect to \( \mathcal{F}(B_j) \).
The fifth uses independence of coin flips. That is, \( B_j \) tells you about the previous \( j-1 \)  coin flips. But that is independent from the \( j \)-th coin flip.
The sixth is a straight calculation of expected value, \( X_j \) is symmetric around \( 0 \).
The last is multiplying and adding.
</p>

<p>
Now we calculate the expected value with respect to the previous filtration.
</p>
\begin{align}
\E[W_n \vert X_1, \cdots, X_{n-1}]
&= \E[\E[W_n \vert X_1, \cdots, X_n] \vert X_1, \cdots, X_{n_1}] \\
&= \E[\E[W_n] \vert X_1, \cdots, X_{n_1}] \\
&= \E[0 \vert X_1, \cdots, X_{n_1}] \\
&= 0 \\
&= \E[X_{n-1}]
\end{align}
<p>
The first equality uses the tower rule.
The second notes that \( W_n \) is \( X_1, \cdots, X_n \) measurable.
The third equality uses the expected value we calculated previously.
The rest is noting that the expectations are the same as what we desire.
</p>
</div>
</div>

<div id="outline-container-orgc7ba585" class="outline-4">
<h4 id="orgc7ba585"><span class="section-number-4">3.1.9.</span> Problem 9</h4>
<div class="outline-text-4" id="text-3-1-9">
<p>
Let \( X \sim N(\mu_1, \sigma_1^2) \) and \( Y \sim N(\mu_2, \sigma_2^2) \) be independent random variables.
</p>

<p>
Denote \( \varphi_{X + Y}(t) = \E[\exp(it(X + Y))] \) to be the characteristic function of \( X+Y \).
</p>

\begin{align}
\varphi_{X+Y}(t)
&= \varphi_X(t) \varphi_Y(t) \\
&= \exp(it \mu_1 - \frac{\sigma_1^2 t^2}{2}) \exp(it \mu_2 - \frac{\sigma_2^2 t^2}{2}) \\
&= \exp(it (\mu_1 + \mu_2) - \frac{(\sigma_1^2 \sigma_2^2) t^2}{2}) \\
\end{align}
<p>
Hence \( X+Y \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \)
Similarly,
</p>
\begin{align}
\varphi_{X-Y}(t)
&= \varphi_X(t) \varphi_Y(-t) \\
&= \exp(it \mu_1 - \frac{\sigma_1^2 t^2}{2}) \exp(-it \mu_2 - \frac{\sigma_2^2 t^2}{2}) \\
&= \exp(it (\mu_1 - \mu_2) - \frac{(\sigma_1^2 \sigma_2^2) t^2}{2}) \\
\end{align}

<p>
Hence \( X-Y \sim N(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2) \)
</p>

<p>
Independence: In class we had that for <b>normal</b> RV's \( \Cov(X,Y) = 0 \implies \) Independence (False in general).
Hence, we perform a quick calculation.
</p>

\begin{align*}
\Cov(X+Y,X-Y)
&= \Cov(X,X-Y) + \Cov(Y,X-Y) \\
&= \Cov(X,X) - \Cov(X, Y) + \Cov(Y,X) - \Cov(Y,Y) \\
&= \sigma_1^2 - \sigma_2^2
\end{align*}
<p>
This shows that if \( X \) and \( Y \) have the same variance then they are independent. It's curious, symbolically they must have the same variance if they want to be independent. But it's not intuitive for me at least.
</p>
</div>
</div>

<div id="outline-container-org4addbe4" class="outline-4">
<h4 id="org4addbe4"><span class="section-number-4">3.1.10.</span> Problem 10</h4>
<div class="outline-text-4" id="text-3-1-10">
<p>
Let \( X \sim \exp(\lambda) \). \( X \) has density \( f(x) \colon = \lambda e^{- \lambda x} . (x > 0)\).
</p>

<p>
\( X \) has distribution function
</p>

\begin{align*}
F(t)
&= \int_0^{t} \lambda e^{- \lambda x} dx \\
&= \lambda [\frac{1}{-\lambda} e^{- \lambda x}]_{x = 0}^{x = t} \\
&= - [e^{- \lambda t} - 1] \\
&= 1 - e^{-\lambda t}
\end{align*}
</div>


<ol class="org-ol">
<li><a id="org4fb72bd"></a>A<br />
<div class="outline-text-5" id="text-3-1-10-1">
\begin{align*}
P\{X \geq s + t \vert X \geq s \}
&= \frac{P\{X \geq s + t\}}{P\{X \geq s\}} \\
&= \frac{1 - F(s + t)}{1 - F(s)} \\
&= \frac{e^{- \lambda (s + t)}}{e^{- \lambda s}} \\
&= \frac{e^{- \lambda s}e^{-\lambda t}}{e^{- \lambda s}} \\
&= e^{-\lambda t} \\
&= P\{X \geq t \}
\end{align*}
</div>
</li>

<li><a id="org54b4345"></a>B<br />
<div class="outline-text-5" id="text-3-1-10-2">
<p>
Suppose \( Y \) is a random variable that has the memoryless property.
</p>

<p>
Then \( P\{ Y \geq s + t \vert Y \geq s \} = P\{ Y \geq t \} \)
Also from the conditional distribution we have \( \frac{P\{ Y \geq s + t\}}{P\{ Y \geq s \}} = P\{ Y \geq t \} \)
Hence \( P\{Y \geq s + t\} = P\{Y \geq s \} P\{Y \geq t\} \)
</p>

<p>
Note, \( P\{ Y \geq s\} = 1 - F_Y(s) \), by hypothesis \( F_Y \) (the distribution of \( Y \)) is continuous.
</p>

<p>
Denote \( g(s) = 1 - F_Y(s) \). We then have a functional equation \( g(s+t) = g(s) g(t) \) where \( g \) is decreasing (\( F_Y \) is increasing function) and continuous because as \( F_Y \) is continuous again by hypothesis.
</p>

<p>
Setting \( h(x) = \log(g(x)) \) and taking the log of both sides gives us the famous Cauchy functional equation for a continuous function \( h(x + y) = h(x) + h(y) \)  meaning \( h(x) = \lambda x \). To actually prove it, I would show that \( h(nx) = nh(x) \) for any integer \( n \). Then I would show that \( h(qx) = qh(x) \) for any rational \( q \). Then I would show by continuity \( h(cx) = ch(x) \) for any real \( c \). Then I would show that \( h(x) = cx \) by linear algebra. Hopefully saying <b>famous</b> and <b>Cauchy</b> in the same sentence is a proof.
</p>

<p>
Hence \( g(x) = e^{\lambda x} \). Because \( g \) is monotone decreasing we note that \( \lambda < 0 \). Hence the only random variable with continuous distribution is an exponentially distributed random variable.
</p>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</body>
</html>
