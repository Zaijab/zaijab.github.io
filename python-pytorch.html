<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-10-03 Thu 14:26 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Python PyTorch</title>
<meta name="author" content="Zain Jabbar" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>

          <link rel="stylesheet" href="static/css/site.css" type="text/css"/>
          <header><div class="menu"><ul>
          <li><a href="/">/</a></li>
          <li><a href="/about">/about</a></li>
          <li><a href="/posts">/posts</a></li></ul></div></header>
          <script src="static/js/nastaliq.js"></script>
          <script src="static/js/stacking.js"></script>
          <link href='https://unpkg.com/tippy.js@6.2.3/themes/light.css' rel='stylesheet'>
          <script src="https://unpkg.com/@popperjs/core@2"></script>
          <script src="https://unpkg.com/tippy.js@6"></script>
          <script>
          document.addEventListener('DOMContentLoaded', function() {
            let page = document.querySelector('.page');
            if (page) {
              initializePreviews(page);
            }
          });
          </script>
<script>MathJax = { loader: { load: ['[custom]/xypic.js'], paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'} }, tex: { packages: {'[+]': ['xypic']}, macros: { R: "{\\bf R}" } } };</script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js"></script>
<div class="grid-container"><div class="ds-grid"><div class="page">
</head>
<body>
<div id="content" class="content">
<h1 class="title">Python PyTorch</h1>
<p>
How to use the Torch library in <a href="python.html#ID-9b39a10f-bb89-4764-926b-83306ded3e01">Python</a> for <a href="machine-learning.html#ID-48564624-f1a4-4de9-83b2-9b60b49682c4">Machine Learning</a>.
</p>
<div id="outline-container-orgabca3e4" class="outline-2">
<h2 id="orgabca3e4"><span class="section-number-2">1.</span> Purpose</h2>
</div>

<div id="outline-container-orgcd76626" class="outline-2">
<h2 id="orgcd76626"><span class="section-number-2">2.</span> Overview</h2>
</div>

<div id="outline-container-org0c1d664" class="outline-2">
<h2 id="org0c1d664"><span class="section-number-2">3.</span> Examples</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org27a640d" class="outline-3">
<h3 id="org27a640d"><span class="section-number-3">3.1.</span> MNIST</h3>
<div class="outline-text-3" id="text-3-1">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torchvision

<span style="color: #005e8b;">n_epochs</span> = 3
<span style="color: #005e8b;">batch_size_train</span> = 64
<span style="color: #005e8b;">batch_size_test</span> = 1000
<span style="color: #005e8b;">learning_rate</span> = 0.01
<span style="color: #005e8b;">momentum</span> = 0.5
<span style="color: #005e8b;">log_interval</span> = 10

<span style="color: #005e8b;">random_seed</span> = 1
torch.backends.cudnn.<span style="color: #005e8b;">enabled</span> = <span style="color: #0000b0;">False</span>
torch.manual_seed<span style="color: #000000;">(</span>random_seed<span style="color: #000000;">)</span>

<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
  torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                             transform=torchvision.transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                               torchvision.transforms.ToTensor<span style="color: #808000;">()</span>,
                               torchvision.transforms.Normalize<span style="color: #808000;">(</span>
                                 <span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                             <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
  batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

<span style="color: #005e8b;">test_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
  torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">False</span>, download=<span style="color: #0000b0;">True</span>,
                             transform=torchvision.transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                               torchvision.transforms.ToTensor<span style="color: #808000;">()</span>,
                               torchvision.transforms.Normalize<span style="color: #808000;">(</span>
                                 <span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                             <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
  batch_size=batch_size_test, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

<span style="color: #005e8b;">examples</span> = <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>test_loader<span style="color: #000000;">)</span>
batch_idx, <span style="color: #000000;">(</span><span style="color: #005e8b;">example_data</span>, <span style="color: #005e8b;">example_targets</span><span style="color: #000000;">)</span> = <span style="color: #8f0075;">next</span><span style="color: #000000;">(</span>examples<span style="color: #000000;">)</span>

<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt

<span style="color: #005e8b;">fig</span> = plt.figure<span style="color: #000000;">()</span>
<span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>6<span style="color: #000000;">)</span>:
  plt.subplot<span style="color: #000000;">(</span>2,3,i+1<span style="color: #000000;">)</span>
  plt.tight_layout<span style="color: #000000;">()</span>
  plt.imshow<span style="color: #000000;">(</span>example_data<span style="color: #dd22dd;">[</span>i<span style="color: #dd22dd;">][</span>0<span style="color: #dd22dd;">]</span>, cmap=<span style="color: #3548cf;">'gray'</span>, interpolation=<span style="color: #3548cf;">'none'</span><span style="color: #000000;">)</span>
  plt.title<span style="color: #000000;">(</span><span style="color: #3548cf;">"Ground Truth: {}"</span>.<span style="color: #8f0075;">format</span><span style="color: #dd22dd;">(</span>example_targets<span style="color: #008899;">[</span>i<span style="color: #008899;">]</span><span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
  plt.xticks<span style="color: #000000;">(</span><span style="color: #dd22dd;">[]</span><span style="color: #000000;">)</span>
  plt.yticks<span style="color: #000000;">(</span><span style="color: #dd22dd;">[]</span><span style="color: #000000;">)</span>
fig
</pre>
</div>

<p>
<img src="./.ob-jupyter/c19d752e894ab2c6da17b024f9242a0bcd73cf92.png" alt="c19d752e894ab2c6da17b024f9242a0bcd73cf92.png" />
<img src="./.ob-jupyter/c19d752e894ab2c6da17b024f9242a0bcd73cf92.png" alt="c19d752e894ab2c6da17b024f9242a0bcd73cf92.png" />
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">
<span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim

<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">MultiLayerNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>MultiLayerNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>nn.Linear<span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #008899;">[</span>i<span style="color: #008899;">]</span>, layer_sizes<span style="color: #008899;">[</span>i+1<span style="color: #008899;">]</span><span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
    
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>  <span style="color: #595959;"># </span><span style="color: #595959;">Flatten the input tensor except for the batch dimension</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #005e8b;">x</span> = <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>  <span style="color: #595959;"># </span><span style="color: #595959;">Output layer without activation</span>
        <span style="color: #531ab6;">return</span> x

<span style="color: #595959;"># </span><span style="color: #595959;">Define the network architecture</span>
<span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>128, 64, 32<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>  <span style="color: #595959;"># </span><span style="color: #595959;">Example: 3 hidden layers plus input and output layer</span>
<span style="color: #005e8b;">model</span> = MultiLayerNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Optimizer and loss function</span>
<span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=learning_rate, momentum=momentum<span style="color: #000000;">)</span>
<span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example training loop</span>
<span style="color: #531ab6;">def</span> <span style="color: #721045;">train</span><span style="color: #000000;">(</span>epoch<span style="color: #000000;">)</span>:
    model.train<span style="color: #000000;">()</span>
    <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
        optimizer.zero_grad<span style="color: #000000;">()</span>
        <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
        <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
        loss.backward<span style="color: #000000;">()</span>
        optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">if</span> batch_idx % log_interval == 0:
            <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Train Epoch: </span>{epoch}<span style="color: #3548cf;"> [</span>{batch_idx * <span style="color: #8f0075;">len</span>(data)}<span style="color: #3548cf;">/</span>{<span style="color: #8f0075;">len</span>(train_loader.dataset)}<span style="color: #3548cf;"> (</span>{100. * batch_idx / <span style="color: #8f0075;">len</span>(train_loader):.0f}<span style="color: #3548cf;">%)]</span><span style="color: #0000b0;">\t</span><span style="color: #3548cf;">Loss: </span>{loss.item():.6f}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example of training the model</span>
<span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>1, n_epochs + 1<span style="color: #000000;">)</span>:
    train<span style="color: #000000;">(</span>epoch<span style="color: #000000;">)</span>

</pre>
</div>

<pre class="example" id="org8da11ad">
Train Epoch: 1 [0/60000 (0%)]	Loss: 2.385096
Train Epoch: 1 [640/60000 (1%)]	Loss: 2.358713
Train Epoch: 1 [1280/60000 (2%)]	Loss: 2.301008
Train Epoch: 1 [1920/60000 (3%)]	Loss: 2.293913
Train Epoch: 1 [2560/60000 (4%)]	Loss: 2.303028
Train Epoch: 1 [3200/60000 (5%)]	Loss: 2.315463
Train Epoch: 1 [3840/60000 (6%)]	Loss: 2.303805
Train Epoch: 1 [4480/60000 (7%)]	Loss: 2.317358
Train Epoch: 1 [5120/60000 (9%)]	Loss: 2.289500
Train Epoch: 1 [5760/60000 (10%)]	Loss: 2.302371
Train Epoch: 1 [6400/60000 (11%)]	Loss: 2.290801
Train Epoch: 1 [7040/60000 (12%)]	Loss: 2.286545
Train Epoch: 1 [7680/60000 (13%)]	Loss: 2.327992
Train Epoch: 1 [8320/60000 (14%)]	Loss: 2.314388
Train Epoch: 1 [8960/60000 (15%)]	Loss: 2.305534
Train Epoch: 1 [9600/60000 (16%)]	Loss: 2.298719
Train Epoch: 1 [10240/60000 (17%)]	Loss: 2.307938
Train Epoch: 1 [10880/60000 (18%)]	Loss: 2.306931
Train Epoch: 1 [11520/60000 (19%)]	Loss: 2.312992
Train Epoch: 1 [12160/60000 (20%)]	Loss: 2.301420
Train Epoch: 1 [12800/60000 (21%)]	Loss: 2.296477
Train Epoch: 1 [13440/60000 (22%)]	Loss: 2.293622
Train Epoch: 1 [14080/60000 (23%)]	Loss: 2.302766
Train Epoch: 1 [14720/60000 (25%)]	Loss: 2.331322
Train Epoch: 1 [15360/60000 (26%)]	Loss: 2.287327
Train Epoch: 1 [16000/60000 (27%)]	Loss: 2.294928
Train Epoch: 1 [16640/60000 (28%)]	Loss: 2.329630
Train Epoch: 1 [17280/60000 (29%)]	Loss: 2.304795
Train Epoch: 1 [17920/60000 (30%)]	Loss: 2.310084
Train Epoch: 1 [18560/60000 (31%)]	Loss: 2.296270
Train Epoch: 1 [19200/60000 (32%)]	Loss: 2.301918
Train Epoch: 1 [19840/60000 (33%)]	Loss: 2.296821
Train Epoch: 1 [20480/60000 (34%)]	Loss: 2.313798
Train Epoch: 1 [21120/60000 (35%)]	Loss: 2.301497
Train Epoch: 1 [21760/60000 (36%)]	Loss: 2.298261
Train Epoch: 1 [22400/60000 (37%)]	Loss: 2.310850
Train Epoch: 1 [23040/60000 (38%)]	Loss: 2.296573
Train Epoch: 1 [23680/60000 (39%)]	Loss: 2.305285
Train Epoch: 1 [24320/60000 (41%)]	Loss: 2.309992
Train Epoch: 1 [24960/60000 (42%)]	Loss: 2.300113
Train Epoch: 1 [25600/60000 (43%)]	Loss: 2.304913
Train Epoch: 1 [26240/60000 (44%)]	Loss: 2.304014
Train Epoch: 1 [26880/60000 (45%)]	Loss: 2.303046
Train Epoch: 1 [27520/60000 (46%)]	Loss: 2.281087
Train Epoch: 1 [28160/60000 (47%)]	Loss: 2.291744
Train Epoch: 1 [28800/60000 (48%)]	Loss: 2.308064
Train Epoch: 1 [29440/60000 (49%)]	Loss: 2.305803
Train Epoch: 1 [30080/60000 (50%)]	Loss: 2.296331
Train Epoch: 1 [30720/60000 (51%)]	Loss: 2.301073
Train Epoch: 1 [31360/60000 (52%)]	Loss: 2.300646
Train Epoch: 1 [32000/60000 (53%)]	Loss: 2.290384
Train Epoch: 1 [32640/60000 (54%)]	Loss: 2.288228
Train Epoch: 1 [33280/60000 (55%)]	Loss: 2.292485
Train Epoch: 1 [33920/60000 (57%)]	Loss: 2.303533
Train Epoch: 1 [34560/60000 (58%)]	Loss: 2.309538
Train Epoch: 1 [35200/60000 (59%)]	Loss: 2.310059
Train Epoch: 1 [35840/60000 (60%)]	Loss: 2.305730
Train Epoch: 1 [36480/60000 (61%)]	Loss: 2.304363
Train Epoch: 1 [37120/60000 (62%)]	Loss: 2.308058
Train Epoch: 1 [37760/60000 (63%)]	Loss: 2.291358
Train Epoch: 1 [38400/60000 (64%)]	Loss: 2.300776
Train Epoch: 1 [39040/60000 (65%)]	Loss: 2.312346
Train Epoch: 1 [39680/60000 (66%)]	Loss: 2.307284
Train Epoch: 1 [40320/60000 (67%)]	Loss: 2.296728
Train Epoch: 1 [40960/60000 (68%)]	Loss: 2.291497
Train Epoch: 1 [41600/60000 (69%)]	Loss: 2.286850
Train Epoch: 1 [42240/60000 (70%)]	Loss: 2.306036
Train Epoch: 1 [42880/60000 (71%)]	Loss: 2.296506
Train Epoch: 1 [43520/60000 (72%)]	Loss: 2.309544
Train Epoch: 1 [44160/60000 (74%)]	Loss: 2.290441
Train Epoch: 1 [44800/60000 (75%)]	Loss: 2.302175
Train Epoch: 1 [45440/60000 (76%)]	Loss: 2.303220
Train Epoch: 1 [46080/60000 (77%)]	Loss: 2.298772
Train Epoch: 1 [46720/60000 (78%)]	Loss: 2.295260
Train Epoch: 1 [47360/60000 (79%)]	Loss: 2.301393
Train Epoch: 1 [48000/60000 (80%)]	Loss: 2.304510
Train Epoch: 1 [48640/60000 (81%)]	Loss: 2.300156
Train Epoch: 1 [49280/60000 (82%)]	Loss: 2.290047
Train Epoch: 1 [49920/60000 (83%)]	Loss: 2.298460
Train Epoch: 1 [50560/60000 (84%)]	Loss: 2.299316
Train Epoch: 1 [51200/60000 (85%)]	Loss: 2.285703
Train Epoch: 1 [51840/60000 (86%)]	Loss: 2.303211
Train Epoch: 1 [52480/60000 (87%)]	Loss: 2.314299
Train Epoch: 1 [53120/60000 (88%)]	Loss: 2.287977
Train Epoch: 1 [53760/60000 (90%)]	Loss: 2.318513
Train Epoch: 1 [54400/60000 (91%)]	Loss: 2.307416
Train Epoch: 1 [55040/60000 (92%)]	Loss: 2.302808
Train Epoch: 1 [55680/60000 (93%)]	Loss: 2.306230
Train Epoch: 1 [56320/60000 (94%)]	Loss: 2.297386
Train Epoch: 1 [56960/60000 (95%)]	Loss: 2.294016
Train Epoch: 1 [57600/60000 (96%)]	Loss: 2.299497
Train Epoch: 1 [58240/60000 (97%)]	Loss: 2.295126
Train Epoch: 1 [58880/60000 (98%)]	Loss: 2.311256
Train Epoch: 1 [59520/60000 (99%)]	Loss: 2.303230
Train Epoch: 2 [0/60000 (0%)]	Loss: 2.288913
Train Epoch: 2 [640/60000 (1%)]	Loss: 2.298381
Train Epoch: 2 [1280/60000 (2%)]	Loss: 2.317007
Train Epoch: 2 [1920/60000 (3%)]	Loss: 2.310679
Train Epoch: 2 [2560/60000 (4%)]	Loss: 2.301724
Train Epoch: 2 [3200/60000 (5%)]	Loss: 2.303855
Train Epoch: 2 [3840/60000 (6%)]	Loss: 2.299796
Train Epoch: 2 [4480/60000 (7%)]	Loss: 2.301068
Train Epoch: 2 [5120/60000 (9%)]	Loss: 2.300470
Train Epoch: 2 [5760/60000 (10%)]	Loss: 2.301236
Train Epoch: 2 [6400/60000 (11%)]	Loss: 2.292290
Train Epoch: 2 [7040/60000 (12%)]	Loss: 2.294517
Train Epoch: 2 [7680/60000 (13%)]	Loss: 2.304445
Train Epoch: 2 [8320/60000 (14%)]	Loss: 2.292490
Train Epoch: 2 [8960/60000 (15%)]	Loss: 2.303126
Train Epoch: 2 [9600/60000 (16%)]	Loss: 2.293264
Train Epoch: 2 [10240/60000 (17%)]	Loss: 2.329648
Train Epoch: 2 [10880/60000 (18%)]	Loss: 2.287288
Train Epoch: 2 [11520/60000 (19%)]	Loss: 2.295470
Train Epoch: 2 [12160/60000 (20%)]	Loss: 2.291314
Train Epoch: 2 [12800/60000 (21%)]	Loss: 2.306928
Train Epoch: 2 [13440/60000 (22%)]	Loss: 2.289589
Train Epoch: 2 [14080/60000 (23%)]	Loss: 2.314500
Train Epoch: 2 [14720/60000 (25%)]	Loss: 2.296815
Train Epoch: 2 [15360/60000 (26%)]	Loss: 2.297661
Train Epoch: 2 [16000/60000 (27%)]	Loss: 2.294153
Train Epoch: 2 [16640/60000 (28%)]	Loss: 2.289486
Train Epoch: 2 [17280/60000 (29%)]	Loss: 2.304422
Train Epoch: 2 [17920/60000 (30%)]	Loss: 2.314797
Train Epoch: 2 [18560/60000 (31%)]	Loss: 2.301608
Train Epoch: 2 [19200/60000 (32%)]	Loss: 2.285531
Train Epoch: 2 [19840/60000 (33%)]	Loss: 2.317008
Train Epoch: 2 [20480/60000 (34%)]	Loss: 2.300267
Train Epoch: 2 [21120/60000 (35%)]	Loss: 2.300460
Train Epoch: 2 [21760/60000 (36%)]	Loss: 2.301199
Train Epoch: 2 [22400/60000 (37%)]	Loss: 2.302233
Train Epoch: 2 [23040/60000 (38%)]	Loss: 2.304351
Train Epoch: 2 [23680/60000 (39%)]	Loss: 2.283071
Train Epoch: 2 [24320/60000 (41%)]	Loss: 2.294349
Train Epoch: 2 [24960/60000 (42%)]	Loss: 2.291830
Train Epoch: 2 [25600/60000 (43%)]	Loss: 2.283660
Train Epoch: 2 [26240/60000 (44%)]	Loss: 2.312520
Train Epoch: 2 [26880/60000 (45%)]	Loss: 2.283541
Train Epoch: 2 [27520/60000 (46%)]	Loss: 2.298270
Train Epoch: 2 [28160/60000 (47%)]	Loss: 2.293580
Train Epoch: 2 [28800/60000 (48%)]	Loss: 2.280508
Train Epoch: 2 [29440/60000 (49%)]	Loss: 2.304064
Train Epoch: 2 [30080/60000 (50%)]	Loss: 2.297221
Train Epoch: 2 [30720/60000 (51%)]	Loss: 2.300562
Train Epoch: 2 [31360/60000 (52%)]	Loss: 2.312655
Train Epoch: 2 [32000/60000 (53%)]	Loss: 2.299777
Train Epoch: 2 [32640/60000 (54%)]	Loss: 2.288396
Train Epoch: 2 [33280/60000 (55%)]	Loss: 2.297800
Train Epoch: 2 [33920/60000 (57%)]	Loss: 2.300268
Train Epoch: 2 [34560/60000 (58%)]	Loss: 2.299311
Train Epoch: 2 [35200/60000 (59%)]	Loss: 2.316621
Train Epoch: 2 [35840/60000 (60%)]	Loss: 2.312788
Train Epoch: 2 [36480/60000 (61%)]	Loss: 2.306925
Train Epoch: 2 [37120/60000 (62%)]	Loss: 2.288342
Train Epoch: 2 [37760/60000 (63%)]	Loss: 2.298011
Train Epoch: 2 [38400/60000 (64%)]	Loss: 2.298501
Train Epoch: 2 [39040/60000 (65%)]	Loss: 2.294260
Train Epoch: 2 [39680/60000 (66%)]	Loss: 2.298462
Train Epoch: 2 [40320/60000 (67%)]	Loss: 2.314915
Train Epoch: 2 [40960/60000 (68%)]	Loss: 2.296944
Train Epoch: 2 [41600/60000 (69%)]	Loss: 2.310477
Train Epoch: 2 [42240/60000 (70%)]	Loss: 2.299576
Train Epoch: 2 [42880/60000 (71%)]	Loss: 2.297068
Train Epoch: 2 [43520/60000 (72%)]	Loss: 2.294639
Train Epoch: 2 [44160/60000 (74%)]	Loss: 2.267916
Train Epoch: 2 [44800/60000 (75%)]	Loss: 2.292493
Train Epoch: 2 [45440/60000 (76%)]	Loss: 2.291331
Train Epoch: 2 [46080/60000 (77%)]	Loss: 2.282326
Train Epoch: 2 [46720/60000 (78%)]	Loss: 2.274691
Train Epoch: 2 [47360/60000 (79%)]	Loss: 2.298259
Train Epoch: 2 [48000/60000 (80%)]	Loss: 2.296094
Train Epoch: 2 [48640/60000 (81%)]	Loss: 2.303764
Train Epoch: 2 [49280/60000 (82%)]	Loss: 2.310033
Train Epoch: 2 [49920/60000 (83%)]	Loss: 2.301688
Train Epoch: 2 [50560/60000 (84%)]	Loss: 2.298084
Train Epoch: 2 [51200/60000 (85%)]	Loss: 2.307931
Train Epoch: 2 [51840/60000 (86%)]	Loss: 2.293657
Train Epoch: 2 [52480/60000 (87%)]	Loss: 2.309092
Train Epoch: 2 [53120/60000 (88%)]	Loss: 2.300025
Train Epoch: 2 [53760/60000 (90%)]	Loss: 2.299077
Train Epoch: 2 [54400/60000 (91%)]	Loss: 2.299102
Train Epoch: 2 [55040/60000 (92%)]	Loss: 2.296930
Train Epoch: 2 [55680/60000 (93%)]	Loss: 2.281675
Train Epoch: 2 [56320/60000 (94%)]	Loss: 2.313937
Train Epoch: 2 [56960/60000 (95%)]	Loss: 2.303171
Train Epoch: 2 [57600/60000 (96%)]	Loss: 2.296360
Train Epoch: 2 [58240/60000 (97%)]	Loss: 2.311202
Train Epoch: 2 [58880/60000 (98%)]	Loss: 2.302699
Train Epoch: 2 [59520/60000 (99%)]	Loss: 2.283117
Train Epoch: 3 [0/60000 (0%)]	Loss: 2.291504
Train Epoch: 3 [640/60000 (1%)]	Loss: 2.282617
Train Epoch: 3 [1280/60000 (2%)]	Loss: 2.313164
Train Epoch: 3 [1920/60000 (3%)]	Loss: 2.300960
Train Epoch: 3 [2560/60000 (4%)]	Loss: 2.296529
Train Epoch: 3 [3200/60000 (5%)]	Loss: 2.301054
Train Epoch: 3 [3840/60000 (6%)]	Loss: 2.312456
Train Epoch: 3 [4480/60000 (7%)]	Loss: 2.303310
Train Epoch: 3 [5120/60000 (9%)]	Loss: 2.258321
Train Epoch: 3 [5760/60000 (10%)]	Loss: 2.290996
Train Epoch: 3 [6400/60000 (11%)]	Loss: 2.288428
Train Epoch: 3 [7040/60000 (12%)]	Loss: 2.308073
Train Epoch: 3 [7680/60000 (13%)]	Loss: 2.298165
Train Epoch: 3 [8320/60000 (14%)]	Loss: 2.299388
Train Epoch: 3 [8960/60000 (15%)]	Loss: 2.274753
Train Epoch: 3 [9600/60000 (16%)]	Loss: 2.311748
Train Epoch: 3 [10240/60000 (17%)]	Loss: 2.296280
Train Epoch: 3 [10880/60000 (18%)]	Loss: 2.298727
Train Epoch: 3 [11520/60000 (19%)]	Loss: 2.294015
Train Epoch: 3 [12160/60000 (20%)]	Loss: 2.295007
Train Epoch: 3 [12800/60000 (21%)]	Loss: 2.294599
Train Epoch: 3 [13440/60000 (22%)]	Loss: 2.295586
Train Epoch: 3 [14080/60000 (23%)]	Loss: 2.298033
Train Epoch: 3 [14720/60000 (25%)]	Loss: 2.299108
Train Epoch: 3 [15360/60000 (26%)]	Loss: 2.281291
Train Epoch: 3 [16000/60000 (27%)]	Loss: 2.294044
Train Epoch: 3 [16640/60000 (28%)]	Loss: 2.297594
Train Epoch: 3 [17280/60000 (29%)]	Loss: 2.296043
Train Epoch: 3 [17920/60000 (30%)]	Loss: 2.290023
Train Epoch: 3 [18560/60000 (31%)]	Loss: 2.291425
Train Epoch: 3 [19200/60000 (32%)]	Loss: 2.300992
Train Epoch: 3 [19840/60000 (33%)]	Loss: 2.288448
Train Epoch: 3 [20480/60000 (34%)]	Loss: 2.305419
Train Epoch: 3 [21120/60000 (35%)]	Loss: 2.294065
Train Epoch: 3 [21760/60000 (36%)]	Loss: 2.300374
Train Epoch: 3 [22400/60000 (37%)]	Loss: 2.292289
Train Epoch: 3 [23040/60000 (38%)]	Loss: 2.289689
Train Epoch: 3 [23680/60000 (39%)]	Loss: 2.293979
Train Epoch: 3 [24320/60000 (41%)]	Loss: 2.293819
Train Epoch: 3 [24960/60000 (42%)]	Loss: 2.288655
Train Epoch: 3 [25600/60000 (43%)]	Loss: 2.294372
Train Epoch: 3 [26240/60000 (44%)]	Loss: 2.296310
Train Epoch: 3 [26880/60000 (45%)]	Loss: 2.299816
Train Epoch: 3 [27520/60000 (46%)]	Loss: 2.296674
Train Epoch: 3 [28160/60000 (47%)]	Loss: 2.298690
Train Epoch: 3 [28800/60000 (48%)]	Loss: 2.291843
Train Epoch: 3 [29440/60000 (49%)]	Loss: 2.294229
Train Epoch: 3 [30080/60000 (50%)]	Loss: 2.292226
Train Epoch: 3 [30720/60000 (51%)]	Loss: 2.287867
Train Epoch: 3 [31360/60000 (52%)]	Loss: 2.291812
Train Epoch: 3 [32000/60000 (53%)]	Loss: 2.295155
Train Epoch: 3 [32640/60000 (54%)]	Loss: 2.304667
Train Epoch: 3 [33280/60000 (55%)]	Loss: 2.298279
Train Epoch: 3 [33920/60000 (57%)]	Loss: 2.292922
Train Epoch: 3 [34560/60000 (58%)]	Loss: 2.288983
Train Epoch: 3 [35200/60000 (59%)]	Loss: 2.302597
Train Epoch: 3 [35840/60000 (60%)]	Loss: 2.295396
Train Epoch: 3 [36480/60000 (61%)]	Loss: 2.297033
Train Epoch: 3 [37120/60000 (62%)]	Loss: 2.303109
Train Epoch: 3 [37760/60000 (63%)]	Loss: 2.294200
Train Epoch: 3 [38400/60000 (64%)]	Loss: 2.292369
Train Epoch: 3 [39040/60000 (65%)]	Loss: 2.288982
Train Epoch: 3 [39680/60000 (66%)]	Loss: 2.295994
Train Epoch: 3 [40320/60000 (67%)]	Loss: 2.288853
Train Epoch: 3 [40960/60000 (68%)]	Loss: 2.291435
Train Epoch: 3 [41600/60000 (69%)]	Loss: 2.285139
Train Epoch: 3 [42240/60000 (70%)]	Loss: 2.287546
Train Epoch: 3 [42880/60000 (71%)]	Loss: 2.299084
Train Epoch: 3 [43520/60000 (72%)]	Loss: 2.289164
Train Epoch: 3 [44160/60000 (74%)]	Loss: 2.296533
Train Epoch: 3 [44800/60000 (75%)]	Loss: 2.297116
Train Epoch: 3 [45440/60000 (76%)]	Loss: 2.293636
Train Epoch: 3 [46080/60000 (77%)]	Loss: 2.289562
Train Epoch: 3 [46720/60000 (78%)]	Loss: 2.287230
Train Epoch: 3 [47360/60000 (79%)]	Loss: 2.285909
Train Epoch: 3 [48000/60000 (80%)]	Loss: 2.291257
Train Epoch: 3 [48640/60000 (81%)]	Loss: 2.309173
Train Epoch: 3 [49280/60000 (82%)]	Loss: 2.298013
Train Epoch: 3 [49920/60000 (83%)]	Loss: 2.289240
Train Epoch: 3 [50560/60000 (84%)]	Loss: 2.297141
Train Epoch: 3 [51200/60000 (85%)]	Loss: 2.280318
Train Epoch: 3 [51840/60000 (86%)]	Loss: 2.296965
Train Epoch: 3 [52480/60000 (87%)]	Loss: 2.277202
Train Epoch: 3 [53120/60000 (88%)]	Loss: 2.291803
Train Epoch: 3 [53760/60000 (90%)]	Loss: 2.312039
Train Epoch: 3 [54400/60000 (91%)]	Loss: 2.295261
Train Epoch: 3 [55040/60000 (92%)]	Loss: 2.273329
Train Epoch: 3 [55680/60000 (93%)]	Loss: 2.280555
Train Epoch: 3 [56320/60000 (94%)]	Loss: 2.291034
Train Epoch: 3 [56960/60000 (95%)]	Loss: 2.294358
Train Epoch: 3 [57600/60000 (96%)]	Loss: 2.304090
Train Epoch: 3 [58240/60000 (97%)]	Loss: 2.293759
Train Epoch: 3 [58880/60000 (98%)]	Loss: 2.295875
Train Epoch: 3 [59520/60000 (99%)]	Loss: 2.287370
</pre>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #531ab6;">def</span> <span style="color: #721045;">extract_and_plot_histogram</span><span style="color: #000000;">(</span>model<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Collect all weights across all layers</span>
    <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
    <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
        <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:  <span style="color: #595959;"># </span><span style="color: #595959;">This checks if the parameter is a weight matrix</span>
            <span style="color: #005e8b;">weights</span> = param.data.numpy<span style="color: #000000;">()</span>  <span style="color: #595959;"># </span><span style="color: #595959;">Convert to numpy array</span>
            all_weights.extend<span style="color: #000000;">(</span>weights.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>  <span style="color: #595959;"># </span><span style="color: #595959;">Flatten and extend the list</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot histogram of weights</span>
    plt.figure<span style="color: #000000;">(</span>figsize=<span style="color: #dd22dd;">(</span>10, 5<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
    plt.hist<span style="color: #000000;">(</span>all_weights, bins=100, alpha=0.7<span style="color: #000000;">)</span>
    plt.title<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights'</span><span style="color: #000000;">)</span>
    plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
    plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
    plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
    plt.show<span style="color: #000000;">()</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Assuming `model` is already defined and trained</span>
extract_and_plot_histogram<span style="color: #000000;">(</span>model<span style="color: #000000;">)</span>

</pre>
</div>


<div id="orga5f3e71" class="figure">
<p><img src="./.ob-jupyter/3f753761ddcbd83fe38eaffc86aa46ddefa6939f.png" alt="3f753761ddcbd83fe38eaffc86aa46ddefa6939f.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torchvision
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #595959;"># </span><span style="color: #595959;">Define the neural network model</span>
<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">CustomNetwork</span><span style="color: #000000;">(</span>nn.Module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, layer_sizes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>CustomNetwork, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">layers</span> = nn.ModuleList<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> i <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #dd22dd;">)</span> - 1<span style="color: #000000;">)</span>:
            <span style="color: #531ab6;">self</span>.layers.append<span style="color: #000000;">(</span>nn.Linear<span style="color: #dd22dd;">(</span>layer_sizes<span style="color: #008899;">[</span>i<span style="color: #008899;">]</span>, layer_sizes<span style="color: #008899;">[</span>i+1<span style="color: #008899;">]</span><span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">x</span> = torch.flatten<span style="color: #000000;">(</span>x, 1<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">for</span> layer <span style="color: #531ab6;">in</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>:-1<span style="color: #000000;">]</span>:
            <span style="color: #005e8b;">x</span> = torch.sigmoid<span style="color: #000000;">(</span>layer<span style="color: #dd22dd;">(</span>x<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
        <span style="color: #531ab6;">return</span> <span style="color: #531ab6;">self</span>.layers<span style="color: #000000;">[</span>-1<span style="color: #000000;">](</span>x<span style="color: #000000;">)</span>

<span style="color: #531ab6;">def</span> <span style="color: #721045;">create_and_train_model</span><span style="color: #000000;">(</span>n_layers, neurons_per_layer, epochs, train_loader<span style="color: #000000;">)</span>:
    <span style="color: #595959;"># </span><span style="color: #595959;">Create layer sizes array</span>
    <span style="color: #005e8b;">layer_sizes</span> = <span style="color: #000000;">[</span>784<span style="color: #000000;">]</span> + <span style="color: #000000;">[</span>neurons_per_layer<span style="color: #000000;">]</span> * n_layers + <span style="color: #000000;">[</span>10<span style="color: #000000;">]</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Initialize the model</span>
    <span style="color: #005e8b;">model</span> = CustomNetwork<span style="color: #000000;">(</span>layer_sizes<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">optimizer</span> = optim.SGD<span style="color: #000000;">(</span>model.parameters<span style="color: #dd22dd;">()</span>, lr=0.01, momentum=0.5<span style="color: #000000;">)</span>
    <span style="color: #005e8b;">criterion</span> = nn.CrossEntropyLoss<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Function to plot histogram of weights</span>
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">plot_weights</span><span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>:
        <span style="color: #005e8b;">all_weights</span> = <span style="color: #000000;">[]</span>
        <span style="color: #531ab6;">for</span> name, param <span style="color: #531ab6;">in</span> model.named_parameters<span style="color: #000000;">()</span>:
            <span style="color: #531ab6;">if</span> <span style="color: #3548cf;">'weight'</span> <span style="color: #531ab6;">in</span> name:
                all_weights.extend<span style="color: #000000;">(</span>param.data.cpu<span style="color: #dd22dd;">()</span>.numpy<span style="color: #dd22dd;">()</span>.flatten<span style="color: #dd22dd;">()</span><span style="color: #000000;">)</span>
        plt.hist<span style="color: #000000;">(</span>all_weights, bins=50, alpha=0.7<span style="color: #000000;">)</span>
        plt.title<span style="color: #000000;">(</span>title<span style="color: #000000;">)</span>
        plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Weight Values'</span><span style="color: #000000;">)</span>
        plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">'Frequency'</span><span style="color: #000000;">)</span>
        plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
        plt.show<span style="color: #000000;">()</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights before training</span>
    plot_weights<span style="color: #000000;">(</span><span style="color: #3548cf;">'Histogram of Weights Before Training'</span><span style="color: #000000;">)</span>

    <span style="color: #595959;"># </span><span style="color: #595959;">Training loop</span>
    <span style="color: #531ab6;">for</span> epoch <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">range</span><span style="color: #000000;">(</span>epochs<span style="color: #000000;">)</span>:
        model.train<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">for</span> batch_idx, <span style="color: #000000;">(</span>data, target<span style="color: #000000;">)</span> <span style="color: #531ab6;">in</span> <span style="color: #8f0075;">enumerate</span><span style="color: #000000;">(</span>train_loader<span style="color: #000000;">)</span>:
            optimizer.zero_grad<span style="color: #000000;">()</span>
            <span style="color: #005e8b;">output</span> = model<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
            <span style="color: #005e8b;">loss</span> = criterion<span style="color: #000000;">(</span>output, target<span style="color: #000000;">)</span>
            loss.backward<span style="color: #000000;">()</span>
            optimizer.step<span style="color: #000000;">()</span>
        <span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Epoch </span>{epoch+1}<span style="color: #3548cf;">, Loss: </span>{loss.item()}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

        <span style="color: #595959;"># </span><span style="color: #595959;">Plot weights after each epoch</span>
        plot_weights<span style="color: #000000;">(</span>f<span style="color: #3548cf;">'Histogram of Weights After Epoch </span>{epoch+1}<span style="color: #3548cf;">'</span><span style="color: #000000;">)</span>

<span style="color: #595959;"># </span><span style="color: #595959;">Example usage:</span>
<span style="color: #005e8b;">batch_size_train</span> = 64
<span style="color: #005e8b;">train_loader</span> = torch.utils.data.DataLoader<span style="color: #000000;">(</span>
    torchvision.datasets.MNIST<span style="color: #dd22dd;">(</span>root=<span style="color: #3548cf;">'~/.cache/datasets/mnist'</span>, train=<span style="color: #0000b0;">True</span>, download=<span style="color: #0000b0;">True</span>,
                               transform=transforms.Compose<span style="color: #008899;">(</span><span style="color: #972500;">[</span>
                                   transforms.ToTensor<span style="color: #808000;">()</span>,
                                   transforms.Normalize<span style="color: #808000;">(</span><span style="color: #531ab6;">(</span>0.1307,<span style="color: #531ab6;">)</span>, <span style="color: #531ab6;">(</span>0.3081,<span style="color: #531ab6;">)</span><span style="color: #808000;">)</span>
                               <span style="color: #972500;">]</span><span style="color: #008899;">)</span><span style="color: #dd22dd;">)</span>,
    batch_size=batch_size_train, shuffle=<span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>

create_and_train_model<span style="color: #000000;">(</span>n_layers=3, neurons_per_layer=128, epochs=3, train_loader=train_loader<span style="color: #000000;">)</span>

</pre>
</div>

<pre class="example">
5ed329aa-4105-427a-a0f3-3f7915019560
</pre>
</div>
</div>
<div id="outline-container-orgf226077" class="outline-3">
<h3 id="orgf226077"><span class="section-number-3">3.2.</span> Basics</h3>
<div class="outline-text-3" id="text-3-2">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #595959;">#</span><span style="color: #595959;">imports</span>
<span style="color: #531ab6;">import</span> torch
<span style="color: #531ab6;">import</span> torch.nn <span style="color: #531ab6;">as</span> nn
<span style="color: #531ab6;">import</span> torch.optim <span style="color: #531ab6;">as</span> optim
<span style="color: #531ab6;">import</span> torch.nn.functional <span style="color: #531ab6;">as</span> F
<span style="color: #531ab6;">from</span> torch.utils.data <span style="color: #531ab6;">import</span> DataLoader
<span style="color: #531ab6;">import</span> torchvision.datasets <span style="color: #531ab6;">as</span> datasets
<span style="color: #531ab6;">import</span> torchvision.transforms <span style="color: #531ab6;">as</span> transforms

<span style="color: #531ab6;">class</span> <span style="color: #005f5f;">NN</span><span style="color: #000000;">(</span>nn.module<span style="color: #000000;">)</span>:
    <span style="color: #531ab6;">def</span> <span style="color: #721045;">__init__</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, input_size, num_classes<span style="color: #000000;">)</span>:
        <span style="color: #8f0075;">super</span><span style="color: #000000;">(</span>NN, <span style="color: #531ab6;">self</span><span style="color: #000000;">)</span>.__init__<span style="color: #000000;">()</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">fc1</span> = nn.Linear<span style="color: #000000;">(</span>input_size, 50<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">self</span>.<span style="color: #005e8b;">fc2</span> = nn.Linear<span style="color: #000000;">(</span>50, num_classes<span style="color: #000000;">)</span>
        <span style="color: #531ab6;">pass</span>

    <span style="color: #531ab6;">def</span> <span style="color: #721045;">forward</span><span style="color: #000000;">(</span><span style="color: #531ab6;">self</span>, x<span style="color: #000000;">)</span>: 
</pre>
</div>
</div>
</div>
<div id="outline-container-orgddaad9b" class="outline-3">
<h3 id="orgddaad9b"><span class="section-number-3">3.3.</span> Autodiff</h3>
<div class="outline-text-3" id="text-3-3">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #531ab6;">import</span> torch


</pre>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
